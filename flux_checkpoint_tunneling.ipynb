{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "# @title 1. 라이브러리 설치\n",
        "%pip install -q fastapi uvicorn[standard]\n",
        "%pip install -q pyngrok>=7.0.0 # diffusers>=0.27.0 transformers accelerate\n",
        "%pip install -q controlnet_aux Pillow # torch>=2.0.0\n",
        "%pip install -q safetensors pydantic huggingface_hub python-multipart\n",
        "# %pip install -U diffusers transformers accelerate safetensors\n",
        "%pip install -q wget\n",
        "# %pip install -q peft requests\n",
        "\n",
        "# # xformers 사용 시 (버전 호환 확인 필요)\n",
        "# %pip install -q fastapi uvicorn[standard] pyngrok>=7.0.0  # 1) FastAPI / 서버용\n",
        "# %pip install -q diffusers==0.33.1 transformers==4.51.3 accelerate==1.6.0  # 2) Diffusers + Transformers + Accelerate\n",
        "# %pip install -q controlnet_aux Pillow safetensors pydantic huggingface_hub python-multipart # 3) Flux용 부가 기능들\n",
        "# %pip install -q ninja wget  # 4) 기타 도구\n",
        "\n",
        "# %pip uninstall -y xformers  # 5) 기존 xformers 제거\n",
        "# %pip install -q git+https://github.com/facebookresearch/xformers.git  # 6) Colab(PyTorch 2.6.0+cu124) 환경에 맞춰 xFormers를 소스에서 빌드 설치\n",
        "# # %pip install -q peft requests\n",
        "\n",
        "print(\"✅ 라이브러리 설치 완료\")"
      ],
      "metadata": {
        "id": "w-k2065nuVgM"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 라이브러리 호환성 확인\n",
        "# jedi는 상관 없음 (colab 특성 상, 설치 안한다고 함)\n",
        "!pip check"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7AY8e66XRJzV",
        "outputId": "d3b77356-9b5e-480e-9926-fdf245fa3d10"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ipython 7.34.0 requires jedi, which is not installed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title InstantX 코드 파일 다운로드\n",
        "import wget\n",
        "import os\n",
        "\n",
        "# Flux InstantX 파일 리스트\n",
        "files = [\n",
        "    (\"pipeline_flux_ipa.py\", \"https://huggingface.co/InstantX/FLUX.1-dev-IP-Adapter/resolve/main/pipeline_flux_ipa.py?download=true\"),\n",
        "    (\"transformer_flux.py\", \"https://huggingface.co/InstantX/FLUX.1-dev-IP-Adapter/resolve/main/transformer_flux.py?download=true\"),\n",
        "    (\"attention_processor.py\", \"https://huggingface.co/InstantX/FLUX.1-dev-IP-Adapter/resolve/main/attention_processor.py?download=true\"),\n",
        "    (\"infer_flux_ipa_siglip.py\", \"https://huggingface.co/InstantX/FLUX.1-dev-IP-Adapter/resolve/main/infer_flux_ipa_siglip.py?download=true\"),\n",
        "    (\"ip-adapter.bin\", \"https://huggingface.co/InstantX/FLUX.1-dev-IP-Adapter/resolve/main/ip-adapter.bin?download=true\"),\n",
        "]\n",
        "\n",
        "for filename, url in files:\n",
        "    if not os.path.exists(filename):\n",
        "        wget.download(url, out=filename)\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "_ntweVQSd75c"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 2. Hugging Face 로그인 / Ngrok 설정 (Authtoken 시크릿 키)\n",
        "import os\n",
        "from google.colab import userdata\n",
        "from huggingface_hub import login\n",
        "from pyngrok import conf, ngrok\n",
        "\n",
        "# Hugging Face 및 ngrok 토큰 (Colab Secret에서 가져오기)\n",
        "hf_token = userdata.get(\"HF_TOKEN\")        # Hugging Face Token\n",
        "civitai_token = userdata.get(\"CIVITAI_TOKEN\")   # CivitAI Token\n",
        "ngrok_token = userdata.get(\"google_ngrok_authtoken\")  # ngrok Token\n",
        "\n",
        "if hf_token == None:\n",
        "  print('x')\n",
        "if civitai_token == None:\n",
        "  print('x')\n",
        "if ngrok_token == None:\n",
        "  print('x')\n",
        "\n",
        "# 포트 설정\n",
        "PORT = 8000\n",
        "\n",
        "# hugging face 로그인\n",
        "print(\"🔑 Hugging Face 로그인 중...\")\n",
        "login(token=hf_token)\n",
        "\n",
        "# ngrok 설정\n",
        "ngrok.set_auth_token(ngrok_token)\n",
        "print(\"✅ Ngrok Authtoken 설정 완료\")"
      ],
      "metadata": {
        "id": "A0jI1SIruVeF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "96eb4d8b-9581-4112-b66d-10fdd8d92971"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔑 Hugging Face 로그인 중...\n",
            "✅ Ngrok Authtoken 설정 완료\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 3. CivitAI Checkpoint 다운로드\n",
        "import os\n",
        "os.environ[\"CIVITAI_API_TOKEN\"] = civitai_token\n",
        "\n",
        "import wget\n",
        "!wget -O illustration_juaner_flux.safetensors \\\n",
        "\"https://civitai.com/api/download/models/1215918?type=Model&format=SafeTensor&size=full&token=$CIVITAI_API_TOKEN\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2R0izjLyvHfL",
        "outputId": "193ceb19-413a-4036-f33d-ff046e555ed2"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-05-06 09:02:12--  https://civitai.com/api/download/models/1215918?type=Model&format=SafeTensor&size=full&token=2ab1653b9d579f186d295f4540908157\n",
            "Resolving civitai.com (civitai.com)... 104.22.18.237, 172.67.12.143, 104.22.19.237, ...\n",
            "Connecting to civitai.com (civitai.com)|104.22.18.237|:443... connected.\n",
            "HTTP request sent, awaiting response... 307 Temporary Redirect\n",
            "Location: https://civitai-delivery-worker-prod.5ac0637cfd0766c97916cefa3764fbdf.r2.cloudflarestorage.com/model/686417/jGhibliV2FluxUltimate.FZyi.safetensors?X-Amz-Expires=86400&response-content-disposition=attachment%3B%20filename%3D%22IllustrationJuanerGhibli_v20.safetensors%22&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=e01358d793ad6966166af8b3064953ad/20250506/us-east-1/s3/aws4_request&X-Amz-Date=20250506T090212Z&X-Amz-SignedHeaders=host&X-Amz-Signature=bff7537dc4e114c059b9336cf8c2e5fd41338bb911a859f96495f4500a6e1ade [following]\n",
            "--2025-05-06 09:02:12--  https://civitai-delivery-worker-prod.5ac0637cfd0766c97916cefa3764fbdf.r2.cloudflarestorage.com/model/686417/jGhibliV2FluxUltimate.FZyi.safetensors?X-Amz-Expires=86400&response-content-disposition=attachment%3B%20filename%3D%22IllustrationJuanerGhibli_v20.safetensors%22&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=e01358d793ad6966166af8b3064953ad/20250506/us-east-1/s3/aws4_request&X-Amz-Date=20250506T090212Z&X-Amz-SignedHeaders=host&X-Amz-Signature=bff7537dc4e114c059b9336cf8c2e5fd41338bb911a859f96495f4500a6e1ade\n",
            "Resolving civitai-delivery-worker-prod.5ac0637cfd0766c97916cefa3764fbdf.r2.cloudflarestorage.com (civitai-delivery-worker-prod.5ac0637cfd0766c97916cefa3764fbdf.r2.cloudflarestorage.com)... 162.159.141.50, 172.66.1.46, 2606:4700:7::12e, ...\n",
            "Connecting to civitai-delivery-worker-prod.5ac0637cfd0766c97916cefa3764fbdf.r2.cloudflarestorage.com (civitai-delivery-worker-prod.5ac0637cfd0766c97916cefa3764fbdf.r2.cloudflarestorage.com)|162.159.141.50|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 11901497256 (11G)\n",
            "Saving to: ‘illustration_juaner_flux.safetensors’\n",
            "\n",
            "illustration_juaner 100%[===================>]  11.08G  89.9MB/s    in 2m 7s   \n",
            "\n",
            "2025-05-06 09:04:20 (89.4 MB/s) - ‘illustration_juaner_flux.safetensors’ saved [11901497256/11901497256]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 4. FastAPI 애플리케이션 코드 작성 (main_server.py 파일 생성)\n",
        "# 이 셀은 FastAPI 서버 코드를 main_server.py 파일로 저장합니다.\n",
        "\n",
        "%%writefile main_server.py\n",
        "import os, io, gc, logging\n",
        "from contextlib import asynccontextmanager\n",
        "from typing import Optional, Any\n",
        "\n",
        "import torch\n",
        "import uvicorn\n",
        "from fastapi import FastAPI, HTTPException, UploadFile, File, Form\n",
        "from fastapi.responses import Response\n",
        "from pydantic import BaseModel\n",
        "from PIL import Image\n",
        "from pyngrok import ngrok\n",
        "\n",
        "# Flux 관련 import\n",
        "from diffusers import FluxPipeline, FluxControlNetModel, FluxControlNetPipeline\n",
        "from diffusers import FluxTransformer2DModel as Checkpoint2DModel\n",
        "from transformers import T5EncoderModel\n",
        "\n",
        "# IP Adapter 관련 import\n",
        "from controlnet_aux import CannyDetector\n",
        "from pipeline_flux_ipa import FluxPipeline as FluxPipelineIP\n",
        "from transformer_flux import FluxTransformer2DModel\n",
        "from infer_flux_ipa_siglip import resize_img, IPAdapter\n",
        "\n",
        "# --- 설정 상수 ---\n",
        "BASE_MODEL_ID = \"black-forest-labs/FLUX.1-dev\"\n",
        "CHECKPOINT_MODEL_ID = \"illustration_juaner_flux.safetensors\"\n",
        "CONTROLNET_MODEL_ID = \"Shakker-Labs/FLUX.1-dev-ControlNet-Union-Pro-2.0\"\n",
        "IMAGE_ENCODER_PATH = \"google/siglip-so400m-patch14-384\"\n",
        "IPADAPTER_PATH = \"./ip-adapter.bin\"\n",
        "\n",
        "DEFAULT_STEPS = 30\n",
        "DEFAULT_GUIDANCE_SCALE = 3.5\n",
        "DEFAULT_CONTROLNET_SCALE = 0.7\n",
        "DEFAULT_IPADAPTER_SCALE = 0.7\n",
        "IMAGE_WIDTH = 1024\n",
        "IMAGE_HEIGHT = 1024\n",
        "PORT = 8000\n",
        "\n",
        "# --- 전역 상태 ---\n",
        "base_pipe: Optional[FluxPipeline] = None\n",
        "checkpoint_pipe: Optional[FluxPipeline] = None\n",
        "controlnet_pipe: Optional[FluxControlNetPipeline] = None\n",
        "pipe_ip: Optional[FluxPipelineIP] = None\n",
        "ip_model: Optional[IPAdapter] = None\n",
        "controlnet_preprocessor: Optional[Any] = None\n",
        "\n",
        "device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "dtype: torch.dtype = torch.bfloat16 if (device == \"cuda\" and torch.cuda.is_bf16_supported()) else torch.float16\n",
        "\n",
        "# --- 로깅 설정 ---\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# --- 유틸 함수 ---\n",
        "def load_pil_image(image_bytes: bytes) -> Image.Image:\n",
        "    return Image.open(io.BytesIO(image_bytes)).convert(\"RGB\")\n",
        "\n",
        "def image_to_bytes(image: Image.Image) -> bytes:\n",
        "    byte_arr = io.BytesIO()\n",
        "    image.save(byte_arr, format='PNG')\n",
        "    byte_arr.seek(0)\n",
        "    return byte_arr.getvalue()\n",
        "\n",
        "def get_generator(seed: Optional[int] = None) -> torch.Generator:\n",
        "    if seed is None:\n",
        "        seed = torch.randint(0, 2**32 - 1, (1,)).item()\n",
        "    logger.info(f\"Using seed: {seed}\")\n",
        "    return torch.Generator(device=\"cuda\").manual_seed(seed)\n",
        "\n",
        "async def prepare_reference_image(uploaded_image: UploadFile) -> Image.Image:\n",
        "    image = load_pil_image(await uploaded_image.read())\n",
        "    return resize_img(image)\n",
        "\n",
        "def prepare_control_image(uploaded_image: UploadFile) -> Image.Image:\n",
        "    if controlnet_preprocessor is None:\n",
        "        raise RuntimeError(\"ControlNet preprocessor not loaded.\")\n",
        "    image = load_pil_image(uploaded_image.file.read())\n",
        "    return controlnet_preprocessor(image)\n",
        "\n",
        "# --- 모델 로딩 ---\n",
        "def load_models():\n",
        "    global base_pipe, controlnet_pipe, pipe_ip, ip_model, controlnet_preprocessor\n",
        "\n",
        "    logger.info(f\"Loading models to CPU with dtype: {dtype}\")\n",
        "\n",
        "    '''\n",
        "    2025.05.06 - checkpoint 모델로 교체하여 주석처리\n",
        "    '''\n",
        "\n",
        "    # Base pipeline\n",
        "    # base_pipe = FluxPipeline.from_pretrained(BASE_MODEL_ID, torch_dtype=dtype).to(\"cpu\")\n",
        "    # base_pipe.enable_model_cpu_offload()\n",
        "    # base_pipe.enable_attention_slicing()\n",
        "    # base_pipe.enable_vae_slicing()\n",
        "    # base_pipe.enable_xformers_memory_efficient_attention()\n",
        "\n",
        "    # # ControlNet pipeline\n",
        "    # controlnet_model = FluxControlNetModel.from_pretrained(CONTROLNET_MODEL_ID, torch_dtype=dtype)\n",
        "    # controlnet_pipe = FluxControlNetPipeline.from_pretrained(BASE_MODEL_ID, controlnet=controlnet_model, torch_dtype=dtype).to(\"cpu\")\n",
        "    # controlnet_pipe.enable_model_cpu_offload()\n",
        "    # controlnet_pipe.enable_attention_slicing()\n",
        "    # controlnet_pipe.enable_vae_slicing()\n",
        "    # # controlnet_pipe.enable_xformers_memory_efficient_attention()\n",
        "\n",
        "    # # IP-Adapter pipeline\n",
        "    # transformer = FluxTransformer2DModel.from_pretrained(BASE_MODEL_ID, subfolder=\"transformer\", torch_dtype=dtype)\n",
        "    # pipe_ip = FluxPipelineIP.from_pretrained(BASE_MODEL_ID, transformer=transformer, torch_dtype=dtype).to(\"cpu\")\n",
        "    # pipe_ip.enable_model_cpu_offload()\n",
        "    # pipe_ip.enable_attention_slicing()\n",
        "    # pipe_ip.enable_vae_slicing()\n",
        "    # # pipe_ip.enable_xformers_memory_efficient_attention()\n",
        "\n",
        "    # ip_model = IPAdapter(pipe_ip, IMAGE_ENCODER_PATH, IPADAPTER_PATH, device=\"cuda\", num_tokens=128)\n",
        "\n",
        "    # controlnet_preprocessor = CannyDetector()\n",
        "    logger.info(\"✅ All models loaded and prepared on CPU.\")\n",
        "\n",
        "\n",
        "# --- Checkpoint 모델 로딩 ---\n",
        "def load_checkpoint_models():\n",
        "    global checkpoint_pipe\n",
        "    logger.info(f\"Loading Checkpoint models to SSD with dtype: {dtype}\")\n",
        "\n",
        "    # 1) CivitAI에서 받은 safetensors 체크포인트 (transformer 가중치만 포함된 파일)\n",
        "    checkpoint = \"illustration_juaner_flux.safetensors\"\n",
        "\n",
        "    # 2) FluxTransformer2DModel 만 단일 파일에서 로드\n",
        "    transformer = Checkpoint2DModel.from_single_file(\n",
        "        checkpoint,\n",
        "        torch_dtype=torch.bfloat16\n",
        "    )\n",
        "\n",
        "    # 3) 원본 Flux.1-dev의 T5 text encoder (text_encoder_2) 로드\n",
        "    text_encoder_2 = T5EncoderModel.from_pretrained(\n",
        "        \"black-forest-labs/FLUX.1-dev\",\n",
        "        subfolder=\"text_encoder_2\",\n",
        "        torch_dtype=torch.bfloat16\n",
        "    )\n",
        "\n",
        "    # 4) 파이프라인 생성 (나머지 요소는 자동으로 Flux.1-dev에서 불러옵니다)\n",
        "    checkpoint_pipe = FluxPipeline.from_pretrained(\n",
        "        \"black-forest-labs/FLUX.1-dev\",\n",
        "        transformer=transformer,\n",
        "        text_encoder_2=text_encoder_2,\n",
        "        torch_dtype=torch.bfloat16\n",
        "    )\n",
        "\n",
        "    # 5) VRAM 최적화 (선택)\n",
        "    checkpoint_pipe.enable_model_cpu_offload()\n",
        "    checkpoint_pipe.enable_attention_slicing()\n",
        "\n",
        "\n",
        "\n",
        "    logger.info(\"✅ All models loaded and prepared on CPU.\")\n",
        "\n",
        "\n",
        "# --- 서버 생명주기 ---\n",
        "@asynccontextmanager\n",
        "async def lifespan(app: FastAPI):\n",
        "    logger.info(\"App starting...\")\n",
        "    # load_models()\n",
        "    load_checkpoint_models()  # 25.05.06 - checkpoint model 사용\n",
        "\n",
        "    # ✅ Colab에서 설정한 환경변수로부터 ngrok URL 가져오기\n",
        "    public_url = os.environ.get(\"NGROK_STATIC_URL\", None)\n",
        "    app.state.ngrok_url = public_url\n",
        "    logger.info(f\"Ngrok (external) URL registered: {public_url}\")\n",
        "\n",
        "    yield\n",
        "\n",
        "    logger.info(\"App shutting down...\")\n",
        "    del base_pipe, controlnet_pipe, pipe_ip, ip_model, checkpoint_pipe\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "\n",
        "# --- FastAPI 앱 생성 ---\n",
        "app = FastAPI(lifespan=lifespan)\n",
        "\n",
        "# --- 요청 모델 ---\n",
        "class TextToImageRequest(BaseModel):\n",
        "    prompt: str\n",
        "    negative_prompt: Optional[str] = \"\"\n",
        "    num_inference_steps: int = DEFAULT_STEPS\n",
        "    guidance_scale: float = DEFAULT_GUIDANCE_SCALE\n",
        "    seed: Optional[int] = None\n",
        "\n",
        "# --- 루트 확인 ---\n",
        "@app.get(\"/\")\n",
        "async def read_root():\n",
        "    return {\n",
        "        \"message\": \"Flux Unified API is running.\",\n",
        "        \"device\": device,\n",
        "        \"ngrok_url\": app.state.ngrok_url if hasattr(app.state, \"ngrok_url\") else None,\n",
        "    }\n",
        "\n",
        "# --- 텍스트 → 이미지 ---\n",
        "@app.post(\"/generate/text-to-image\", response_class=Response)\n",
        "async def generate_text_to_image(request: TextToImageRequest):\n",
        "    if checkpoint_pipe is None: # base_pipe -> checkpoint_pipe (25.05.06)\n",
        "        raise HTTPException(status_code=503, detail=\"Checkpoint pipeline not loaded.\")\n",
        "\n",
        "    generator = get_generator(request.seed)\n",
        "\n",
        "    try:\n",
        "        with torch.inference_mode():\n",
        "            result = checkpoint_pipe(   # base_pipe -> checkpoint_pipe (25.05.06)\n",
        "                prompt=request.prompt,\n",
        "                negative_prompt=request.negative_prompt,\n",
        "                width=IMAGE_WIDTH,\n",
        "                height=IMAGE_HEIGHT,\n",
        "                num_inference_steps=request.num_inference_steps,\n",
        "                guidance_scale=request.guidance_scale,\n",
        "                generator=generator,\n",
        "            )\n",
        "            image_bytes = image_to_bytes(result.images[0])\n",
        "    finally:\n",
        "        del result\n",
        "        torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "\n",
        "    return Response(content=image_bytes, media_type=\"image/png\")\n",
        "\n",
        "# --- 이미지 → 이미지 (ControlNet) ---\n",
        "@app.post(\"/generate/image-to-image\", response_class=Response)\n",
        "async def generate_image_to_image(\n",
        "    prompt: str = Form(...),\n",
        "    negative_prompt: Optional[str] = Form(\"\"),\n",
        "    controlnet_scale: float = Form(DEFAULT_CONTROLNET_SCALE),\n",
        "    num_inference_steps: int = Form(DEFAULT_STEPS),\n",
        "    guidance_scale: float = Form(DEFAULT_GUIDANCE_SCALE),\n",
        "    seed: Optional[int] = Form(None),\n",
        "    image: UploadFile = File(...)\n",
        "):\n",
        "    if controlnet_pipe is None:\n",
        "        raise HTTPException(status_code=503, detail=\"ControlNet pipeline not ready.\")\n",
        "\n",
        "    control_image = prepare_control_image(image)\n",
        "    controlnet_pipe.to(\"cuda\")\n",
        "    generator = get_generator(seed)\n",
        "\n",
        "    try:\n",
        "        with torch.inference_mode():\n",
        "            result = controlnet_pipe(\n",
        "                prompt=prompt,\n",
        "                image=control_image,\n",
        "                width=IMAGE_WIDTH,\n",
        "                height=IMAGE_HEIGHT,\n",
        "                num_inference_steps=num_inference_steps,\n",
        "                guidance_scale=guidance_scale,\n",
        "                controlnet_conditioning_scale=controlnet_scale,\n",
        "                generator=generator,\n",
        "            )\n",
        "            image_bytes = image_to_bytes(result.images[0])\n",
        "    finally:\n",
        "        del result\n",
        "        controlnet_pipe.to(\"cpu\")\n",
        "        torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "\n",
        "    return Response(content=image_bytes, media_type=\"image/png\")\n",
        "\n",
        "# --- IP-Adapter 기반 참조 이미지 생성 ---\n",
        "@app.post(\"/generate/ip-adapter-image\", response_class=Response)\n",
        "async def generate_ip_adapter_image(\n",
        "    prompt: str = Form(...),\n",
        "    ipadapter_scale: float = Form(DEFAULT_IPADAPTER_SCALE),\n",
        "    num_inference_steps: int = Form(DEFAULT_STEPS),\n",
        "    seed: Optional[int] = Form(None),\n",
        "    image: UploadFile = File(...)\n",
        "):\n",
        "    if ip_model is None:\n",
        "        raise HTTPException(status_code=503, detail=\"IP-Adapter model not ready.\")\n",
        "\n",
        "    reference_image = await prepare_reference_image(image)\n",
        "    pipe_ip.to(\"cuda\")\n",
        "\n",
        "    try:\n",
        "        with torch.inference_mode():\n",
        "            output_images = ip_model.generate(\n",
        "                pil_image=reference_image,\n",
        "                prompt=prompt,\n",
        "                scale=ipadapter_scale,\n",
        "                width=IMAGE_WIDTH,\n",
        "                height=IMAGE_HEIGHT,\n",
        "                seed=seed,\n",
        "            )\n",
        "            image_bytes = image_to_bytes(output_images[0])\n",
        "    finally:\n",
        "        pipe_ip.to(\"cpu\")\n",
        "        torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "\n",
        "    return Response(content=image_bytes, media_type=\"image/png\")\n",
        "\n",
        "# --- 서버 실행 ---\n",
        "if __name__ == \"__main__\":\n",
        "    logger.info(\"Starting Uvicorn server...\")\n",
        "    uvicorn.run(\"main_server:app\", host=\"0.0.0.0\", port=PORT, reload=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XXvUlWb9HRcD",
        "outputId": "d3933391-b01e-488a-ea92-27b118cc0efd"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting main_server.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Fin.\n",
        "import subprocess\n",
        "from pyngrok import ngrok\n",
        "\n",
        "# ngrok 프로세스 종료\n",
        "try:\n",
        "    ngrok.kill()\n",
        "    print(\"✅ ngrok 프로세스 종료 완료\")\n",
        "except Exception as e:\n",
        "    print(f\"⚠️ ngrok 종료 중 오류 (무시 가능): {e}\")\n",
        "\n",
        "# uvicorn 서버 프로세스 종료\n",
        "subprocess.run(['pkill', '-f', 'uvicorn main_server:app'], stderr=subprocess.DEVNULL)\n",
        "subprocess.run(['pkill', '-f', 'ngrok'], stderr=subprocess.DEVNULL)\n",
        "\n",
        "import gc\n",
        "import torch\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "\n",
        "print(\"🧹 GPU 메모리 및 캐시 정리 완료\")\n",
        "\n",
        "\n",
        "!ps -ef | grep -E \"main_server.py|ngrok\" | grep -v grep\n",
        "\n",
        "\n",
        "\n",
        "import subprocess\n",
        "\n",
        "# main_server.py 종료\n",
        "subprocess.run(['pkill', '-f', 'main_server.py'])\n",
        "print(\"✅ FastAPI 서버 프로세스 종료 완료\")\n",
        "\n",
        "!ps -ef | grep -E \"main_server.py\" | grep -v grep\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "9hmI4LjHXoTF",
        "outputId": "4356a2e5-9c3d-46bf-c28f-76a89785f998"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:pyngrok.process:Killing ngrok process: 3333\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ ngrok 프로세스 종료 완료\n",
            "🧹 GPU 메모리 및 캐시 정리 완료\n",
            "root        3355       1 99 09:09 ?        00:02:22 python3 main_server.py\n",
            "✅ FastAPI 서버 프로세스 종료 완료\n",
            "root        3355       1 99 09:09 ?        00:02:22 python3 main_server.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 🚀 ngrok 연결 및 FastAPI 서버 실행\n",
        "# 환경 변수 설정\n",
        "%env PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True\n",
        "\n",
        "import time\n",
        "import os\n",
        "import subprocess\n",
        "import logging\n",
        "from pyngrok import ngrok\n",
        "\n",
        "# --- 로깅 설정 ---\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "pyngrok_logger = logging.getLogger(\"pyngrok\")\n",
        "pyngrok_logger.setLevel(logging.INFO)\n",
        "\n",
        "# --- 설정값 ---\n",
        "PORT = 8000\n",
        "LOG_FILE = \"uvicorn_server.log\"\n",
        "STATIC_NGROK_DOMAIN = \"clam-talented-promptly.ngrok-free.app\"  # 사용자 고정 도메인\n",
        "MAX_WAIT_SECONDS = 300  # 최대 대기 시간\n",
        "READY_KEYWORD = \"All models loaded\"\n",
        "\n",
        "# --- ngrok 및 서버 종료 ---\n",
        "print(\"🛠 기존 ngrok / 서버 종료 시도...\")\n",
        "try:\n",
        "    ngrok.kill()\n",
        "    print(\"✅ ngrok 종료 완료\")\n",
        "except Exception as e:\n",
        "    print(f\"⚠️ ngrok 종료 중 오류 (무시 가능): {e}\")\n",
        "\n",
        "subprocess.run(['pkill', '-f', 'uvicorn main_server:app'], stderr=subprocess.DEVNULL)\n",
        "subprocess.run(['pkill', '-f', f'ngrok.*http.*{PORT}'], stderr=subprocess.DEVNULL)\n",
        "subprocess.run(['pkill', '-f', '/root/.config/ngrok/ngrok'], stderr=subprocess.DEVNULL)\n",
        "time.sleep(3)\n",
        "\n",
        "# --- ngrok 고정 도메인 연결 ---\n",
        "print(f\"🌐 ngrok 고정 도메인 연결 시도: {STATIC_NGROK_DOMAIN}...\")\n",
        "public_url = ngrok.connect(\n",
        "    addr=PORT,\n",
        "    proto=\"http\",\n",
        "    domain=STATIC_NGROK_DOMAIN\n",
        ").public_url\n",
        "print(f\"✅ ngrok 연결 완료: {public_url}\")\n",
        "\n",
        "# ✅ ngrok URL을 환경 변수로 등록 (main_server.py에서 사용함)\n",
        "os.environ[\"NGROK_STATIC_URL\"] = public_url\n",
        "\n",
        "# --- FastAPI 서버 백그라운드 실행 ---\n",
        "print(f\"🚀 FastAPI 서버 실행 중... 로그 파일: {LOG_FILE}\")\n",
        "if os.path.exists(LOG_FILE):\n",
        "    os.remove(LOG_FILE)\n",
        "\n",
        "subprocess.Popen(\n",
        "    f\"nohup python main_server.py > {LOG_FILE} 2>&1 &\",\n",
        "    shell=True\n",
        ")\n",
        "time.sleep(5)  # 초기 부팅 대기\n",
        "\n",
        "# --- 서버 준비 대기 ---\n",
        "print(f\"⏳ 모델 로딩 대기 중... ('{READY_KEYWORD}' 감지)\")\n",
        "ready_detected = False\n",
        "\n",
        "for i in range(MAX_WAIT_SECONDS):\n",
        "    print(f\"{i+1}\", end=\" \", flush=True)\n",
        "    if (i + 1) % 30 == 0:\n",
        "        print()\n",
        "\n",
        "    if (i + 1) % 5 == 0 and os.path.exists(LOG_FILE):\n",
        "        try:\n",
        "            with open(LOG_FILE, 'r') as f:\n",
        "                if READY_KEYWORD in f.read():\n",
        "                    print(\"\\n✅ 모델 로딩 감지 완료!\")\n",
        "                    ready_detected = True\n",
        "                    break\n",
        "        except Exception as e:\n",
        "            print(f\"\\n⚠️ 로그 확인 오류: {e}\")\n",
        "\n",
        "    time.sleep(1)\n",
        "\n",
        "if not ready_detected:\n",
        "    print(\"\\n⚠️ 최대 대기 시간 초과: 모델 로딩 실패 가능성 있음\")\n",
        "\n",
        "# --- 안정화 대기 ---\n",
        "print(\"⌛ 안정화를 위해 10초 대기 중...\")\n",
        "time.sleep(10)\n",
        "\n",
        "# --- 결과 출력 ---\n",
        "print(f\"\\n🎯 서버 실행 완료! 외부 접속 URL:\")\n",
        "print(f\"🔗 {public_url}\")\n",
        "\n",
        "print(\"\\n📋 현재 실행 중인 프로세스:\")\n",
        "!ps -ef | grep -E \"main_server.py|ngrok\" | grep -v grep\n"
      ],
      "metadata": {
        "id": "oCqXd-e1uVX7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d384821d-1dbc-4c70-d67b-7b498a07eafb"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "env: PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True\n",
            "🛠 기존 ngrok / 서버 종료 시도...\n",
            "✅ ngrok 종료 완료\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:pyngrok.ngrok:Opening tunnel named: http-8000-2911904d-9052-4d8d-946d-6eea7e6d8725\n",
            "INFO:pyngrok.process.ngrok:t=2025-05-06T09:09:56+0000 lvl=info msg=\"no configuration paths supplied\"\n",
            "INFO:pyngrok.process.ngrok:t=2025-05-06T09:09:56+0000 lvl=info msg=\"using configuration at default config path\" path=/root/.config/ngrok/ngrok.yml\n",
            "INFO:pyngrok.process.ngrok:t=2025-05-06T09:09:56+0000 lvl=info msg=\"open config file\" path=/root/.config/ngrok/ngrok.yml err=nil\n",
            "INFO:pyngrok.process.ngrok:t=2025-05-06T09:09:56+0000 lvl=info msg=\"starting web service\" obj=web addr=127.0.0.1:4040 allow_hosts=[]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🌐 ngrok 고정 도메인 연결 시도: clam-talented-promptly.ngrok-free.app...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:pyngrok.process.ngrok:t=2025-05-06T09:09:57+0000 lvl=info msg=\"client session established\" obj=tunnels.session\n",
            "INFO:pyngrok.process.ngrok:t=2025-05-06T09:09:57+0000 lvl=info msg=\"tunnel session started\" obj=tunnels.session\n",
            "INFO:pyngrok.process.ngrok:t=2025-05-06T09:09:57+0000 lvl=info msg=start pg=/api/tunnels id=0a27af6aeb1faea1\n",
            "INFO:pyngrok.process.ngrok:t=2025-05-06T09:09:57+0000 lvl=info msg=end pg=/api/tunnels id=0a27af6aeb1faea1 status=200 dur=222.079µs\n",
            "INFO:pyngrok.process.ngrok:t=2025-05-06T09:09:57+0000 lvl=info msg=start pg=/api/tunnels id=596519940276125a\n",
            "INFO:pyngrok.process.ngrok:t=2025-05-06T09:09:57+0000 lvl=info msg=end pg=/api/tunnels id=596519940276125a status=200 dur=72.933µs\n",
            "INFO:pyngrok.process.ngrok:t=2025-05-06T09:09:57+0000 lvl=info msg=start pg=/api/tunnels id=fa96f5698a97df62\n",
            "INFO:pyngrok.process.ngrok:t=2025-05-06T09:09:57+0000 lvl=info msg=\"started tunnel\" obj=tunnels name=http-8000-2911904d-9052-4d8d-946d-6eea7e6d8725 addr=http://localhost:8000 url=https://clam-talented-promptly.ngrok-free.app\n",
            "INFO:pyngrok.process.ngrok:t=2025-05-06T09:09:57+0000 lvl=info msg=end pg=/api/tunnels id=fa96f5698a97df62 status=201 dur=224.132439ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ ngrok 연결 완료: https://clam-talented-promptly.ngrok-free.app\n",
            "🚀 FastAPI 서버 실행 중... 로그 파일: uvicorn_server.log\n",
            "⏳ 모델 로딩 대기 중... ('All models loaded' 감지)\n",
            "1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 \n",
            "✅ 모델 로딩 감지 완료!\n",
            "⌛ 안정화를 위해 10초 대기 중...\n",
            "\n",
            "🎯 서버 실행 완료! 외부 접속 URL:\n",
            "🔗 https://clam-talented-promptly.ngrok-free.app\n",
            "\n",
            "📋 현재 실행 중인 프로세스:\n",
            "root        3333     485  0 09:09 ?        00:00:00 /root/.config/ngrok/ngrok start --none --log=stdout\n",
            "root        3355       1 99 09:09 ?        00:01:36 python3 main_server.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qrax2DLnOrYR",
        "outputId": "307a46aa-51c8-4dc0-b1bc-4cf8af650b39"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fri May  2 00:01:14 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  NVIDIA A100-SXM4-40GB          Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   34C    P0             55W /  400W |     541MiB /  40960MiB |      0%      Default |\n",
            "|                                         |                        |             Disabled |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title [구버전 - to('cuda') 사용 중] FastAPI 애플리케이션 코드 작성 (main_server.py 파일 생성)\n",
        "'''\n",
        "# 이 셀은 FastAPI 서버 코드를 main_server.py 파일로 저장합니다.\n",
        "%%writefile main_server.py\n",
        "import os\n",
        "import io\n",
        "import logging\n",
        "import gc\n",
        "from contextlib import asynccontextmanager\n",
        "from typing import Optional, Any\n",
        "\n",
        "import torch\n",
        "import uvicorn\n",
        "from fastapi import FastAPI, HTTPException, UploadFile, File, Form\n",
        "from fastapi.responses import Response\n",
        "from pydantic import BaseModel\n",
        "from PIL import Image\n",
        "from pyngrok import ngrok\n",
        "\n",
        "# Diffusers - Flux + ControlNet\n",
        "from diffusers import FluxPipeline, FluxControlNetModel, FluxControlNetPipeline\n",
        "from controlnet_aux import CannyDetector\n",
        "\n",
        "# IP-Adapter\n",
        "from pipeline_flux_ipa import FluxPipeline as FluxPipelineIP\n",
        "from transformer_flux import FluxTransformer2DModel\n",
        "from infer_flux_ipa_siglip import resize_img, IPAdapter\n",
        "\n",
        "# --- Configuration ---\n",
        "BASE_MODEL_ID = \"black-forest-labs/FLUX.1-dev\"\n",
        "CONTROLNET_MODEL_ID = \"Shakker-Labs/FLUX.1-dev-ControlNet-Union-Pro-2.0\"\n",
        "IMAGE_ENCODER_PATH = \"google/siglip-so400m-patch14-384\"\n",
        "IPADAPTER_PATH = \"./ip-adapter.bin\"\n",
        "\n",
        "DEFAULT_STEPS = 30\n",
        "DEFAULT_GUIDANCE_SCALE = 3.5\n",
        "DEFAULT_CONTROLNET_SCALE = 0.7\n",
        "DEFAULT_IPADAPTER_SCALE = 0.7\n",
        "IMAGE_WIDTH = 1024\n",
        "IMAGE_HEIGHT = 1024\n",
        "PORT = 8000\n",
        "\n",
        "# --- Global State ---\n",
        "base_pipe: Optional[FluxPipeline] = None\n",
        "controlnet_pipe: Optional[FluxControlNetPipeline] = None\n",
        "controlnet_preprocessor: Optional[Any] = None\n",
        "pipe_ip: Optional[FluxPipelineIP] = None\n",
        "ip_model: Optional[IPAdapter] = None\n",
        "device: Optional[str] = None\n",
        "\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# --- Helper Functions ---\n",
        "def load_pil_image(image_bytes: bytes) -> Image.Image:\n",
        "    return Image.open(io.BytesIO(image_bytes)).convert(\"RGB\")\n",
        "\n",
        "def image_to_bytes(image: Image.Image) -> bytes:\n",
        "    byte_arr = io.BytesIO()\n",
        "    image.save(byte_arr, format='PNG')\n",
        "    byte_arr.seek(0)\n",
        "    return byte_arr.getvalue()\n",
        "\n",
        "def get_generator(seed: Optional[int] = None) -> torch.Generator:\n",
        "    if seed is None:\n",
        "        seed = torch.randint(0, 2**32 - 1, (1,)).item()\n",
        "    logger.info(f\"Using seed: {seed}\")\n",
        "    return torch.Generator(device=device).manual_seed(seed)\n",
        "\n",
        "async def prepare_reference_image(uploaded_image: UploadFile) -> Image.Image:\n",
        "    image = load_pil_image(await uploaded_image.read())\n",
        "    return resize_img(image)\n",
        "\n",
        "def prepare_control_image(uploaded_image: UploadFile) -> Image.Image:\n",
        "    if controlnet_preprocessor is None:\n",
        "        raise RuntimeError(\"ControlNet preprocessor not loaded.\")\n",
        "    image = load_pil_image(uploaded_image.file.read())\n",
        "    control_image = controlnet_preprocessor(image)\n",
        "    return control_image\n",
        "\n",
        "# --- Model Loading ---\n",
        "def load_models():\n",
        "    global base_pipe, controlnet_pipe, controlnet_preprocessor, pipe_ip, ip_model, device\n",
        "\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    logger.info(f\"Using device: {device}\")\n",
        "    dtype = torch.bfloat16 if device == \"cuda\" and torch.cuda.is_bf16_supported() else torch.float16\n",
        "\n",
        "    colab_log_flag = 'O'\n",
        "    try:\n",
        "        logger.info(\"Loading Flux base model...\")\n",
        "        base_pipe = FluxPipeline.from_pretrained(BASE_MODEL_ID, torch_dtype=dtype)\n",
        "        base_pipe.to(\"cpu\")\n",
        "        base_pipe.enable_model_cpu_offload()\n",
        "        base_pipe.enable_attention_slicing()\n",
        "\n",
        "        logger.info(\"Loading Flux ControlNet model...\")\n",
        "        controlnet_model = FluxControlNetModel.from_pretrained(CONTROLNET_MODEL_ID, torch_dtype=dtype)\n",
        "        controlnet_pipe = FluxControlNetPipeline.from_pretrained(BASE_MODEL_ID, controlnet=controlnet_model, torch_dtype=dtype)\n",
        "        controlnet_pipe.to(\"cpu\")\n",
        "        controlnet_pipe.enable_model_cpu_offload()\n",
        "        controlnet_pipe.enable_attention_slicing()\n",
        "\n",
        "        logger.info(\"Loading Canny Preprocessor...\")\n",
        "        controlnet_preprocessor = CannyDetector()\n",
        "\n",
        "        logger.info(\"Loading Flux IP-Adapter model...\")\n",
        "        transformer = FluxTransformer2DModel.from_pretrained(BASE_MODEL_ID, subfolder=\"transformer\", torch_dtype=dtype)\n",
        "        pipe_ip = FluxPipelineIP.from_pretrained(BASE_MODEL_ID, transformer=transformer, torch_dtype=dtype)\n",
        "        ip_model = IPAdapter(\n",
        "            pipe_ip,\n",
        "            IMAGE_ENCODER_PATH,\n",
        "            IPADAPTER_PATH,\n",
        "            device=device,\n",
        "            num_tokens=128\n",
        "        )\n",
        "        pipe_ip.to(\"cpu\")\n",
        "        pipe_ip.enable_model_cpu_offload()\n",
        "        pipe_ip.enable_attention_slicing()\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.exception(\"Fatal error during model loading\")\n",
        "        colab_log_flag = 'X'\n",
        "        raise RuntimeError(f\"Failed to load models: {e}\")\n",
        "\n",
        "    logger.info(\"✅ All models loaded.\")\n",
        "    print(colab_log_flag)\n",
        "\n",
        "# --- FastAPI Setup ---\n",
        "@asynccontextmanager\n",
        "async def lifespan(app: FastAPI):\n",
        "    logger.info(\"Application startup...\")\n",
        "    load_models()\n",
        "    ngrok_auth_token = os.environ.get(\"NGROK_AUTHTOKEN\")\n",
        "    if ngrok_auth_token:\n",
        "        public_url = ngrok.connect(PORT, \"http\")\n",
        "        logger.info(f\"Ngrok tunnel active at: {public_url}\")\n",
        "        app.state.ngrok_url = public_url\n",
        "    else:\n",
        "        app.state.ngrok_url = None\n",
        "    yield\n",
        "    logger.info(\"Application shutdown...\")\n",
        "    ngrok.kill()\n",
        "    global base_pipe, controlnet_pipe, pipe_ip\n",
        "    del base_pipe, controlnet_pipe, pipe_ip\n",
        "    if device == \"cuda\":\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "app = FastAPI(lifespan=lifespan, title=\"Flux Unified API\")\n",
        "\n",
        "# --- Pydantic Model ---\n",
        "class TextToImageRequest(BaseModel):\n",
        "    prompt: str\n",
        "    negative_prompt: Optional[str] = \"\"\n",
        "    num_inference_steps: int = DEFAULT_STEPS\n",
        "    guidance_scale: float = DEFAULT_GUIDANCE_SCALE\n",
        "    seed: Optional[int] = None\n",
        "\n",
        "# --- API Endpoints ---\n",
        "@app.get(\"/\")\n",
        "async def read_root():\n",
        "    return {\n",
        "        \"message\": \"Flux Unified API running.\",\n",
        "        \"device\": device,\n",
        "        \"ngrok_url\": app.state.ngrok_url if hasattr(app.state, \"ngrok_url\") else None\n",
        "    }\n",
        "\n",
        "@app.post(\"/generate/text-to-image\", response_class=Response)\n",
        "async def generate_text_to_image(request: TextToImageRequest):\n",
        "    if base_pipe is None:\n",
        "        raise HTTPException(status_code=503, detail=\"Base pipeline not ready.\")\n",
        "    base_pipe.to(\"cuda\")\n",
        "    generator = get_generator(request.seed)\n",
        "\n",
        "    try:\n",
        "        with torch.inference_mode():\n",
        "            result = base_pipe(\n",
        "                prompt=request.prompt,\n",
        "                negative_prompt=request.negative_prompt,\n",
        "                width=IMAGE_WIDTH,\n",
        "                height=IMAGE_HEIGHT,\n",
        "                num_inference_steps=request.num_inference_steps,\n",
        "                guidance_scale=request.guidance_scale,\n",
        "                generator=generator,\n",
        "            )\n",
        "        output_image = result.images[0]\n",
        "        img_bytes = image_to_bytes(output_image)\n",
        "        return Response(content=img_bytes, media_type=\"image/png\")\n",
        "    finally:\n",
        "        base_pipe.to(\"cpu\")\n",
        "        torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "\n",
        "@app.post(\"/generate/image-to-image\", response_class=Response)\n",
        "async def generate_image_to_image(\n",
        "    prompt: str = Form(...),\n",
        "    negative_prompt: Optional[str] = Form(\"\"),\n",
        "    controlnet_scale: float = Form(DEFAULT_CONTROLNET_SCALE),\n",
        "    num_inference_steps: int = Form(DEFAULT_STEPS),\n",
        "    guidance_scale: float = Form(DEFAULT_GUIDANCE_SCALE),\n",
        "    seed: Optional[int] = Form(None),\n",
        "    image: UploadFile = File(...)\n",
        "):\n",
        "    if controlnet_pipe is None:\n",
        "        raise HTTPException(status_code=503, detail=\"ControlNet pipeline not ready.\")\n",
        "\n",
        "    controlnet_pipe.to(\"cuda\")\n",
        "    control_image = prepare_control_image(image)\n",
        "    generator = get_generator(seed)\n",
        "\n",
        "    try:\n",
        "        with torch.inference_mode():\n",
        "            result = controlnet_pipe(\n",
        "                prompt=prompt,\n",
        "                image=control_image,\n",
        "                width=IMAGE_WIDTH,\n",
        "                height=IMAGE_HEIGHT,\n",
        "                num_inference_steps=num_inference_steps,\n",
        "                guidance_scale=guidance_scale,\n",
        "                controlnet_conditioning_scale=controlnet_scale,\n",
        "                generator=generator,\n",
        "            )\n",
        "        output_image = result.images[0]\n",
        "        img_bytes = image_to_bytes(output_image)\n",
        "        return Response(content=img_bytes, media_type=\"image/png\")\n",
        "    finally:\n",
        "        controlnet_pipe.to(\"cpu\")\n",
        "        torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "\n",
        "@app.post(\"/generate/ip-adapter-image\", response_class=Response)\n",
        "async def generate_ip_adapter_image(\n",
        "    prompt: str = Form(...),\n",
        "    ipadapter_scale: float = Form(DEFAULT_IPADAPTER_SCALE),\n",
        "    num_inference_steps: int = Form(DEFAULT_STEPS),\n",
        "    seed: Optional[int] = Form(None),\n",
        "    image: UploadFile = File(...)\n",
        "):\n",
        "    if ip_model is None:\n",
        "        raise HTTPException(status_code=503, detail=\"IP-Adapter not ready.\")\n",
        "\n",
        "    pipe_ip.to(\"cuda\")\n",
        "    reference_image = await prepare_reference_image(image)\n",
        "\n",
        "    try:\n",
        "        with torch.inference_mode():\n",
        "            output_images = ip_model.generate(\n",
        "                pil_image=reference_image,\n",
        "                prompt=prompt,\n",
        "                scale=ipadapter_scale,\n",
        "                width=IMAGE_WIDTH,\n",
        "                height=IMAGE_HEIGHT,\n",
        "                seed=seed,\n",
        "            )\n",
        "        output_image = output_images[0]\n",
        "        img_bytes = image_to_bytes(output_image)\n",
        "        return Response(content=img_bytes, media_type=\"image/png\")\n",
        "    finally:\n",
        "        pipe_ip.to(\"cpu\")\n",
        "        torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "\n",
        "# --- Main ---\n",
        "if __name__ == \"__main__\":\n",
        "    logger.info(\"Starting Uvicorn server...\")\n",
        "    uvicorn.run(\"main_server:app\", host=\"0.0.0.0\", port=PORT, reload=False)\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        },
        "id": "QGXTSJyYllKv",
        "outputId": "6f20acd9-9490-4079-fe85-562ecff68553"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n# 이 셀은 FastAPI 서버 코드를 main_server.py 파일로 저장합니다.\\n%%writefile main_server.py\\nimport os\\nimport io\\nimport logging\\nimport gc\\nfrom contextlib import asynccontextmanager\\nfrom typing import Optional, Any\\n\\nimport torch\\nimport uvicorn\\nfrom fastapi import FastAPI, HTTPException, UploadFile, File, Form\\nfrom fastapi.responses import Response\\nfrom pydantic import BaseModel\\nfrom PIL import Image\\nfrom pyngrok import ngrok\\n\\n# Diffusers - Flux + ControlNet\\nfrom diffusers import FluxPipeline, FluxControlNetModel, FluxControlNetPipeline\\nfrom controlnet_aux import CannyDetector\\n\\n# IP-Adapter\\nfrom pipeline_flux_ipa import FluxPipeline as FluxPipelineIP\\nfrom transformer_flux import FluxTransformer2DModel\\nfrom infer_flux_ipa_siglip import resize_img, IPAdapter\\n\\n# --- Configuration ---\\nBASE_MODEL_ID = \"black-forest-labs/FLUX.1-dev\"\\nCONTROLNET_MODEL_ID = \"Shakker-Labs/FLUX.1-dev-ControlNet-Union-Pro-2.0\"\\nIMAGE_ENCODER_PATH = \"google/siglip-so400m-patch14-384\"\\nIPADAPTER_PATH = \"./ip-adapter.bin\"\\n\\nDEFAULT_STEPS = 30\\nDEFAULT_GUIDANCE_SCALE = 3.5\\nDEFAULT_CONTROLNET_SCALE = 0.7\\nDEFAULT_IPADAPTER_SCALE = 0.7\\nIMAGE_WIDTH = 1024\\nIMAGE_HEIGHT = 1024\\nPORT = 8000\\n\\n# --- Global State ---\\nbase_pipe: Optional[FluxPipeline] = None\\ncontrolnet_pipe: Optional[FluxControlNetPipeline] = None\\ncontrolnet_preprocessor: Optional[Any] = None\\npipe_ip: Optional[FluxPipelineIP] = None\\nip_model: Optional[IPAdapter] = None\\ndevice: Optional[str] = None\\n\\nlogging.basicConfig(level=logging.INFO)\\nlogger = logging.getLogger(__name__)\\n\\n# --- Helper Functions ---\\ndef load_pil_image(image_bytes: bytes) -> Image.Image:\\n    return Image.open(io.BytesIO(image_bytes)).convert(\"RGB\")\\n\\ndef image_to_bytes(image: Image.Image) -> bytes:\\n    byte_arr = io.BytesIO()\\n    image.save(byte_arr, format=\\'PNG\\')\\n    byte_arr.seek(0)\\n    return byte_arr.getvalue()\\n\\ndef get_generator(seed: Optional[int] = None) -> torch.Generator:\\n    if seed is None:\\n        seed = torch.randint(0, 2**32 - 1, (1,)).item()\\n    logger.info(f\"Using seed: {seed}\")\\n    return torch.Generator(device=device).manual_seed(seed)\\n\\nasync def prepare_reference_image(uploaded_image: UploadFile) -> Image.Image:\\n    image = load_pil_image(await uploaded_image.read())\\n    return resize_img(image)\\n\\ndef prepare_control_image(uploaded_image: UploadFile) -> Image.Image:\\n    if controlnet_preprocessor is None:\\n        raise RuntimeError(\"ControlNet preprocessor not loaded.\")\\n    image = load_pil_image(uploaded_image.file.read())\\n    control_image = controlnet_preprocessor(image)\\n    return control_image\\n\\n# --- Model Loading ---\\ndef load_models():\\n    global base_pipe, controlnet_pipe, controlnet_preprocessor, pipe_ip, ip_model, device\\n\\n    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\\n    logger.info(f\"Using device: {device}\")\\n    dtype = torch.bfloat16 if device == \"cuda\" and torch.cuda.is_bf16_supported() else torch.float16\\n\\n    colab_log_flag = \\'O\\'\\n    try:\\n        logger.info(\"Loading Flux base model...\")\\n        base_pipe = FluxPipeline.from_pretrained(BASE_MODEL_ID, torch_dtype=dtype)\\n        base_pipe.to(\"cpu\")\\n        base_pipe.enable_model_cpu_offload()\\n        base_pipe.enable_attention_slicing()\\n\\n        logger.info(\"Loading Flux ControlNet model...\")\\n        controlnet_model = FluxControlNetModel.from_pretrained(CONTROLNET_MODEL_ID, torch_dtype=dtype)\\n        controlnet_pipe = FluxControlNetPipeline.from_pretrained(BASE_MODEL_ID, controlnet=controlnet_model, torch_dtype=dtype)\\n        controlnet_pipe.to(\"cpu\")\\n        controlnet_pipe.enable_model_cpu_offload()\\n        controlnet_pipe.enable_attention_slicing()\\n\\n        logger.info(\"Loading Canny Preprocessor...\")\\n        controlnet_preprocessor = CannyDetector()\\n\\n        logger.info(\"Loading Flux IP-Adapter model...\")\\n        transformer = FluxTransformer2DModel.from_pretrained(BASE_MODEL_ID, subfolder=\"transformer\", torch_dtype=dtype)\\n        pipe_ip = FluxPipelineIP.from_pretrained(BASE_MODEL_ID, transformer=transformer, torch_dtype=dtype)\\n        ip_model = IPAdapter(\\n            pipe_ip,\\n            IMAGE_ENCODER_PATH,\\n            IPADAPTER_PATH,\\n            device=device,\\n            num_tokens=128\\n        )\\n        pipe_ip.to(\"cpu\")\\n        pipe_ip.enable_model_cpu_offload()\\n        pipe_ip.enable_attention_slicing()\\n\\n    except Exception as e:\\n        logger.exception(\"Fatal error during model loading\")\\n        colab_log_flag = \\'X\\'\\n        raise RuntimeError(f\"Failed to load models: {e}\")\\n\\n    logger.info(\"✅ All models loaded.\")\\n    print(colab_log_flag)\\n\\n# --- FastAPI Setup ---\\n@asynccontextmanager\\nasync def lifespan(app: FastAPI):\\n    logger.info(\"Application startup...\")\\n    load_models()\\n    ngrok_auth_token = os.environ.get(\"NGROK_AUTHTOKEN\")\\n    if ngrok_auth_token:\\n        public_url = ngrok.connect(PORT, \"http\")\\n        logger.info(f\"Ngrok tunnel active at: {public_url}\")\\n        app.state.ngrok_url = public_url\\n    else:\\n        app.state.ngrok_url = None\\n    yield\\n    logger.info(\"Application shutdown...\")\\n    ngrok.kill()\\n    global base_pipe, controlnet_pipe, pipe_ip\\n    del base_pipe, controlnet_pipe, pipe_ip\\n    if device == \"cuda\":\\n        torch.cuda.empty_cache()\\n\\napp = FastAPI(lifespan=lifespan, title=\"Flux Unified API\")\\n\\n# --- Pydantic Model ---\\nclass TextToImageRequest(BaseModel):\\n    prompt: str\\n    negative_prompt: Optional[str] = \"\"\\n    num_inference_steps: int = DEFAULT_STEPS\\n    guidance_scale: float = DEFAULT_GUIDANCE_SCALE\\n    seed: Optional[int] = None\\n\\n# --- API Endpoints ---\\n@app.get(\"/\")\\nasync def read_root():\\n    return {\\n        \"message\": \"Flux Unified API running.\",\\n        \"device\": device,\\n        \"ngrok_url\": app.state.ngrok_url if hasattr(app.state, \"ngrok_url\") else None\\n    }\\n\\n@app.post(\"/generate/text-to-image\", response_class=Response)\\nasync def generate_text_to_image(request: TextToImageRequest):\\n    if base_pipe is None:\\n        raise HTTPException(status_code=503, detail=\"Base pipeline not ready.\")\\n    base_pipe.to(\"cuda\")\\n    generator = get_generator(request.seed)\\n\\n    try:\\n        with torch.inference_mode():\\n            result = base_pipe(\\n                prompt=request.prompt,\\n                negative_prompt=request.negative_prompt,\\n                width=IMAGE_WIDTH,\\n                height=IMAGE_HEIGHT,\\n                num_inference_steps=request.num_inference_steps,\\n                guidance_scale=request.guidance_scale,\\n                generator=generator,\\n            )\\n        output_image = result.images[0]\\n        img_bytes = image_to_bytes(output_image)\\n        return Response(content=img_bytes, media_type=\"image/png\")\\n    finally:\\n        base_pipe.to(\"cpu\")\\n        torch.cuda.empty_cache()\\n        gc.collect()\\n\\n@app.post(\"/generate/image-to-image\", response_class=Response)\\nasync def generate_image_to_image(\\n    prompt: str = Form(...),\\n    negative_prompt: Optional[str] = Form(\"\"),\\n    controlnet_scale: float = Form(DEFAULT_CONTROLNET_SCALE),\\n    num_inference_steps: int = Form(DEFAULT_STEPS),\\n    guidance_scale: float = Form(DEFAULT_GUIDANCE_SCALE),\\n    seed: Optional[int] = Form(None),\\n    image: UploadFile = File(...)\\n):\\n    if controlnet_pipe is None:\\n        raise HTTPException(status_code=503, detail=\"ControlNet pipeline not ready.\")\\n\\n    controlnet_pipe.to(\"cuda\")\\n    control_image = prepare_control_image(image)\\n    generator = get_generator(seed)\\n\\n    try:\\n        with torch.inference_mode():\\n            result = controlnet_pipe(\\n                prompt=prompt,\\n                image=control_image,\\n                width=IMAGE_WIDTH,\\n                height=IMAGE_HEIGHT,\\n                num_inference_steps=num_inference_steps,\\n                guidance_scale=guidance_scale,\\n                controlnet_conditioning_scale=controlnet_scale,\\n                generator=generator,\\n            )\\n        output_image = result.images[0]\\n        img_bytes = image_to_bytes(output_image)\\n        return Response(content=img_bytes, media_type=\"image/png\")\\n    finally:\\n        controlnet_pipe.to(\"cpu\")\\n        torch.cuda.empty_cache()\\n        gc.collect()\\n\\n@app.post(\"/generate/ip-adapter-image\", response_class=Response)\\nasync def generate_ip_adapter_image(\\n    prompt: str = Form(...),\\n    ipadapter_scale: float = Form(DEFAULT_IPADAPTER_SCALE),\\n    num_inference_steps: int = Form(DEFAULT_STEPS),\\n    seed: Optional[int] = Form(None),\\n    image: UploadFile = File(...)\\n):\\n    if ip_model is None:\\n        raise HTTPException(status_code=503, detail=\"IP-Adapter not ready.\")\\n\\n    pipe_ip.to(\"cuda\")\\n    reference_image = await prepare_reference_image(image)\\n\\n    try:\\n        with torch.inference_mode():\\n            output_images = ip_model.generate(\\n                pil_image=reference_image,\\n                prompt=prompt,\\n                scale=ipadapter_scale,\\n                width=IMAGE_WIDTH,\\n                height=IMAGE_HEIGHT,\\n                seed=seed,\\n            )\\n        output_image = output_images[0]\\n        img_bytes = image_to_bytes(output_image)\\n        return Response(content=img_bytes, media_type=\"image/png\")\\n    finally:\\n        pipe_ip.to(\"cpu\")\\n        torch.cuda.empty_cache()\\n        gc.collect()\\n\\n# --- Main ---\\nif __name__ == \"__main__\":\\n    logger.info(\"Starting Uvicorn server...\")\\n    uvicorn.run(\"main_server:app\", host=\"0.0.0.0\", port=PORT, reload=False)\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    }
  ]
}