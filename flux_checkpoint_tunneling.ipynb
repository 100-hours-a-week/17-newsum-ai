{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "# @title 1. ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜\n",
        "%pip install -q fastapi uvicorn[standard]\n",
        "%pip install -q pyngrok>=7.0.0 # diffusers>=0.27.0 transformers accelerate\n",
        "%pip install -q controlnet_aux Pillow # torch>=2.0.0\n",
        "%pip install -q safetensors pydantic huggingface_hub python-multipart\n",
        "# %pip install -U diffusers transformers accelerate safetensors\n",
        "%pip install -q wget\n",
        "# %pip install -q peft requests\n",
        "\n",
        "# # xformers ì‚¬ìš© ì‹œ (ë²„ì „ í˜¸í™˜ í™•ì¸ í•„ìš”)\n",
        "# %pip install -q fastapi uvicorn[standard] pyngrok>=7.0.0  # 1) FastAPI / ì„œë²„ìš©\n",
        "# %pip install -q diffusers==0.33.1 transformers==4.51.3 accelerate==1.6.0  # 2) Diffusers + Transformers + Accelerate\n",
        "# %pip install -q controlnet_aux Pillow safetensors pydantic huggingface_hub python-multipart # 3) Fluxìš© ë¶€ê°€ ê¸°ëŠ¥ë“¤\n",
        "# %pip install -q ninja wget  # 4) ê¸°íƒ€ ë„êµ¬\n",
        "\n",
        "# %pip uninstall -y xformers  # 5) ê¸°ì¡´ xformers ì œê±°\n",
        "# %pip install -q git+https://github.com/facebookresearch/xformers.git  # 6) Colab(PyTorch 2.6.0+cu124) í™˜ê²½ì— ë§žì¶° xFormersë¥¼ ì†ŒìŠ¤ì—ì„œ ë¹Œë“œ ì„¤ì¹˜\n",
        "# # %pip install -q peft requests\n",
        "\n",
        "print(\"âœ… ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜ ì™„ë£Œ\")"
      ],
      "metadata": {
        "id": "w-k2065nuVgM"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ë¼ì´ë¸ŒëŸ¬ë¦¬ í˜¸í™˜ì„± í™•ì¸\n",
        "# jediëŠ” ìƒê´€ ì—†ìŒ (colab íŠ¹ì„± ìƒ, ì„¤ì¹˜ ì•ˆí•œë‹¤ê³  í•¨)\n",
        "!pip check"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7AY8e66XRJzV",
        "outputId": "d3b77356-9b5e-480e-9926-fdf245fa3d10"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ipython 7.34.0 requires jedi, which is not installed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title InstantX ì½”ë“œ íŒŒì¼ ë‹¤ìš´ë¡œë“œ\n",
        "import wget\n",
        "import os\n",
        "\n",
        "# Flux InstantX íŒŒì¼ ë¦¬ìŠ¤íŠ¸\n",
        "files = [\n",
        "    (\"pipeline_flux_ipa.py\", \"https://huggingface.co/InstantX/FLUX.1-dev-IP-Adapter/resolve/main/pipeline_flux_ipa.py?download=true\"),\n",
        "    (\"transformer_flux.py\", \"https://huggingface.co/InstantX/FLUX.1-dev-IP-Adapter/resolve/main/transformer_flux.py?download=true\"),\n",
        "    (\"attention_processor.py\", \"https://huggingface.co/InstantX/FLUX.1-dev-IP-Adapter/resolve/main/attention_processor.py?download=true\"),\n",
        "    (\"infer_flux_ipa_siglip.py\", \"https://huggingface.co/InstantX/FLUX.1-dev-IP-Adapter/resolve/main/infer_flux_ipa_siglip.py?download=true\"),\n",
        "    (\"ip-adapter.bin\", \"https://huggingface.co/InstantX/FLUX.1-dev-IP-Adapter/resolve/main/ip-adapter.bin?download=true\"),\n",
        "]\n",
        "\n",
        "for filename, url in files:\n",
        "    if not os.path.exists(filename):\n",
        "        wget.download(url, out=filename)\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "_ntweVQSd75c"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 2. Hugging Face ë¡œê·¸ì¸ / Ngrok ì„¤ì • (Authtoken ì‹œí¬ë¦¿ í‚¤)\n",
        "import os\n",
        "from google.colab import userdata\n",
        "from huggingface_hub import login\n",
        "from pyngrok import conf, ngrok\n",
        "\n",
        "# Hugging Face ë° ngrok í† í° (Colab Secretì—ì„œ ê°€ì ¸ì˜¤ê¸°)\n",
        "hf_token = userdata.get(\"HF_TOKEN\")        # Hugging Face Token\n",
        "civitai_token = userdata.get(\"CIVITAI_TOKEN\")   # CivitAI Token\n",
        "ngrok_token = userdata.get(\"google_ngrok_authtoken\")  # ngrok Token\n",
        "\n",
        "if hf_token == None:\n",
        "  print('x')\n",
        "if civitai_token == None:\n",
        "  print('x')\n",
        "if ngrok_token == None:\n",
        "  print('x')\n",
        "\n",
        "# í¬íŠ¸ ì„¤ì •\n",
        "PORT = 8000\n",
        "\n",
        "# hugging face ë¡œê·¸ì¸\n",
        "print(\"ðŸ”‘ Hugging Face ë¡œê·¸ì¸ ì¤‘...\")\n",
        "login(token=hf_token)\n",
        "\n",
        "# ngrok ì„¤ì •\n",
        "ngrok.set_auth_token(ngrok_token)\n",
        "print(\"âœ… Ngrok Authtoken ì„¤ì • ì™„ë£Œ\")"
      ],
      "metadata": {
        "id": "A0jI1SIruVeF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "96eb4d8b-9581-4112-b66d-10fdd8d92971"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ”‘ Hugging Face ë¡œê·¸ì¸ ì¤‘...\n",
            "âœ… Ngrok Authtoken ì„¤ì • ì™„ë£Œ\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 3. CivitAI Checkpoint ë‹¤ìš´ë¡œë“œ\n",
        "import os\n",
        "os.environ[\"CIVITAI_API_TOKEN\"] = civitai_token\n",
        "\n",
        "import wget\n",
        "!wget -O illustration_juaner_flux.safetensors \\\n",
        "\"https://civitai.com/api/download/models/1215918?type=Model&format=SafeTensor&size=full&token=$CIVITAI_API_TOKEN\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2R0izjLyvHfL",
        "outputId": "193ceb19-413a-4036-f33d-ff046e555ed2"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-05-06 09:02:12--  https://civitai.com/api/download/models/1215918?type=Model&format=SafeTensor&size=full&token=2ab1653b9d579f186d295f4540908157\n",
            "Resolving civitai.com (civitai.com)... 104.22.18.237, 172.67.12.143, 104.22.19.237, ...\n",
            "Connecting to civitai.com (civitai.com)|104.22.18.237|:443... connected.\n",
            "HTTP request sent, awaiting response... 307 Temporary Redirect\n",
            "Location: https://civitai-delivery-worker-prod.5ac0637cfd0766c97916cefa3764fbdf.r2.cloudflarestorage.com/model/686417/jGhibliV2FluxUltimate.FZyi.safetensors?X-Amz-Expires=86400&response-content-disposition=attachment%3B%20filename%3D%22IllustrationJuanerGhibli_v20.safetensors%22&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=e01358d793ad6966166af8b3064953ad/20250506/us-east-1/s3/aws4_request&X-Amz-Date=20250506T090212Z&X-Amz-SignedHeaders=host&X-Amz-Signature=bff7537dc4e114c059b9336cf8c2e5fd41338bb911a859f96495f4500a6e1ade [following]\n",
            "--2025-05-06 09:02:12--  https://civitai-delivery-worker-prod.5ac0637cfd0766c97916cefa3764fbdf.r2.cloudflarestorage.com/model/686417/jGhibliV2FluxUltimate.FZyi.safetensors?X-Amz-Expires=86400&response-content-disposition=attachment%3B%20filename%3D%22IllustrationJuanerGhibli_v20.safetensors%22&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=e01358d793ad6966166af8b3064953ad/20250506/us-east-1/s3/aws4_request&X-Amz-Date=20250506T090212Z&X-Amz-SignedHeaders=host&X-Amz-Signature=bff7537dc4e114c059b9336cf8c2e5fd41338bb911a859f96495f4500a6e1ade\n",
            "Resolving civitai-delivery-worker-prod.5ac0637cfd0766c97916cefa3764fbdf.r2.cloudflarestorage.com (civitai-delivery-worker-prod.5ac0637cfd0766c97916cefa3764fbdf.r2.cloudflarestorage.com)... 162.159.141.50, 172.66.1.46, 2606:4700:7::12e, ...\n",
            "Connecting to civitai-delivery-worker-prod.5ac0637cfd0766c97916cefa3764fbdf.r2.cloudflarestorage.com (civitai-delivery-worker-prod.5ac0637cfd0766c97916cefa3764fbdf.r2.cloudflarestorage.com)|162.159.141.50|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 11901497256 (11G)\n",
            "Saving to: â€˜illustration_juaner_flux.safetensorsâ€™\n",
            "\n",
            "illustration_juaner 100%[===================>]  11.08G  89.9MB/s    in 2m 7s   \n",
            "\n",
            "2025-05-06 09:04:20 (89.4 MB/s) - â€˜illustration_juaner_flux.safetensorsâ€™ saved [11901497256/11901497256]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 4. FastAPI ì• í”Œë¦¬ì¼€ì´ì…˜ ì½”ë“œ ìž‘ì„± (main_server.py íŒŒì¼ ìƒì„±)\n",
        "# ì´ ì…€ì€ FastAPI ì„œë²„ ì½”ë“œë¥¼ main_server.py íŒŒì¼ë¡œ ì €ìž¥í•©ë‹ˆë‹¤.\n",
        "\n",
        "%%writefile main_server.py\n",
        "import os, io, gc, logging\n",
        "from contextlib import asynccontextmanager\n",
        "from typing import Optional, Any\n",
        "\n",
        "import torch\n",
        "import uvicorn\n",
        "from fastapi import FastAPI, HTTPException, UploadFile, File, Form\n",
        "from fastapi.responses import Response\n",
        "from pydantic import BaseModel\n",
        "from PIL import Image\n",
        "from pyngrok import ngrok\n",
        "\n",
        "# Flux ê´€ë ¨ import\n",
        "from diffusers import FluxPipeline, FluxControlNetModel, FluxControlNetPipeline\n",
        "from diffusers import FluxTransformer2DModel as Checkpoint2DModel\n",
        "from transformers import T5EncoderModel\n",
        "\n",
        "# IP Adapter ê´€ë ¨ import\n",
        "from controlnet_aux import CannyDetector\n",
        "from pipeline_flux_ipa import FluxPipeline as FluxPipelineIP\n",
        "from transformer_flux import FluxTransformer2DModel\n",
        "from infer_flux_ipa_siglip import resize_img, IPAdapter\n",
        "\n",
        "# --- ì„¤ì • ìƒìˆ˜ ---\n",
        "BASE_MODEL_ID = \"black-forest-labs/FLUX.1-dev\"\n",
        "CHECKPOINT_MODEL_ID = \"illustration_juaner_flux.safetensors\"\n",
        "CONTROLNET_MODEL_ID = \"Shakker-Labs/FLUX.1-dev-ControlNet-Union-Pro-2.0\"\n",
        "IMAGE_ENCODER_PATH = \"google/siglip-so400m-patch14-384\"\n",
        "IPADAPTER_PATH = \"./ip-adapter.bin\"\n",
        "\n",
        "DEFAULT_STEPS = 30\n",
        "DEFAULT_GUIDANCE_SCALE = 3.5\n",
        "DEFAULT_CONTROLNET_SCALE = 0.7\n",
        "DEFAULT_IPADAPTER_SCALE = 0.7\n",
        "IMAGE_WIDTH = 1024\n",
        "IMAGE_HEIGHT = 1024\n",
        "PORT = 8000\n",
        "\n",
        "# --- ì „ì—­ ìƒíƒœ ---\n",
        "base_pipe: Optional[FluxPipeline] = None\n",
        "checkpoint_pipe: Optional[FluxPipeline] = None\n",
        "controlnet_pipe: Optional[FluxControlNetPipeline] = None\n",
        "pipe_ip: Optional[FluxPipelineIP] = None\n",
        "ip_model: Optional[IPAdapter] = None\n",
        "controlnet_preprocessor: Optional[Any] = None\n",
        "\n",
        "device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "dtype: torch.dtype = torch.bfloat16 if (device == \"cuda\" and torch.cuda.is_bf16_supported()) else torch.float16\n",
        "\n",
        "# --- ë¡œê¹… ì„¤ì • ---\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# --- ìœ í‹¸ í•¨ìˆ˜ ---\n",
        "def load_pil_image(image_bytes: bytes) -> Image.Image:\n",
        "    return Image.open(io.BytesIO(image_bytes)).convert(\"RGB\")\n",
        "\n",
        "def image_to_bytes(image: Image.Image) -> bytes:\n",
        "    byte_arr = io.BytesIO()\n",
        "    image.save(byte_arr, format='PNG')\n",
        "    byte_arr.seek(0)\n",
        "    return byte_arr.getvalue()\n",
        "\n",
        "def get_generator(seed: Optional[int] = None) -> torch.Generator:\n",
        "    if seed is None:\n",
        "        seed = torch.randint(0, 2**32 - 1, (1,)).item()\n",
        "    logger.info(f\"Using seed: {seed}\")\n",
        "    return torch.Generator(device=\"cuda\").manual_seed(seed)\n",
        "\n",
        "async def prepare_reference_image(uploaded_image: UploadFile) -> Image.Image:\n",
        "    image = load_pil_image(await uploaded_image.read())\n",
        "    return resize_img(image)\n",
        "\n",
        "def prepare_control_image(uploaded_image: UploadFile) -> Image.Image:\n",
        "    if controlnet_preprocessor is None:\n",
        "        raise RuntimeError(\"ControlNet preprocessor not loaded.\")\n",
        "    image = load_pil_image(uploaded_image.file.read())\n",
        "    return controlnet_preprocessor(image)\n",
        "\n",
        "# --- ëª¨ë¸ ë¡œë”© ---\n",
        "def load_models():\n",
        "    global base_pipe, controlnet_pipe, pipe_ip, ip_model, controlnet_preprocessor\n",
        "\n",
        "    logger.info(f\"Loading models to CPU with dtype: {dtype}\")\n",
        "\n",
        "    '''\n",
        "    2025.05.06 - checkpoint ëª¨ë¸ë¡œ êµì²´í•˜ì—¬ ì£¼ì„ì²˜ë¦¬\n",
        "    '''\n",
        "\n",
        "    # Base pipeline\n",
        "    # base_pipe = FluxPipeline.from_pretrained(BASE_MODEL_ID, torch_dtype=dtype).to(\"cpu\")\n",
        "    # base_pipe.enable_model_cpu_offload()\n",
        "    # base_pipe.enable_attention_slicing()\n",
        "    # base_pipe.enable_vae_slicing()\n",
        "    # base_pipe.enable_xformers_memory_efficient_attention()\n",
        "\n",
        "    # # ControlNet pipeline\n",
        "    # controlnet_model = FluxControlNetModel.from_pretrained(CONTROLNET_MODEL_ID, torch_dtype=dtype)\n",
        "    # controlnet_pipe = FluxControlNetPipeline.from_pretrained(BASE_MODEL_ID, controlnet=controlnet_model, torch_dtype=dtype).to(\"cpu\")\n",
        "    # controlnet_pipe.enable_model_cpu_offload()\n",
        "    # controlnet_pipe.enable_attention_slicing()\n",
        "    # controlnet_pipe.enable_vae_slicing()\n",
        "    # # controlnet_pipe.enable_xformers_memory_efficient_attention()\n",
        "\n",
        "    # # IP-Adapter pipeline\n",
        "    # transformer = FluxTransformer2DModel.from_pretrained(BASE_MODEL_ID, subfolder=\"transformer\", torch_dtype=dtype)\n",
        "    # pipe_ip = FluxPipelineIP.from_pretrained(BASE_MODEL_ID, transformer=transformer, torch_dtype=dtype).to(\"cpu\")\n",
        "    # pipe_ip.enable_model_cpu_offload()\n",
        "    # pipe_ip.enable_attention_slicing()\n",
        "    # pipe_ip.enable_vae_slicing()\n",
        "    # # pipe_ip.enable_xformers_memory_efficient_attention()\n",
        "\n",
        "    # ip_model = IPAdapter(pipe_ip, IMAGE_ENCODER_PATH, IPADAPTER_PATH, device=\"cuda\", num_tokens=128)\n",
        "\n",
        "    # controlnet_preprocessor = CannyDetector()\n",
        "    logger.info(\"âœ… All models loaded and prepared on CPU.\")\n",
        "\n",
        "\n",
        "# --- Checkpoint ëª¨ë¸ ë¡œë”© ---\n",
        "def load_checkpoint_models():\n",
        "    global checkpoint_pipe\n",
        "    logger.info(f\"Loading Checkpoint models to SSD with dtype: {dtype}\")\n",
        "\n",
        "    # 1) CivitAIì—ì„œ ë°›ì€ safetensors ì²´í¬í¬ì¸íŠ¸ (transformer ê°€ì¤‘ì¹˜ë§Œ í¬í•¨ëœ íŒŒì¼)\n",
        "    checkpoint = \"illustration_juaner_flux.safetensors\"\n",
        "\n",
        "    # 2) FluxTransformer2DModel ë§Œ ë‹¨ì¼ íŒŒì¼ì—ì„œ ë¡œë“œ\n",
        "    transformer = Checkpoint2DModel.from_single_file(\n",
        "        checkpoint,\n",
        "        torch_dtype=torch.bfloat16\n",
        "    )\n",
        "\n",
        "    # 3) ì›ë³¸ Flux.1-devì˜ T5 text encoder (text_encoder_2) ë¡œë“œ\n",
        "    text_encoder_2 = T5EncoderModel.from_pretrained(\n",
        "        \"black-forest-labs/FLUX.1-dev\",\n",
        "        subfolder=\"text_encoder_2\",\n",
        "        torch_dtype=torch.bfloat16\n",
        "    )\n",
        "\n",
        "    # 4) íŒŒì´í”„ë¼ì¸ ìƒì„± (ë‚˜ë¨¸ì§€ ìš”ì†ŒëŠ” ìžë™ìœ¼ë¡œ Flux.1-devì—ì„œ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤)\n",
        "    checkpoint_pipe = FluxPipeline.from_pretrained(\n",
        "        \"black-forest-labs/FLUX.1-dev\",\n",
        "        transformer=transformer,\n",
        "        text_encoder_2=text_encoder_2,\n",
        "        torch_dtype=torch.bfloat16\n",
        "    )\n",
        "\n",
        "    # 5) VRAM ìµœì í™” (ì„ íƒ)\n",
        "    checkpoint_pipe.enable_model_cpu_offload()\n",
        "    checkpoint_pipe.enable_attention_slicing()\n",
        "\n",
        "\n",
        "\n",
        "    logger.info(\"âœ… All models loaded and prepared on CPU.\")\n",
        "\n",
        "\n",
        "# --- ì„œë²„ ìƒëª…ì£¼ê¸° ---\n",
        "@asynccontextmanager\n",
        "async def lifespan(app: FastAPI):\n",
        "    logger.info(\"App starting...\")\n",
        "    # load_models()\n",
        "    load_checkpoint_models()  # 25.05.06 - checkpoint model ì‚¬ìš©\n",
        "\n",
        "    # âœ… Colabì—ì„œ ì„¤ì •í•œ í™˜ê²½ë³€ìˆ˜ë¡œë¶€í„° ngrok URL ê°€ì ¸ì˜¤ê¸°\n",
        "    public_url = os.environ.get(\"NGROK_STATIC_URL\", None)\n",
        "    app.state.ngrok_url = public_url\n",
        "    logger.info(f\"Ngrok (external) URL registered: {public_url}\")\n",
        "\n",
        "    yield\n",
        "\n",
        "    logger.info(\"App shutting down...\")\n",
        "    del base_pipe, controlnet_pipe, pipe_ip, ip_model, checkpoint_pipe\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "\n",
        "# --- FastAPI ì•± ìƒì„± ---\n",
        "app = FastAPI(lifespan=lifespan)\n",
        "\n",
        "# --- ìš”ì²­ ëª¨ë¸ ---\n",
        "class TextToImageRequest(BaseModel):\n",
        "    prompt: str\n",
        "    negative_prompt: Optional[str] = \"\"\n",
        "    num_inference_steps: int = DEFAULT_STEPS\n",
        "    guidance_scale: float = DEFAULT_GUIDANCE_SCALE\n",
        "    seed: Optional[int] = None\n",
        "\n",
        "# --- ë£¨íŠ¸ í™•ì¸ ---\n",
        "@app.get(\"/\")\n",
        "async def read_root():\n",
        "    return {\n",
        "        \"message\": \"Flux Unified API is running.\",\n",
        "        \"device\": device,\n",
        "        \"ngrok_url\": app.state.ngrok_url if hasattr(app.state, \"ngrok_url\") else None,\n",
        "    }\n",
        "\n",
        "# --- í…ìŠ¤íŠ¸ â†’ ì´ë¯¸ì§€ ---\n",
        "@app.post(\"/generate/text-to-image\", response_class=Response)\n",
        "async def generate_text_to_image(request: TextToImageRequest):\n",
        "    if checkpoint_pipe is None: # base_pipe -> checkpoint_pipe (25.05.06)\n",
        "        raise HTTPException(status_code=503, detail=\"Checkpoint pipeline not loaded.\")\n",
        "\n",
        "    generator = get_generator(request.seed)\n",
        "\n",
        "    try:\n",
        "        with torch.inference_mode():\n",
        "            result = checkpoint_pipe(   # base_pipe -> checkpoint_pipe (25.05.06)\n",
        "                prompt=request.prompt,\n",
        "                negative_prompt=request.negative_prompt,\n",
        "                width=IMAGE_WIDTH,\n",
        "                height=IMAGE_HEIGHT,\n",
        "                num_inference_steps=request.num_inference_steps,\n",
        "                guidance_scale=request.guidance_scale,\n",
        "                generator=generator,\n",
        "            )\n",
        "            image_bytes = image_to_bytes(result.images[0])\n",
        "    finally:\n",
        "        del result\n",
        "        torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "\n",
        "    return Response(content=image_bytes, media_type=\"image/png\")\n",
        "\n",
        "# --- ì´ë¯¸ì§€ â†’ ì´ë¯¸ì§€ (ControlNet) ---\n",
        "@app.post(\"/generate/image-to-image\", response_class=Response)\n",
        "async def generate_image_to_image(\n",
        "    prompt: str = Form(...),\n",
        "    negative_prompt: Optional[str] = Form(\"\"),\n",
        "    controlnet_scale: float = Form(DEFAULT_CONTROLNET_SCALE),\n",
        "    num_inference_steps: int = Form(DEFAULT_STEPS),\n",
        "    guidance_scale: float = Form(DEFAULT_GUIDANCE_SCALE),\n",
        "    seed: Optional[int] = Form(None),\n",
        "    image: UploadFile = File(...)\n",
        "):\n",
        "    if controlnet_pipe is None:\n",
        "        raise HTTPException(status_code=503, detail=\"ControlNet pipeline not ready.\")\n",
        "\n",
        "    control_image = prepare_control_image(image)\n",
        "    controlnet_pipe.to(\"cuda\")\n",
        "    generator = get_generator(seed)\n",
        "\n",
        "    try:\n",
        "        with torch.inference_mode():\n",
        "            result = controlnet_pipe(\n",
        "                prompt=prompt,\n",
        "                image=control_image,\n",
        "                width=IMAGE_WIDTH,\n",
        "                height=IMAGE_HEIGHT,\n",
        "                num_inference_steps=num_inference_steps,\n",
        "                guidance_scale=guidance_scale,\n",
        "                controlnet_conditioning_scale=controlnet_scale,\n",
        "                generator=generator,\n",
        "            )\n",
        "            image_bytes = image_to_bytes(result.images[0])\n",
        "    finally:\n",
        "        del result\n",
        "        controlnet_pipe.to(\"cpu\")\n",
        "        torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "\n",
        "    return Response(content=image_bytes, media_type=\"image/png\")\n",
        "\n",
        "# --- IP-Adapter ê¸°ë°˜ ì°¸ì¡° ì´ë¯¸ì§€ ìƒì„± ---\n",
        "@app.post(\"/generate/ip-adapter-image\", response_class=Response)\n",
        "async def generate_ip_adapter_image(\n",
        "    prompt: str = Form(...),\n",
        "    ipadapter_scale: float = Form(DEFAULT_IPADAPTER_SCALE),\n",
        "    num_inference_steps: int = Form(DEFAULT_STEPS),\n",
        "    seed: Optional[int] = Form(None),\n",
        "    image: UploadFile = File(...)\n",
        "):\n",
        "    if ip_model is None:\n",
        "        raise HTTPException(status_code=503, detail=\"IP-Adapter model not ready.\")\n",
        "\n",
        "    reference_image = await prepare_reference_image(image)\n",
        "    pipe_ip.to(\"cuda\")\n",
        "\n",
        "    try:\n",
        "        with torch.inference_mode():\n",
        "            output_images = ip_model.generate(\n",
        "                pil_image=reference_image,\n",
        "                prompt=prompt,\n",
        "                scale=ipadapter_scale,\n",
        "                width=IMAGE_WIDTH,\n",
        "                height=IMAGE_HEIGHT,\n",
        "                seed=seed,\n",
        "            )\n",
        "            image_bytes = image_to_bytes(output_images[0])\n",
        "    finally:\n",
        "        pipe_ip.to(\"cpu\")\n",
        "        torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "\n",
        "    return Response(content=image_bytes, media_type=\"image/png\")\n",
        "\n",
        "# --- ì„œë²„ ì‹¤í–‰ ---\n",
        "if __name__ == \"__main__\":\n",
        "    logger.info(\"Starting Uvicorn server...\")\n",
        "    uvicorn.run(\"main_server:app\", host=\"0.0.0.0\", port=PORT, reload=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XXvUlWb9HRcD",
        "outputId": "d3933391-b01e-488a-ea92-27b118cc0efd"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting main_server.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Fin.\n",
        "import subprocess\n",
        "from pyngrok import ngrok\n",
        "\n",
        "# ngrok í”„ë¡œì„¸ìŠ¤ ì¢…ë£Œ\n",
        "try:\n",
        "    ngrok.kill()\n",
        "    print(\"âœ… ngrok í”„ë¡œì„¸ìŠ¤ ì¢…ë£Œ ì™„ë£Œ\")\n",
        "except Exception as e:\n",
        "    print(f\"âš ï¸ ngrok ì¢…ë£Œ ì¤‘ ì˜¤ë¥˜ (ë¬´ì‹œ ê°€ëŠ¥): {e}\")\n",
        "\n",
        "# uvicorn ì„œë²„ í”„ë¡œì„¸ìŠ¤ ì¢…ë£Œ\n",
        "subprocess.run(['pkill', '-f', 'uvicorn main_server:app'], stderr=subprocess.DEVNULL)\n",
        "subprocess.run(['pkill', '-f', 'ngrok'], stderr=subprocess.DEVNULL)\n",
        "\n",
        "import gc\n",
        "import torch\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "\n",
        "print(\"ðŸ§¹ GPU ë©”ëª¨ë¦¬ ë° ìºì‹œ ì •ë¦¬ ì™„ë£Œ\")\n",
        "\n",
        "\n",
        "!ps -ef | grep -E \"main_server.py|ngrok\" | grep -v grep\n",
        "\n",
        "\n",
        "\n",
        "import subprocess\n",
        "\n",
        "# main_server.py ì¢…ë£Œ\n",
        "subprocess.run(['pkill', '-f', 'main_server.py'])\n",
        "print(\"âœ… FastAPI ì„œë²„ í”„ë¡œì„¸ìŠ¤ ì¢…ë£Œ ì™„ë£Œ\")\n",
        "\n",
        "!ps -ef | grep -E \"main_server.py\" | grep -v grep\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "9hmI4LjHXoTF",
        "outputId": "4356a2e5-9c3d-46bf-c28f-76a89785f998"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:pyngrok.process:Killing ngrok process: 3333\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… ngrok í”„ë¡œì„¸ìŠ¤ ì¢…ë£Œ ì™„ë£Œ\n",
            "ðŸ§¹ GPU ë©”ëª¨ë¦¬ ë° ìºì‹œ ì •ë¦¬ ì™„ë£Œ\n",
            "root        3355       1 99 09:09 ?        00:02:22 python3 main_server.py\n",
            "âœ… FastAPI ì„œë²„ í”„ë¡œì„¸ìŠ¤ ì¢…ë£Œ ì™„ë£Œ\n",
            "root        3355       1 99 09:09 ?        00:02:22 python3 main_server.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title ðŸš€ ngrok ì—°ê²° ë° FastAPI ì„œë²„ ì‹¤í–‰\n",
        "# í™˜ê²½ ë³€ìˆ˜ ì„¤ì •\n",
        "%env PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True\n",
        "\n",
        "import time\n",
        "import os\n",
        "import subprocess\n",
        "import logging\n",
        "from pyngrok import ngrok\n",
        "\n",
        "# --- ë¡œê¹… ì„¤ì • ---\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "pyngrok_logger = logging.getLogger(\"pyngrok\")\n",
        "pyngrok_logger.setLevel(logging.INFO)\n",
        "\n",
        "# --- ì„¤ì •ê°’ ---\n",
        "PORT = 8000\n",
        "LOG_FILE = \"uvicorn_server.log\"\n",
        "STATIC_NGROK_DOMAIN = \"clam-talented-promptly.ngrok-free.app\"  # ì‚¬ìš©ìž ê³ ì • ë„ë©”ì¸\n",
        "MAX_WAIT_SECONDS = 300  # ìµœëŒ€ ëŒ€ê¸° ì‹œê°„\n",
        "READY_KEYWORD = \"All models loaded\"\n",
        "\n",
        "# --- ngrok ë° ì„œë²„ ì¢…ë£Œ ---\n",
        "print(\"ðŸ›  ê¸°ì¡´ ngrok / ì„œë²„ ì¢…ë£Œ ì‹œë„...\")\n",
        "try:\n",
        "    ngrok.kill()\n",
        "    print(\"âœ… ngrok ì¢…ë£Œ ì™„ë£Œ\")\n",
        "except Exception as e:\n",
        "    print(f\"âš ï¸ ngrok ì¢…ë£Œ ì¤‘ ì˜¤ë¥˜ (ë¬´ì‹œ ê°€ëŠ¥): {e}\")\n",
        "\n",
        "subprocess.run(['pkill', '-f', 'uvicorn main_server:app'], stderr=subprocess.DEVNULL)\n",
        "subprocess.run(['pkill', '-f', f'ngrok.*http.*{PORT}'], stderr=subprocess.DEVNULL)\n",
        "subprocess.run(['pkill', '-f', '/root/.config/ngrok/ngrok'], stderr=subprocess.DEVNULL)\n",
        "time.sleep(3)\n",
        "\n",
        "# --- ngrok ê³ ì • ë„ë©”ì¸ ì—°ê²° ---\n",
        "print(f\"ðŸŒ ngrok ê³ ì • ë„ë©”ì¸ ì—°ê²° ì‹œë„: {STATIC_NGROK_DOMAIN}...\")\n",
        "public_url = ngrok.connect(\n",
        "    addr=PORT,\n",
        "    proto=\"http\",\n",
        "    domain=STATIC_NGROK_DOMAIN\n",
        ").public_url\n",
        "print(f\"âœ… ngrok ì—°ê²° ì™„ë£Œ: {public_url}\")\n",
        "\n",
        "# âœ… ngrok URLì„ í™˜ê²½ ë³€ìˆ˜ë¡œ ë“±ë¡ (main_server.pyì—ì„œ ì‚¬ìš©í•¨)\n",
        "os.environ[\"NGROK_STATIC_URL\"] = public_url\n",
        "\n",
        "# --- FastAPI ì„œë²„ ë°±ê·¸ë¼ìš´ë“œ ì‹¤í–‰ ---\n",
        "print(f\"ðŸš€ FastAPI ì„œë²„ ì‹¤í–‰ ì¤‘... ë¡œê·¸ íŒŒì¼: {LOG_FILE}\")\n",
        "if os.path.exists(LOG_FILE):\n",
        "    os.remove(LOG_FILE)\n",
        "\n",
        "subprocess.Popen(\n",
        "    f\"nohup python main_server.py > {LOG_FILE} 2>&1 &\",\n",
        "    shell=True\n",
        ")\n",
        "time.sleep(5)  # ì´ˆê¸° ë¶€íŒ… ëŒ€ê¸°\n",
        "\n",
        "# --- ì„œë²„ ì¤€ë¹„ ëŒ€ê¸° ---\n",
        "print(f\"â³ ëª¨ë¸ ë¡œë”© ëŒ€ê¸° ì¤‘... ('{READY_KEYWORD}' ê°ì§€)\")\n",
        "ready_detected = False\n",
        "\n",
        "for i in range(MAX_WAIT_SECONDS):\n",
        "    print(f\"{i+1}\", end=\" \", flush=True)\n",
        "    if (i + 1) % 30 == 0:\n",
        "        print()\n",
        "\n",
        "    if (i + 1) % 5 == 0 and os.path.exists(LOG_FILE):\n",
        "        try:\n",
        "            with open(LOG_FILE, 'r') as f:\n",
        "                if READY_KEYWORD in f.read():\n",
        "                    print(\"\\nâœ… ëª¨ë¸ ë¡œë”© ê°ì§€ ì™„ë£Œ!\")\n",
        "                    ready_detected = True\n",
        "                    break\n",
        "        except Exception as e:\n",
        "            print(f\"\\nâš ï¸ ë¡œê·¸ í™•ì¸ ì˜¤ë¥˜: {e}\")\n",
        "\n",
        "    time.sleep(1)\n",
        "\n",
        "if not ready_detected:\n",
        "    print(\"\\nâš ï¸ ìµœëŒ€ ëŒ€ê¸° ì‹œê°„ ì´ˆê³¼: ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨ ê°€ëŠ¥ì„± ìžˆìŒ\")\n",
        "\n",
        "# --- ì•ˆì •í™” ëŒ€ê¸° ---\n",
        "print(\"âŒ› ì•ˆì •í™”ë¥¼ ìœ„í•´ 10ì´ˆ ëŒ€ê¸° ì¤‘...\")\n",
        "time.sleep(10)\n",
        "\n",
        "# --- ê²°ê³¼ ì¶œë ¥ ---\n",
        "print(f\"\\nðŸŽ¯ ì„œë²„ ì‹¤í–‰ ì™„ë£Œ! ì™¸ë¶€ ì ‘ì† URL:\")\n",
        "print(f\"ðŸ”— {public_url}\")\n",
        "\n",
        "print(\"\\nðŸ“‹ í˜„ìž¬ ì‹¤í–‰ ì¤‘ì¸ í”„ë¡œì„¸ìŠ¤:\")\n",
        "!ps -ef | grep -E \"main_server.py|ngrok\" | grep -v grep\n"
      ],
      "metadata": {
        "id": "oCqXd-e1uVX7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d384821d-1dbc-4c70-d67b-7b498a07eafb"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "env: PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True\n",
            "ðŸ›  ê¸°ì¡´ ngrok / ì„œë²„ ì¢…ë£Œ ì‹œë„...\n",
            "âœ… ngrok ì¢…ë£Œ ì™„ë£Œ\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:pyngrok.ngrok:Opening tunnel named: http-8000-2911904d-9052-4d8d-946d-6eea7e6d8725\n",
            "INFO:pyngrok.process.ngrok:t=2025-05-06T09:09:56+0000 lvl=info msg=\"no configuration paths supplied\"\n",
            "INFO:pyngrok.process.ngrok:t=2025-05-06T09:09:56+0000 lvl=info msg=\"using configuration at default config path\" path=/root/.config/ngrok/ngrok.yml\n",
            "INFO:pyngrok.process.ngrok:t=2025-05-06T09:09:56+0000 lvl=info msg=\"open config file\" path=/root/.config/ngrok/ngrok.yml err=nil\n",
            "INFO:pyngrok.process.ngrok:t=2025-05-06T09:09:56+0000 lvl=info msg=\"starting web service\" obj=web addr=127.0.0.1:4040 allow_hosts=[]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸŒ ngrok ê³ ì • ë„ë©”ì¸ ì—°ê²° ì‹œë„: clam-talented-promptly.ngrok-free.app...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:pyngrok.process.ngrok:t=2025-05-06T09:09:57+0000 lvl=info msg=\"client session established\" obj=tunnels.session\n",
            "INFO:pyngrok.process.ngrok:t=2025-05-06T09:09:57+0000 lvl=info msg=\"tunnel session started\" obj=tunnels.session\n",
            "INFO:pyngrok.process.ngrok:t=2025-05-06T09:09:57+0000 lvl=info msg=start pg=/api/tunnels id=0a27af6aeb1faea1\n",
            "INFO:pyngrok.process.ngrok:t=2025-05-06T09:09:57+0000 lvl=info msg=end pg=/api/tunnels id=0a27af6aeb1faea1 status=200 dur=222.079Âµs\n",
            "INFO:pyngrok.process.ngrok:t=2025-05-06T09:09:57+0000 lvl=info msg=start pg=/api/tunnels id=596519940276125a\n",
            "INFO:pyngrok.process.ngrok:t=2025-05-06T09:09:57+0000 lvl=info msg=end pg=/api/tunnels id=596519940276125a status=200 dur=72.933Âµs\n",
            "INFO:pyngrok.process.ngrok:t=2025-05-06T09:09:57+0000 lvl=info msg=start pg=/api/tunnels id=fa96f5698a97df62\n",
            "INFO:pyngrok.process.ngrok:t=2025-05-06T09:09:57+0000 lvl=info msg=\"started tunnel\" obj=tunnels name=http-8000-2911904d-9052-4d8d-946d-6eea7e6d8725 addr=http://localhost:8000 url=https://clam-talented-promptly.ngrok-free.app\n",
            "INFO:pyngrok.process.ngrok:t=2025-05-06T09:09:57+0000 lvl=info msg=end pg=/api/tunnels id=fa96f5698a97df62 status=201 dur=224.132439ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… ngrok ì—°ê²° ì™„ë£Œ: https://clam-talented-promptly.ngrok-free.app\n",
            "ðŸš€ FastAPI ì„œë²„ ì‹¤í–‰ ì¤‘... ë¡œê·¸ íŒŒì¼: uvicorn_server.log\n",
            "â³ ëª¨ë¸ ë¡œë”© ëŒ€ê¸° ì¤‘... ('All models loaded' ê°ì§€)\n",
            "1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 \n",
            "âœ… ëª¨ë¸ ë¡œë”© ê°ì§€ ì™„ë£Œ!\n",
            "âŒ› ì•ˆì •í™”ë¥¼ ìœ„í•´ 10ì´ˆ ëŒ€ê¸° ì¤‘...\n",
            "\n",
            "ðŸŽ¯ ì„œë²„ ì‹¤í–‰ ì™„ë£Œ! ì™¸ë¶€ ì ‘ì† URL:\n",
            "ðŸ”— https://clam-talented-promptly.ngrok-free.app\n",
            "\n",
            "ðŸ“‹ í˜„ìž¬ ì‹¤í–‰ ì¤‘ì¸ í”„ë¡œì„¸ìŠ¤:\n",
            "root        3333     485  0 09:09 ?        00:00:00 /root/.config/ngrok/ngrok start --none --log=stdout\n",
            "root        3355       1 99 09:09 ?        00:01:36 python3 main_server.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qrax2DLnOrYR",
        "outputId": "307a46aa-51c8-4dc0-b1bc-4cf8af650b39"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fri May  2 00:01:14 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  NVIDIA A100-SXM4-40GB          Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   34C    P0             55W /  400W |     541MiB /  40960MiB |      0%      Default |\n",
            "|                                         |                        |             Disabled |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title [êµ¬ë²„ì „ - to('cuda') ì‚¬ìš© ì¤‘] FastAPI ì• í”Œë¦¬ì¼€ì´ì…˜ ì½”ë“œ ìž‘ì„± (main_server.py íŒŒì¼ ìƒì„±)\n",
        "'''\n",
        "# ì´ ì…€ì€ FastAPI ì„œë²„ ì½”ë“œë¥¼ main_server.py íŒŒì¼ë¡œ ì €ìž¥í•©ë‹ˆë‹¤.\n",
        "%%writefile main_server.py\n",
        "import os\n",
        "import io\n",
        "import logging\n",
        "import gc\n",
        "from contextlib import asynccontextmanager\n",
        "from typing import Optional, Any\n",
        "\n",
        "import torch\n",
        "import uvicorn\n",
        "from fastapi import FastAPI, HTTPException, UploadFile, File, Form\n",
        "from fastapi.responses import Response\n",
        "from pydantic import BaseModel\n",
        "from PIL import Image\n",
        "from pyngrok import ngrok\n",
        "\n",
        "# Diffusers - Flux + ControlNet\n",
        "from diffusers import FluxPipeline, FluxControlNetModel, FluxControlNetPipeline\n",
        "from controlnet_aux import CannyDetector\n",
        "\n",
        "# IP-Adapter\n",
        "from pipeline_flux_ipa import FluxPipeline as FluxPipelineIP\n",
        "from transformer_flux import FluxTransformer2DModel\n",
        "from infer_flux_ipa_siglip import resize_img, IPAdapter\n",
        "\n",
        "# --- Configuration ---\n",
        "BASE_MODEL_ID = \"black-forest-labs/FLUX.1-dev\"\n",
        "CONTROLNET_MODEL_ID = \"Shakker-Labs/FLUX.1-dev-ControlNet-Union-Pro-2.0\"\n",
        "IMAGE_ENCODER_PATH = \"google/siglip-so400m-patch14-384\"\n",
        "IPADAPTER_PATH = \"./ip-adapter.bin\"\n",
        "\n",
        "DEFAULT_STEPS = 30\n",
        "DEFAULT_GUIDANCE_SCALE = 3.5\n",
        "DEFAULT_CONTROLNET_SCALE = 0.7\n",
        "DEFAULT_IPADAPTER_SCALE = 0.7\n",
        "IMAGE_WIDTH = 1024\n",
        "IMAGE_HEIGHT = 1024\n",
        "PORT = 8000\n",
        "\n",
        "# --- Global State ---\n",
        "base_pipe: Optional[FluxPipeline] = None\n",
        "controlnet_pipe: Optional[FluxControlNetPipeline] = None\n",
        "controlnet_preprocessor: Optional[Any] = None\n",
        "pipe_ip: Optional[FluxPipelineIP] = None\n",
        "ip_model: Optional[IPAdapter] = None\n",
        "device: Optional[str] = None\n",
        "\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# --- Helper Functions ---\n",
        "def load_pil_image(image_bytes: bytes) -> Image.Image:\n",
        "    return Image.open(io.BytesIO(image_bytes)).convert(\"RGB\")\n",
        "\n",
        "def image_to_bytes(image: Image.Image) -> bytes:\n",
        "    byte_arr = io.BytesIO()\n",
        "    image.save(byte_arr, format='PNG')\n",
        "    byte_arr.seek(0)\n",
        "    return byte_arr.getvalue()\n",
        "\n",
        "def get_generator(seed: Optional[int] = None) -> torch.Generator:\n",
        "    if seed is None:\n",
        "        seed = torch.randint(0, 2**32 - 1, (1,)).item()\n",
        "    logger.info(f\"Using seed: {seed}\")\n",
        "    return torch.Generator(device=device).manual_seed(seed)\n",
        "\n",
        "async def prepare_reference_image(uploaded_image: UploadFile) -> Image.Image:\n",
        "    image = load_pil_image(await uploaded_image.read())\n",
        "    return resize_img(image)\n",
        "\n",
        "def prepare_control_image(uploaded_image: UploadFile) -> Image.Image:\n",
        "    if controlnet_preprocessor is None:\n",
        "        raise RuntimeError(\"ControlNet preprocessor not loaded.\")\n",
        "    image = load_pil_image(uploaded_image.file.read())\n",
        "    control_image = controlnet_preprocessor(image)\n",
        "    return control_image\n",
        "\n",
        "# --- Model Loading ---\n",
        "def load_models():\n",
        "    global base_pipe, controlnet_pipe, controlnet_preprocessor, pipe_ip, ip_model, device\n",
        "\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    logger.info(f\"Using device: {device}\")\n",
        "    dtype = torch.bfloat16 if device == \"cuda\" and torch.cuda.is_bf16_supported() else torch.float16\n",
        "\n",
        "    colab_log_flag = 'O'\n",
        "    try:\n",
        "        logger.info(\"Loading Flux base model...\")\n",
        "        base_pipe = FluxPipeline.from_pretrained(BASE_MODEL_ID, torch_dtype=dtype)\n",
        "        base_pipe.to(\"cpu\")\n",
        "        base_pipe.enable_model_cpu_offload()\n",
        "        base_pipe.enable_attention_slicing()\n",
        "\n",
        "        logger.info(\"Loading Flux ControlNet model...\")\n",
        "        controlnet_model = FluxControlNetModel.from_pretrained(CONTROLNET_MODEL_ID, torch_dtype=dtype)\n",
        "        controlnet_pipe = FluxControlNetPipeline.from_pretrained(BASE_MODEL_ID, controlnet=controlnet_model, torch_dtype=dtype)\n",
        "        controlnet_pipe.to(\"cpu\")\n",
        "        controlnet_pipe.enable_model_cpu_offload()\n",
        "        controlnet_pipe.enable_attention_slicing()\n",
        "\n",
        "        logger.info(\"Loading Canny Preprocessor...\")\n",
        "        controlnet_preprocessor = CannyDetector()\n",
        "\n",
        "        logger.info(\"Loading Flux IP-Adapter model...\")\n",
        "        transformer = FluxTransformer2DModel.from_pretrained(BASE_MODEL_ID, subfolder=\"transformer\", torch_dtype=dtype)\n",
        "        pipe_ip = FluxPipelineIP.from_pretrained(BASE_MODEL_ID, transformer=transformer, torch_dtype=dtype)\n",
        "        ip_model = IPAdapter(\n",
        "            pipe_ip,\n",
        "            IMAGE_ENCODER_PATH,\n",
        "            IPADAPTER_PATH,\n",
        "            device=device,\n",
        "            num_tokens=128\n",
        "        )\n",
        "        pipe_ip.to(\"cpu\")\n",
        "        pipe_ip.enable_model_cpu_offload()\n",
        "        pipe_ip.enable_attention_slicing()\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.exception(\"Fatal error during model loading\")\n",
        "        colab_log_flag = 'X'\n",
        "        raise RuntimeError(f\"Failed to load models: {e}\")\n",
        "\n",
        "    logger.info(\"âœ… All models loaded.\")\n",
        "    print(colab_log_flag)\n",
        "\n",
        "# --- FastAPI Setup ---\n",
        "@asynccontextmanager\n",
        "async def lifespan(app: FastAPI):\n",
        "    logger.info(\"Application startup...\")\n",
        "    load_models()\n",
        "    ngrok_auth_token = os.environ.get(\"NGROK_AUTHTOKEN\")\n",
        "    if ngrok_auth_token:\n",
        "        public_url = ngrok.connect(PORT, \"http\")\n",
        "        logger.info(f\"Ngrok tunnel active at: {public_url}\")\n",
        "        app.state.ngrok_url = public_url\n",
        "    else:\n",
        "        app.state.ngrok_url = None\n",
        "    yield\n",
        "    logger.info(\"Application shutdown...\")\n",
        "    ngrok.kill()\n",
        "    global base_pipe, controlnet_pipe, pipe_ip\n",
        "    del base_pipe, controlnet_pipe, pipe_ip\n",
        "    if device == \"cuda\":\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "app = FastAPI(lifespan=lifespan, title=\"Flux Unified API\")\n",
        "\n",
        "# --- Pydantic Model ---\n",
        "class TextToImageRequest(BaseModel):\n",
        "    prompt: str\n",
        "    negative_prompt: Optional[str] = \"\"\n",
        "    num_inference_steps: int = DEFAULT_STEPS\n",
        "    guidance_scale: float = DEFAULT_GUIDANCE_SCALE\n",
        "    seed: Optional[int] = None\n",
        "\n",
        "# --- API Endpoints ---\n",
        "@app.get(\"/\")\n",
        "async def read_root():\n",
        "    return {\n",
        "        \"message\": \"Flux Unified API running.\",\n",
        "        \"device\": device,\n",
        "        \"ngrok_url\": app.state.ngrok_url if hasattr(app.state, \"ngrok_url\") else None\n",
        "    }\n",
        "\n",
        "@app.post(\"/generate/text-to-image\", response_class=Response)\n",
        "async def generate_text_to_image(request: TextToImageRequest):\n",
        "    if base_pipe is None:\n",
        "        raise HTTPException(status_code=503, detail=\"Base pipeline not ready.\")\n",
        "    base_pipe.to(\"cuda\")\n",
        "    generator = get_generator(request.seed)\n",
        "\n",
        "    try:\n",
        "        with torch.inference_mode():\n",
        "            result = base_pipe(\n",
        "                prompt=request.prompt,\n",
        "                negative_prompt=request.negative_prompt,\n",
        "                width=IMAGE_WIDTH,\n",
        "                height=IMAGE_HEIGHT,\n",
        "                num_inference_steps=request.num_inference_steps,\n",
        "                guidance_scale=request.guidance_scale,\n",
        "                generator=generator,\n",
        "            )\n",
        "        output_image = result.images[0]\n",
        "        img_bytes = image_to_bytes(output_image)\n",
        "        return Response(content=img_bytes, media_type=\"image/png\")\n",
        "    finally:\n",
        "        base_pipe.to(\"cpu\")\n",
        "        torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "\n",
        "@app.post(\"/generate/image-to-image\", response_class=Response)\n",
        "async def generate_image_to_image(\n",
        "    prompt: str = Form(...),\n",
        "    negative_prompt: Optional[str] = Form(\"\"),\n",
        "    controlnet_scale: float = Form(DEFAULT_CONTROLNET_SCALE),\n",
        "    num_inference_steps: int = Form(DEFAULT_STEPS),\n",
        "    guidance_scale: float = Form(DEFAULT_GUIDANCE_SCALE),\n",
        "    seed: Optional[int] = Form(None),\n",
        "    image: UploadFile = File(...)\n",
        "):\n",
        "    if controlnet_pipe is None:\n",
        "        raise HTTPException(status_code=503, detail=\"ControlNet pipeline not ready.\")\n",
        "\n",
        "    controlnet_pipe.to(\"cuda\")\n",
        "    control_image = prepare_control_image(image)\n",
        "    generator = get_generator(seed)\n",
        "\n",
        "    try:\n",
        "        with torch.inference_mode():\n",
        "            result = controlnet_pipe(\n",
        "                prompt=prompt,\n",
        "                image=control_image,\n",
        "                width=IMAGE_WIDTH,\n",
        "                height=IMAGE_HEIGHT,\n",
        "                num_inference_steps=num_inference_steps,\n",
        "                guidance_scale=guidance_scale,\n",
        "                controlnet_conditioning_scale=controlnet_scale,\n",
        "                generator=generator,\n",
        "            )\n",
        "        output_image = result.images[0]\n",
        "        img_bytes = image_to_bytes(output_image)\n",
        "        return Response(content=img_bytes, media_type=\"image/png\")\n",
        "    finally:\n",
        "        controlnet_pipe.to(\"cpu\")\n",
        "        torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "\n",
        "@app.post(\"/generate/ip-adapter-image\", response_class=Response)\n",
        "async def generate_ip_adapter_image(\n",
        "    prompt: str = Form(...),\n",
        "    ipadapter_scale: float = Form(DEFAULT_IPADAPTER_SCALE),\n",
        "    num_inference_steps: int = Form(DEFAULT_STEPS),\n",
        "    seed: Optional[int] = Form(None),\n",
        "    image: UploadFile = File(...)\n",
        "):\n",
        "    if ip_model is None:\n",
        "        raise HTTPException(status_code=503, detail=\"IP-Adapter not ready.\")\n",
        "\n",
        "    pipe_ip.to(\"cuda\")\n",
        "    reference_image = await prepare_reference_image(image)\n",
        "\n",
        "    try:\n",
        "        with torch.inference_mode():\n",
        "            output_images = ip_model.generate(\n",
        "                pil_image=reference_image,\n",
        "                prompt=prompt,\n",
        "                scale=ipadapter_scale,\n",
        "                width=IMAGE_WIDTH,\n",
        "                height=IMAGE_HEIGHT,\n",
        "                seed=seed,\n",
        "            )\n",
        "        output_image = output_images[0]\n",
        "        img_bytes = image_to_bytes(output_image)\n",
        "        return Response(content=img_bytes, media_type=\"image/png\")\n",
        "    finally:\n",
        "        pipe_ip.to(\"cpu\")\n",
        "        torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "\n",
        "# --- Main ---\n",
        "if __name__ == \"__main__\":\n",
        "    logger.info(\"Starting Uvicorn server...\")\n",
        "    uvicorn.run(\"main_server:app\", host=\"0.0.0.0\", port=PORT, reload=False)\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        },
        "id": "QGXTSJyYllKv",
        "outputId": "6f20acd9-9490-4079-fe85-562ecff68553"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n# ì´ ì…€ì€ FastAPI ì„œë²„ ì½”ë“œë¥¼ main_server.py íŒŒì¼ë¡œ ì €ìž¥í•©ë‹ˆë‹¤.\\n%%writefile main_server.py\\nimport os\\nimport io\\nimport logging\\nimport gc\\nfrom contextlib import asynccontextmanager\\nfrom typing import Optional, Any\\n\\nimport torch\\nimport uvicorn\\nfrom fastapi import FastAPI, HTTPException, UploadFile, File, Form\\nfrom fastapi.responses import Response\\nfrom pydantic import BaseModel\\nfrom PIL import Image\\nfrom pyngrok import ngrok\\n\\n# Diffusers - Flux + ControlNet\\nfrom diffusers import FluxPipeline, FluxControlNetModel, FluxControlNetPipeline\\nfrom controlnet_aux import CannyDetector\\n\\n# IP-Adapter\\nfrom pipeline_flux_ipa import FluxPipeline as FluxPipelineIP\\nfrom transformer_flux import FluxTransformer2DModel\\nfrom infer_flux_ipa_siglip import resize_img, IPAdapter\\n\\n# --- Configuration ---\\nBASE_MODEL_ID = \"black-forest-labs/FLUX.1-dev\"\\nCONTROLNET_MODEL_ID = \"Shakker-Labs/FLUX.1-dev-ControlNet-Union-Pro-2.0\"\\nIMAGE_ENCODER_PATH = \"google/siglip-so400m-patch14-384\"\\nIPADAPTER_PATH = \"./ip-adapter.bin\"\\n\\nDEFAULT_STEPS = 30\\nDEFAULT_GUIDANCE_SCALE = 3.5\\nDEFAULT_CONTROLNET_SCALE = 0.7\\nDEFAULT_IPADAPTER_SCALE = 0.7\\nIMAGE_WIDTH = 1024\\nIMAGE_HEIGHT = 1024\\nPORT = 8000\\n\\n# --- Global State ---\\nbase_pipe: Optional[FluxPipeline] = None\\ncontrolnet_pipe: Optional[FluxControlNetPipeline] = None\\ncontrolnet_preprocessor: Optional[Any] = None\\npipe_ip: Optional[FluxPipelineIP] = None\\nip_model: Optional[IPAdapter] = None\\ndevice: Optional[str] = None\\n\\nlogging.basicConfig(level=logging.INFO)\\nlogger = logging.getLogger(__name__)\\n\\n# --- Helper Functions ---\\ndef load_pil_image(image_bytes: bytes) -> Image.Image:\\n    return Image.open(io.BytesIO(image_bytes)).convert(\"RGB\")\\n\\ndef image_to_bytes(image: Image.Image) -> bytes:\\n    byte_arr = io.BytesIO()\\n    image.save(byte_arr, format=\\'PNG\\')\\n    byte_arr.seek(0)\\n    return byte_arr.getvalue()\\n\\ndef get_generator(seed: Optional[int] = None) -> torch.Generator:\\n    if seed is None:\\n        seed = torch.randint(0, 2**32 - 1, (1,)).item()\\n    logger.info(f\"Using seed: {seed}\")\\n    return torch.Generator(device=device).manual_seed(seed)\\n\\nasync def prepare_reference_image(uploaded_image: UploadFile) -> Image.Image:\\n    image = load_pil_image(await uploaded_image.read())\\n    return resize_img(image)\\n\\ndef prepare_control_image(uploaded_image: UploadFile) -> Image.Image:\\n    if controlnet_preprocessor is None:\\n        raise RuntimeError(\"ControlNet preprocessor not loaded.\")\\n    image = load_pil_image(uploaded_image.file.read())\\n    control_image = controlnet_preprocessor(image)\\n    return control_image\\n\\n# --- Model Loading ---\\ndef load_models():\\n    global base_pipe, controlnet_pipe, controlnet_preprocessor, pipe_ip, ip_model, device\\n\\n    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\\n    logger.info(f\"Using device: {device}\")\\n    dtype = torch.bfloat16 if device == \"cuda\" and torch.cuda.is_bf16_supported() else torch.float16\\n\\n    colab_log_flag = \\'O\\'\\n    try:\\n        logger.info(\"Loading Flux base model...\")\\n        base_pipe = FluxPipeline.from_pretrained(BASE_MODEL_ID, torch_dtype=dtype)\\n        base_pipe.to(\"cpu\")\\n        base_pipe.enable_model_cpu_offload()\\n        base_pipe.enable_attention_slicing()\\n\\n        logger.info(\"Loading Flux ControlNet model...\")\\n        controlnet_model = FluxControlNetModel.from_pretrained(CONTROLNET_MODEL_ID, torch_dtype=dtype)\\n        controlnet_pipe = FluxControlNetPipeline.from_pretrained(BASE_MODEL_ID, controlnet=controlnet_model, torch_dtype=dtype)\\n        controlnet_pipe.to(\"cpu\")\\n        controlnet_pipe.enable_model_cpu_offload()\\n        controlnet_pipe.enable_attention_slicing()\\n\\n        logger.info(\"Loading Canny Preprocessor...\")\\n        controlnet_preprocessor = CannyDetector()\\n\\n        logger.info(\"Loading Flux IP-Adapter model...\")\\n        transformer = FluxTransformer2DModel.from_pretrained(BASE_MODEL_ID, subfolder=\"transformer\", torch_dtype=dtype)\\n        pipe_ip = FluxPipelineIP.from_pretrained(BASE_MODEL_ID, transformer=transformer, torch_dtype=dtype)\\n        ip_model = IPAdapter(\\n            pipe_ip,\\n            IMAGE_ENCODER_PATH,\\n            IPADAPTER_PATH,\\n            device=device,\\n            num_tokens=128\\n        )\\n        pipe_ip.to(\"cpu\")\\n        pipe_ip.enable_model_cpu_offload()\\n        pipe_ip.enable_attention_slicing()\\n\\n    except Exception as e:\\n        logger.exception(\"Fatal error during model loading\")\\n        colab_log_flag = \\'X\\'\\n        raise RuntimeError(f\"Failed to load models: {e}\")\\n\\n    logger.info(\"âœ… All models loaded.\")\\n    print(colab_log_flag)\\n\\n# --- FastAPI Setup ---\\n@asynccontextmanager\\nasync def lifespan(app: FastAPI):\\n    logger.info(\"Application startup...\")\\n    load_models()\\n    ngrok_auth_token = os.environ.get(\"NGROK_AUTHTOKEN\")\\n    if ngrok_auth_token:\\n        public_url = ngrok.connect(PORT, \"http\")\\n        logger.info(f\"Ngrok tunnel active at: {public_url}\")\\n        app.state.ngrok_url = public_url\\n    else:\\n        app.state.ngrok_url = None\\n    yield\\n    logger.info(\"Application shutdown...\")\\n    ngrok.kill()\\n    global base_pipe, controlnet_pipe, pipe_ip\\n    del base_pipe, controlnet_pipe, pipe_ip\\n    if device == \"cuda\":\\n        torch.cuda.empty_cache()\\n\\napp = FastAPI(lifespan=lifespan, title=\"Flux Unified API\")\\n\\n# --- Pydantic Model ---\\nclass TextToImageRequest(BaseModel):\\n    prompt: str\\n    negative_prompt: Optional[str] = \"\"\\n    num_inference_steps: int = DEFAULT_STEPS\\n    guidance_scale: float = DEFAULT_GUIDANCE_SCALE\\n    seed: Optional[int] = None\\n\\n# --- API Endpoints ---\\n@app.get(\"/\")\\nasync def read_root():\\n    return {\\n        \"message\": \"Flux Unified API running.\",\\n        \"device\": device,\\n        \"ngrok_url\": app.state.ngrok_url if hasattr(app.state, \"ngrok_url\") else None\\n    }\\n\\n@app.post(\"/generate/text-to-image\", response_class=Response)\\nasync def generate_text_to_image(request: TextToImageRequest):\\n    if base_pipe is None:\\n        raise HTTPException(status_code=503, detail=\"Base pipeline not ready.\")\\n    base_pipe.to(\"cuda\")\\n    generator = get_generator(request.seed)\\n\\n    try:\\n        with torch.inference_mode():\\n            result = base_pipe(\\n                prompt=request.prompt,\\n                negative_prompt=request.negative_prompt,\\n                width=IMAGE_WIDTH,\\n                height=IMAGE_HEIGHT,\\n                num_inference_steps=request.num_inference_steps,\\n                guidance_scale=request.guidance_scale,\\n                generator=generator,\\n            )\\n        output_image = result.images[0]\\n        img_bytes = image_to_bytes(output_image)\\n        return Response(content=img_bytes, media_type=\"image/png\")\\n    finally:\\n        base_pipe.to(\"cpu\")\\n        torch.cuda.empty_cache()\\n        gc.collect()\\n\\n@app.post(\"/generate/image-to-image\", response_class=Response)\\nasync def generate_image_to_image(\\n    prompt: str = Form(...),\\n    negative_prompt: Optional[str] = Form(\"\"),\\n    controlnet_scale: float = Form(DEFAULT_CONTROLNET_SCALE),\\n    num_inference_steps: int = Form(DEFAULT_STEPS),\\n    guidance_scale: float = Form(DEFAULT_GUIDANCE_SCALE),\\n    seed: Optional[int] = Form(None),\\n    image: UploadFile = File(...)\\n):\\n    if controlnet_pipe is None:\\n        raise HTTPException(status_code=503, detail=\"ControlNet pipeline not ready.\")\\n\\n    controlnet_pipe.to(\"cuda\")\\n    control_image = prepare_control_image(image)\\n    generator = get_generator(seed)\\n\\n    try:\\n        with torch.inference_mode():\\n            result = controlnet_pipe(\\n                prompt=prompt,\\n                image=control_image,\\n                width=IMAGE_WIDTH,\\n                height=IMAGE_HEIGHT,\\n                num_inference_steps=num_inference_steps,\\n                guidance_scale=guidance_scale,\\n                controlnet_conditioning_scale=controlnet_scale,\\n                generator=generator,\\n            )\\n        output_image = result.images[0]\\n        img_bytes = image_to_bytes(output_image)\\n        return Response(content=img_bytes, media_type=\"image/png\")\\n    finally:\\n        controlnet_pipe.to(\"cpu\")\\n        torch.cuda.empty_cache()\\n        gc.collect()\\n\\n@app.post(\"/generate/ip-adapter-image\", response_class=Response)\\nasync def generate_ip_adapter_image(\\n    prompt: str = Form(...),\\n    ipadapter_scale: float = Form(DEFAULT_IPADAPTER_SCALE),\\n    num_inference_steps: int = Form(DEFAULT_STEPS),\\n    seed: Optional[int] = Form(None),\\n    image: UploadFile = File(...)\\n):\\n    if ip_model is None:\\n        raise HTTPException(status_code=503, detail=\"IP-Adapter not ready.\")\\n\\n    pipe_ip.to(\"cuda\")\\n    reference_image = await prepare_reference_image(image)\\n\\n    try:\\n        with torch.inference_mode():\\n            output_images = ip_model.generate(\\n                pil_image=reference_image,\\n                prompt=prompt,\\n                scale=ipadapter_scale,\\n                width=IMAGE_WIDTH,\\n                height=IMAGE_HEIGHT,\\n                seed=seed,\\n            )\\n        output_image = output_images[0]\\n        img_bytes = image_to_bytes(output_image)\\n        return Response(content=img_bytes, media_type=\"image/png\")\\n    finally:\\n        pipe_ip.to(\"cpu\")\\n        torch.cuda.empty_cache()\\n        gc.collect()\\n\\n# --- Main ---\\nif __name__ == \"__main__\":\\n    logger.info(\"Starting Uvicorn server...\")\\n    uvicorn.run(\"main_server:app\", host=\"0.0.0.0\", port=PORT, reload=False)\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    }
  ]
}