{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "BPe9QPfqlgwD"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "g_start = time.time()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "cellView": "form",
        "id": "w-k2065nuVgM"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "# @title 1. ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜\n",
        "%pip install -q fastapi uvicorn[standard]\n",
        "%pip install -q pyngrok>=7.0.0 # diffusers>=0.27.0 transformers accelerate\n",
        "%pip install -q controlnet_aux Pillow # torch>=2.0.0\n",
        "%pip install -q safetensors pydantic huggingface_hub python-multipart\n",
        "# %pip install -U diffusers transformers accelerate safetensors\n",
        "%pip install -q wget\n",
        "# %pip install -q peft requests\n",
        "\n",
        "# # xformers ì‚¬ìš© ì‹œ (ë²„ì „ í˜¸í™˜ í™•ì¸ í•„ìš”)\n",
        "# %pip install -q fastapi uvicorn[standard] pyngrok>=7.0.0  # 1) FastAPI / ì„œë²„ìš©\n",
        "# %pip install -q diffusers==0.33.1 transformers==4.51.3 accelerate==1.6.0  # 2) Diffusers + Transformers + Accelerate\n",
        "# %pip install -q controlnet_aux Pillow safetensors pydantic huggingface_hub python-multipart # 3) Fluxìš© ë¶€ê°€ ê¸°ëŠ¥ë“¤\n",
        "# %pip install -q ninja wget  # 4) ê¸°íƒ€ ë„êµ¬\n",
        "\n",
        "# %pip uninstall -y xformers  # 5) ê¸°ì¡´ xformers ì œê±°\n",
        "# %pip install -q git+https://github.com/facebookresearch/xformers.git  # 6) Colab(PyTorch 2.6.0+cu124) í™˜ê²½ì— ë§žì¶° xFormersë¥¼ ì†ŒìŠ¤ì—ì„œ ë¹Œë“œ ì„¤ì¹˜\n",
        "# # %pip install -q peft requests\n",
        "\n",
        "print(\"âœ… ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜ ì™„ë£Œ\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7AY8e66XRJzV",
        "outputId": "43ef0033-28fc-4ab8-c0c4-dc7e03678f9e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ipython 7.34.0 requires jedi, which is not installed.\n"
          ]
        }
      ],
      "source": [
        "# ë¼ì´ë¸ŒëŸ¬ë¦¬ í˜¸í™˜ì„± í™•ì¸\n",
        "# jediëŠ” ìƒê´€ ì—†ìŒ (colab íŠ¹ì„± ìƒ, ì„¤ì¹˜ ì•ˆí•œë‹¤ê³  í•¨)\n",
        "!pip check"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A0jI1SIruVeF",
        "outputId": "e751afaa-f7f2-4c5b-de02-469cea995a5e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ”‘ Hugging Face ë¡œê·¸ì¸ ì¤‘...\n",
            "âœ… Ngrok Authtoken ì„¤ì • ì™„ë£Œ\n"
          ]
        }
      ],
      "source": [
        "# @title 2. Hugging Face ë¡œê·¸ì¸ / Ngrok ì„¤ì • (Authtoken ì‹œí¬ë¦¿ í‚¤)\n",
        "import os\n",
        "from google.colab import userdata\n",
        "from huggingface_hub import login\n",
        "from pyngrok import conf, ngrok\n",
        "\n",
        "# Hugging Face ë° ngrok í† í° (Colab Secretì—ì„œ ê°€ì ¸ì˜¤ê¸°)\n",
        "hf_token = userdata.get(\"HF_TOKEN\")        # Hugging Face Token\n",
        "civitai_token = userdata.get(\"CIVITAI_TOKEN\")   # CivitAI Token\n",
        "ngrok_token = userdata.get(\"google_ngrok_authtoken\")  # ngrok Token\n",
        "\n",
        "if hf_token == None:\n",
        "  print('x')\n",
        "if civitai_token == None:\n",
        "  print('x')\n",
        "if ngrok_token == None:\n",
        "  print('x')\n",
        "\n",
        "# í¬íŠ¸ ì„¤ì •\n",
        "PORT = 8000\n",
        "\n",
        "# hugging face ë¡œê·¸ì¸\n",
        "print(\"ðŸ”‘ Hugging Face ë¡œê·¸ì¸ ì¤‘...\")\n",
        "login(token=hf_token)\n",
        "\n",
        "# ngrok ì„¤ì •\n",
        "ngrok.set_auth_token(ngrok_token)\n",
        "print(\"âœ… Ngrok Authtoken ì„¤ì • ì™„ë£Œ\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2R0izjLyvHfL",
        "outputId": "8b1ee9e4-a393-46e6-c71c-5c2e64392d26"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-05-27 07:59:43--  https://civitai.com/api/download/models/1215918?type=Model&format=SafeTensor&size=full&token=2ab1653b9d579f186d295f4540908157\n",
            "Resolving civitai.com (civitai.com)... 172.67.12.143, 104.22.19.237, 104.22.18.237, ...\n",
            "Connecting to civitai.com (civitai.com)|172.67.12.143|:443... connected.\n",
            "HTTP request sent, awaiting response... 307 Temporary Redirect\n",
            "Location: https://civitai-delivery-worker-prod.5ac0637cfd0766c97916cefa3764fbdf.r2.cloudflarestorage.com/model/686417/jGhibliV2FluxUltimate.FZyi.safetensors?X-Amz-Expires=86400&response-content-disposition=attachment%3B%20filename%3D%22IllustrationJuanerGhibli_v20.safetensors%22&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=e01358d793ad6966166af8b3064953ad/20250527/us-east-1/s3/aws4_request&X-Amz-Date=20250527T075943Z&X-Amz-SignedHeaders=host&X-Amz-Signature=25b9f1b92a6ee9c35a0a1b75d6c6e6212a2f6d39ee80f8b967bf28d3df17658d [following]\n",
            "--2025-05-27 07:59:44--  https://civitai-delivery-worker-prod.5ac0637cfd0766c97916cefa3764fbdf.r2.cloudflarestorage.com/model/686417/jGhibliV2FluxUltimate.FZyi.safetensors?X-Amz-Expires=86400&response-content-disposition=attachment%3B%20filename%3D%22IllustrationJuanerGhibli_v20.safetensors%22&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=e01358d793ad6966166af8b3064953ad/20250527/us-east-1/s3/aws4_request&X-Amz-Date=20250527T075943Z&X-Amz-SignedHeaders=host&X-Amz-Signature=25b9f1b92a6ee9c35a0a1b75d6c6e6212a2f6d39ee80f8b967bf28d3df17658d\n",
            "Resolving civitai-delivery-worker-prod.5ac0637cfd0766c97916cefa3764fbdf.r2.cloudflarestorage.com (civitai-delivery-worker-prod.5ac0637cfd0766c97916cefa3764fbdf.r2.cloudflarestorage.com)... 162.159.141.50, 172.66.1.46, 2606:4700:7::12e, ...\n",
            "Connecting to civitai-delivery-worker-prod.5ac0637cfd0766c97916cefa3764fbdf.r2.cloudflarestorage.com (civitai-delivery-worker-prod.5ac0637cfd0766c97916cefa3764fbdf.r2.cloudflarestorage.com)|162.159.141.50|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 11901497256 (11G)\n",
            "Saving to: â€˜illustration_juaner_flux.safetensorsâ€™\n",
            "\n",
            "illustration_juaner 100%[===================>]  11.08G  34.3MB/s    in 3m 14s  \n",
            "\n",
            "2025-05-27 08:02:58 (58.4 MB/s) - â€˜illustration_juaner_flux.safetensorsâ€™ saved [11901497256/11901497256]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# @title 3. CivitAI Checkpoint ë‹¤ìš´ë¡œë“œ\n",
        "import os\n",
        "os.environ[\"CIVITAI_API_TOKEN\"] = civitai_token\n",
        "\n",
        "import wget\n",
        "!wget -O illustration_juaner_flux.safetensors \\\n",
        "\"https://civitai.com/api/download/models/1215918?type=Model&format=SafeTensor&size=full&token=$CIVITAI_API_TOKEN\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 4. Model Pipeline ì •ì˜ - model_loader.py\n",
        "# ì´ ì…€ì€ Model Pipelineì„ ìƒì„±í•˜ëŠ” ì½”ë“œë¥¼ model_loader.py íŒŒì¼ë¡œ ì €ìž¥í•©ë‹ˆë‹¤.\n",
        "\n",
        "%%writefile model_loader.py\n",
        "from diffusers import FluxPipeline, DiffusionPipeline, FluxTransformer2DModel\n",
        "from transformers import T5EncoderModel\n",
        "\n",
        "# Ghibli (Flux)\n",
        "def load_ghibli_flux_model(dtype):\n",
        "    BASE_MODEL_ID = \"black-forest-labs/FLUX.1-dev\"\n",
        "    CHECKPOINT_MODEL_ID = \"illustration_juaner_flux.safetensors\"\n",
        "\n",
        "    transformer = FluxTransformer2DModel.from_single_file(\n",
        "        CHECKPOINT_MODEL_ID, torch_dtype=dtype\n",
        "    )\n",
        "    text_encoder_2 = T5EncoderModel.from_pretrained(\n",
        "        BASE_MODEL_ID, subfolder=\"text_encoder_2\", torch_dtype=dtype\n",
        "    )\n",
        "    pipe = FluxPipeline.from_pretrained(\n",
        "        BASE_MODEL_ID,\n",
        "        transformer=transformer,\n",
        "        text_encoder_2=text_encoder_2,\n",
        "        torch_dtype=dtype,\n",
        "    )\n",
        "    pipe.enable_model_cpu_offload()\n",
        "    pipe.enable_attention_slicing()\n",
        "    return pipe\n",
        "\n",
        "# AnythingXL (stable-diffusion-xl style)\n",
        "def load_anything_xl_model(dtype):\n",
        "    MODEL_ID = \"stabilityai/stable-diffusion-xl-base-1.0\"\n",
        "    pipe = DiffusionPipeline.from_pretrained(\n",
        "        MODEL_ID,\n",
        "        torch_dtype=dtype,\n",
        "        variant=\"fp16\"\n",
        "    )\n",
        "    pipe.enable_model_cpu_offload()\n",
        "    pipe.enable_attention_slicing()\n",
        "    return pipe\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EWf3DH_gKQcO",
        "outputId": "d6776f76-eb8e-42e7-cf7d-c8667823fc39"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing model_loader.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XXvUlWb9HRcD",
        "outputId": "70905741-5568-4ceb-b86c-870f24da26b3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing main_server.py\n"
          ]
        }
      ],
      "source": [
        "# @title 5. FastAPI ì„œë²„ - main_server.py\n",
        "# ì´ ì…€ì€ FastAPI ì„œë²„ ì½”ë“œë¥¼ main_server.py íŒŒì¼ë¡œ ì €ìž¥í•©ë‹ˆë‹¤.\n",
        "\n",
        "%%writefile main_server.py\n",
        "import os, io, gc, logging\n",
        "from contextlib import asynccontextmanager\n",
        "from typing import Optional, Any\n",
        "\n",
        "import torch\n",
        "import uvicorn\n",
        "from fastapi import FastAPI, HTTPException\n",
        "from fastapi.responses import Response\n",
        "from pydantic import BaseModel\n",
        "from PIL import Image\n",
        "from pyngrok import ngrok\n",
        "\n",
        "\n",
        "# --- GPU ì„¤ì • ---\n",
        "device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "dtype: torch.dtype = torch.bfloat16 if (device == \"cuda\" and torch.cuda.is_bf16_supported()) else torch.float16\n",
        "\n",
        "# --- ì„¤ì • ìƒìˆ˜ ---\n",
        "DEFAULT_STEPS = 30\n",
        "DEFAULT_GUIDANCE_SCALE = 3.5\n",
        "IMAGE_WIDTH = 1024\n",
        "IMAGE_HEIGHT = 1024\n",
        "PORT = 8000\n",
        "\n",
        "# --- ë¡œê¹… ì„¤ì • ---\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# --- ìœ í‹¸ í•¨ìˆ˜ ---\n",
        "def load_pil_image(image_bytes: bytes) -> Image.Image:\n",
        "    return Image.open(io.BytesIO(image_bytes)).convert(\"RGB\")\n",
        "\n",
        "def image_to_bytes(image: Image.Image) -> bytes:\n",
        "    byte_arr = io.BytesIO()\n",
        "    image.save(byte_arr, format='PNG')\n",
        "    byte_arr.seek(0)\n",
        "    return byte_arr.getvalue()\n",
        "\n",
        "def get_generator(seed: Optional[int] = None) -> torch.Generator:\n",
        "    if seed is None:\n",
        "        seed = torch.randint(0, 2**32 - 1, (1,)).item()\n",
        "    logger.info(f\"Using seed: {seed}\")\n",
        "    return torch.Generator(device=\"cuda\").manual_seed(seed)\n",
        "\n",
        "# -------------------------- ëª¨ë¸ ì„¸íŒ… ê´€ë ¨ --------------------------\n",
        "# ëª¨ë¸ ë¡œë”© ëª¨ë“ˆ import\n",
        "from model_loader import load_ghibli_flux_model, load_anything_xl_model\n",
        "\n",
        "# --- ì „ì—­ pipe ì„¸íŒ… ---\n",
        "ghibli_flux_pipe: Optional[Any] = None\n",
        "anything_xl_pipe: Optional[Any] = None\n",
        "\n",
        "# --- ëª¨ë¸ ë¡œë”© ---\n",
        "def load_models():\n",
        "    global ghibli_flux_pipe, anything_xl_pipe\n",
        "    ghibli_flux_pipe = load_ghibli_flux_model(dtype)\n",
        "    anything_xl_pipe = load_anything_xl_model(dtype)\n",
        "\n",
        "# --- ëª¨ë¸ í•´ì²´ ---\n",
        "# lifespanì—ì„œ del í˜¸ì¶œ í•„ìš”\n",
        "\n",
        "# ----------------------------------------------------------------\n",
        "\n",
        "\n",
        "# --- ì„œë²„ ìƒëª…ì£¼ê¸° ---\n",
        "@asynccontextmanager\n",
        "async def lifespan(app: FastAPI):\n",
        "    logger.info(\"App starting...\")\n",
        "    load_models()\n",
        "\n",
        "    public_url = os.environ.get(\"NGROK_STATIC_URL\", None)\n",
        "    app.state.ngrok_url = public_url\n",
        "    logger.info(f\"Ngrok (external) URL registered: {public_url}\")\n",
        "\n",
        "    yield\n",
        "\n",
        "    logger.info(\"App shutting down...\")\n",
        "    del ghibli_flux_pipe, anything_xl_pipe\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "# --- FastAPI ì•± ìƒì„± ---\n",
        "app = FastAPI(lifespan=lifespan)\n",
        "\n",
        "# --- ìš”ì²­ ëª¨ë¸ ---\n",
        "class TextToImageRequest(BaseModel):\n",
        "    model_name: str\n",
        "    prompt: str\n",
        "    negative_prompt: Optional[str] = \"\"\n",
        "    num_inference_steps: int = DEFAULT_STEPS\n",
        "    guidance_scale: float = DEFAULT_GUIDANCE_SCALE\n",
        "    seed: Optional[int] = None\n",
        "\n",
        "# --- ë£¨íŠ¸ í™•ì¸ ---\n",
        "@app.get(\"/\")\n",
        "async def read_root():\n",
        "    return {\n",
        "        \"message\": \"Image Model(text to image) API is running.\",\n",
        "        \"device\": device,\n",
        "        \"ngrok_url\": app.state.ngrok_url if hasattr(app.state, \"ngrok_url\") else None,\n",
        "    }\n",
        "\n",
        "# --- í…ìŠ¤íŠ¸ â†’ ì´ë¯¸ì§€ ---\n",
        "@app.post(\"/generate/text-to-image\", response_class=Response)\n",
        "async def generate_text_to_image(request: TextToImageRequest):\n",
        "    if request.model_name == \"ghibli-flux\":\n",
        "        if ghibli_flux_pipe is None:\n",
        "            raise HTTPException(status_code=503, detail=\"Ghibli-Flux pipeline not loaded.\")\n",
        "        pipe = ghibli_flux_pipe\n",
        "    elif request.model_name == \"anything-xl\":\n",
        "        if anything_xl_pipe is None:\n",
        "            raise HTTPException(status_code=503, detail=\"Anything-XL pipeline not loaded.\")\n",
        "        pipe = anything_xl_pipe\n",
        "    else:\n",
        "        raise HTTPException(status_code=400, detail=f\"Invalid mode: {request.mode}\")\n",
        "\n",
        "    generator = get_generator(request.seed)\n",
        "\n",
        "    try:\n",
        "        with torch.inference_mode():\n",
        "            result = pipe(\n",
        "                prompt=request.prompt,\n",
        "                negative_prompt=request.negative_prompt,\n",
        "                width=IMAGE_WIDTH,\n",
        "                height=IMAGE_HEIGHT,\n",
        "                num_inference_steps=request.num_inference_steps,\n",
        "                guidance_scale=request.guidance_scale,\n",
        "                generator=generator,\n",
        "            )\n",
        "            image_bytes = image_to_bytes(result.images[0])\n",
        "    finally:\n",
        "        del result\n",
        "        torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "\n",
        "    return Response(content=image_bytes, media_type=\"image/png\")\n",
        "\n",
        "# --- ì„œë²„ ì‹¤í–‰ ---\n",
        "if __name__ == \"__main__\":\n",
        "    logger.info(\"Starting Uvicorn server...\")\n",
        "    uvicorn.run(\"main_server:app\", host=\"0.0.0.0\", port=PORT, reload=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9hmI4LjHXoTF",
        "outputId": "77e6b4c7-fd43-4e63-bdad-3e62f9798fce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… ngrok í”„ë¡œì„¸ìŠ¤ ì¢…ë£Œ ì™„ë£Œ\n",
            "ðŸ§¹ GPU ë©”ëª¨ë¦¬ ë° ìºì‹œ ì •ë¦¬ ì™„ë£Œ\n",
            "âœ… FastAPI ì„œë²„ í”„ë¡œì„¸ìŠ¤ ì¢…ë£Œ ì™„ë£Œ\n"
          ]
        }
      ],
      "source": [
        "# @title Fin.\n",
        "import subprocess\n",
        "from pyngrok import ngrok\n",
        "\n",
        "# ngrok í”„ë¡œì„¸ìŠ¤ ì¢…ë£Œ\n",
        "try:\n",
        "    ngrok.kill()\n",
        "    print(\"âœ… ngrok í”„ë¡œì„¸ìŠ¤ ì¢…ë£Œ ì™„ë£Œ\")\n",
        "except Exception as e:\n",
        "    print(f\"âš ï¸ ngrok ì¢…ë£Œ ì¤‘ ì˜¤ë¥˜ (ë¬´ì‹œ ê°€ëŠ¥): {e}\")\n",
        "\n",
        "# uvicorn ì„œë²„ í”„ë¡œì„¸ìŠ¤ ì¢…ë£Œ\n",
        "subprocess.run(['pkill', '-f', 'uvicorn main_server:app'], stderr=subprocess.DEVNULL)\n",
        "subprocess.run(['pkill', '-f', 'ngrok'], stderr=subprocess.DEVNULL)\n",
        "\n",
        "import gc\n",
        "import torch\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "\n",
        "print(\"ðŸ§¹ GPU ë©”ëª¨ë¦¬ ë° ìºì‹œ ì •ë¦¬ ì™„ë£Œ\")\n",
        "\n",
        "\n",
        "!ps -ef | grep -E \"main_server.py|ngrok\" | grep -v grep\n",
        "\n",
        "\n",
        "\n",
        "import subprocess\n",
        "\n",
        "# main_server.py ì¢…ë£Œ\n",
        "subprocess.run(['pkill', '-f', 'main_server.py'])\n",
        "print(\"âœ… FastAPI ì„œë²„ í”„ë¡œì„¸ìŠ¤ ì¢…ë£Œ ì™„ë£Œ\")\n",
        "\n",
        "!ps -ef | grep -E \"main_server.py\" | grep -v grep\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "oCqXd-e1uVX7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ed44abf3-ffe9-4d05-d0bf-5868d129a0c6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "env: PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True\n",
            "ðŸ›  ê¸°ì¡´ ngrok / ì„œë²„ ì¢…ë£Œ ì‹œë„...\n",
            "âœ… ngrok ì¢…ë£Œ ì™„ë£Œ\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:pyngrok.ngrok:Opening tunnel named: http-8000-edf0a590-8e40-4bcc-a40c-4d4603db2c36\n",
            "INFO:pyngrok.process.ngrok:t=2025-05-27T08:03:05+0000 lvl=info msg=\"no configuration paths supplied\"\n",
            "INFO:pyngrok.process.ngrok:t=2025-05-27T08:03:05+0000 lvl=info msg=\"using configuration at default config path\" path=/root/.config/ngrok/ngrok.yml\n",
            "INFO:pyngrok.process.ngrok:t=2025-05-27T08:03:05+0000 lvl=info msg=\"open config file\" path=/root/.config/ngrok/ngrok.yml err=nil\n",
            "INFO:pyngrok.process.ngrok:t=2025-05-27T08:03:05+0000 lvl=info msg=\"starting web service\" obj=web addr=127.0.0.1:4040 allow_hosts=[]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸŒ ngrok ê³ ì • ë„ë©”ì¸ ì—°ê²° ì‹œë„: clam-talented-promptly.ngrok-free.app...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:pyngrok.process.ngrok:t=2025-05-27T08:03:06+0000 lvl=info msg=\"client session established\" obj=tunnels.session\n",
            "INFO:pyngrok.process.ngrok:t=2025-05-27T08:03:06+0000 lvl=info msg=\"tunnel session started\" obj=tunnels.session\n",
            "INFO:pyngrok.process.ngrok:t=2025-05-27T08:03:06+0000 lvl=info msg=start pg=/api/tunnels id=09d7b73e5be89387\n",
            "INFO:pyngrok.process.ngrok:t=2025-05-27T08:03:06+0000 lvl=info msg=end pg=/api/tunnels id=09d7b73e5be89387 status=200 dur=300.023Âµs\n",
            "INFO:pyngrok.process.ngrok:t=2025-05-27T08:03:06+0000 lvl=info msg=start pg=/api/tunnels id=a32c1be786dbd981\n",
            "INFO:pyngrok.process.ngrok:t=2025-05-27T08:03:06+0000 lvl=info msg=end pg=/api/tunnels id=a32c1be786dbd981 status=200 dur=112.174Âµs\n",
            "INFO:pyngrok.process.ngrok:t=2025-05-27T08:03:06+0000 lvl=info msg=start pg=/api/tunnels id=e7cad0985e1cd2d8\n",
            "INFO:pyngrok.process.ngrok:t=2025-05-27T08:03:06+0000 lvl=info msg=\"started tunnel\" obj=tunnels name=http-8000-edf0a590-8e40-4bcc-a40c-4d4603db2c36 addr=http://localhost:8000 url=https://clam-talented-promptly.ngrok-free.app\n",
            "INFO:pyngrok.process.ngrok:t=2025-05-27T08:03:06+0000 lvl=info msg=end pg=/api/tunnels id=e7cad0985e1cd2d8 status=201 dur=247.304805ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… ngrok ì—°ê²° ì™„ë£Œ: https://clam-talented-promptly.ngrok-free.app\n",
            "ðŸš€ FastAPI ì„œë²„ ì‹¤í–‰ ì¤‘... ë¡œê·¸ íŒŒì¼: uvicorn_server.log\n",
            "â³ ëª¨ë¸ ë¡œë”© ëŒ€ê¸° ì¤‘... ('Uvicorn running on' ê°ì§€)\n",
            "1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 \n",
            "31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 \n",
            "61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 \n",
            "âœ… ì„œë²„ ì¤€ë¹„ ì™„ë£Œ!\n",
            "âŒ› ì•ˆì •í™”ë¥¼ ìœ„í•´ 10ì´ˆ ëŒ€ê¸° ì¤‘...\n",
            "\n",
            "ðŸŽ¯ ì„œë²„ ì‹¤í–‰ ì™„ë£Œ! ì™¸ë¶€ ì ‘ì† URL:\n",
            "ðŸ”— https://clam-talented-promptly.ngrok-free.app\n",
            "\n",
            "ðŸ“‹ í˜„ìž¬ ì‹¤í–‰ ì¤‘ì¸ í”„ë¡œì„¸ìŠ¤:\n",
            "root        3922    2408  0 08:03 ?        00:00:00 /root/.config/ngrok/ngrok start --none --log stdout\n",
            "root        3951       1 99 08:03 ?        00:02:32 python3 main_server.py\n",
            "\n",
            "Total time : 398.92 seconds\n"
          ]
        }
      ],
      "source": [
        "# @title ðŸš€ ngrok ì—°ê²° ë° FastAPI ì„œë²„ ì‹¤í–‰\n",
        "# í™˜ê²½ ë³€ìˆ˜ ì„¤ì •\n",
        "%env PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True\n",
        "\n",
        "import time\n",
        "import os\n",
        "import subprocess\n",
        "import logging\n",
        "from pyngrok import ngrok\n",
        "\n",
        "# --- ë¡œê¹… ì„¤ì • ---\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "pyngrok_logger = logging.getLogger(\"pyngrok\")\n",
        "pyngrok_logger.setLevel(logging.INFO)\n",
        "\n",
        "# --- ì„¤ì •ê°’ ---\n",
        "PORT = 8000\n",
        "LOG_FILE = \"uvicorn_server.log\"\n",
        "STATIC_NGROK_DOMAIN = \"clam-talented-promptly.ngrok-free.app\"  # ì‚¬ìš©ìž ê³ ì • ë„ë©”ì¸\n",
        "MAX_WAIT_SECONDS = 300  # ìµœëŒ€ ëŒ€ê¸° ì‹œê°„\n",
        "READY_KEYWORD = \"Uvicorn running on\"\n",
        "\n",
        "# --- ngrok ë° ì„œë²„ ì¢…ë£Œ ---\n",
        "print(\"ðŸ›  ê¸°ì¡´ ngrok / ì„œë²„ ì¢…ë£Œ ì‹œë„...\")\n",
        "try:\n",
        "    ngrok.kill()\n",
        "    print(\"âœ… ngrok ì¢…ë£Œ ì™„ë£Œ\")\n",
        "except Exception as e:\n",
        "    print(f\"âš ï¸ ngrok ì¢…ë£Œ ì¤‘ ì˜¤ë¥˜ (ë¬´ì‹œ ê°€ëŠ¥): {e}\")\n",
        "\n",
        "subprocess.run(['pkill', '-f', 'uvicorn main_server:app'], stderr=subprocess.DEVNULL)\n",
        "subprocess.run(['pkill', '-f', f'ngrok.*http.*{PORT}'], stderr=subprocess.DEVNULL)\n",
        "subprocess.run(['pkill', '-f', '/root/.config/ngrok/ngrok'], stderr=subprocess.DEVNULL)\n",
        "time.sleep(3)\n",
        "\n",
        "# --- ngrok ê³ ì • ë„ë©”ì¸ ì—°ê²° ---\n",
        "print(f\"ðŸŒ ngrok ê³ ì • ë„ë©”ì¸ ì—°ê²° ì‹œë„: {STATIC_NGROK_DOMAIN}...\")\n",
        "public_url = ngrok.connect(\n",
        "    addr=PORT,\n",
        "    proto=\"http\",\n",
        "    domain=STATIC_NGROK_DOMAIN\n",
        ").public_url\n",
        "print(f\"âœ… ngrok ì—°ê²° ì™„ë£Œ: {public_url}\")\n",
        "\n",
        "# âœ… ngrok URLì„ í™˜ê²½ ë³€ìˆ˜ë¡œ ë“±ë¡ (main_server.pyì—ì„œ ì‚¬ìš©í•¨)\n",
        "os.environ[\"NGROK_STATIC_URL\"] = public_url\n",
        "\n",
        "# --- FastAPI ì„œë²„ ë°±ê·¸ë¼ìš´ë“œ ì‹¤í–‰ ---\n",
        "print(f\"ðŸš€ FastAPI ì„œë²„ ì‹¤í–‰ ì¤‘... ë¡œê·¸ íŒŒì¼: {LOG_FILE}\")\n",
        "if os.path.exists(LOG_FILE):\n",
        "    os.remove(LOG_FILE)\n",
        "\n",
        "subprocess.Popen(\n",
        "    f\"nohup python main_server.py > {LOG_FILE} 2>&1 &\",\n",
        "    shell=True\n",
        ")\n",
        "time.sleep(5)  # ì´ˆê¸° ë¶€íŒ… ëŒ€ê¸°\n",
        "\n",
        "# --- ì„œë²„ ì¤€ë¹„ ëŒ€ê¸° ---\n",
        "print(f\"â³ ëª¨ë¸ ë¡œë”© ëŒ€ê¸° ì¤‘... ('{READY_KEYWORD}' ê°ì§€)\")\n",
        "ready_detected = False\n",
        "\n",
        "for i in range(MAX_WAIT_SECONDS):\n",
        "    print(f\"{i+1}\", end=\" \", flush=True)\n",
        "    if (i + 1) % 30 == 0:\n",
        "        print()\n",
        "\n",
        "    if (i + 1) % 5 == 0 and os.path.exists(LOG_FILE):\n",
        "        try:\n",
        "            with open(LOG_FILE, 'r') as f:\n",
        "                if READY_KEYWORD in f.read():\n",
        "                    print(\"\\nâœ… ì„œë²„ ì¤€ë¹„ ì™„ë£Œ!\")\n",
        "                    ready_detected = True\n",
        "                    break\n",
        "        except Exception as e:\n",
        "            print(f\"\\nâš ï¸ ë¡œê·¸ í™•ì¸ ì˜¤ë¥˜: {e}\")\n",
        "\n",
        "    time.sleep(1)\n",
        "\n",
        "if not ready_detected:\n",
        "    print(\"\\nâš ï¸ ìµœëŒ€ ëŒ€ê¸° ì‹œê°„ ì´ˆê³¼: ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨ ê°€ëŠ¥ì„± ìžˆìŒ\")\n",
        "\n",
        "# --- ì•ˆì •í™” ëŒ€ê¸° ---\n",
        "print(\"âŒ› ì•ˆì •í™”ë¥¼ ìœ„í•´ 10ì´ˆ ëŒ€ê¸° ì¤‘...\")\n",
        "time.sleep(10)\n",
        "\n",
        "# --- ê²°ê³¼ ì¶œë ¥ ---\n",
        "print(f\"\\nðŸŽ¯ ì„œë²„ ì‹¤í–‰ ì™„ë£Œ! ì™¸ë¶€ ì ‘ì† URL:\")\n",
        "print(f\"ðŸ”— {public_url}\")\n",
        "\n",
        "print(\"\\nðŸ“‹ í˜„ìž¬ ì‹¤í–‰ ì¤‘ì¸ í”„ë¡œì„¸ìŠ¤:\")\n",
        "!ps -ef | grep -E \"main_server.py|ngrok\" | grep -v grep\n",
        "\n",
        "g_end = time.time()\n",
        "print(f\"\\nTotal time : {(g_end - g_start):.2f} seconds\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qrax2DLnOrYR",
        "outputId": "c311dd6b-89ee-4fb1-c8a8-25f247f24026"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tue May 27 08:04:45 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  NVIDIA A100-SXM4-40GB          Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   33C    P0             46W /  400W |       5MiB /  40960MiB |      0%      Default |\n",
            "|                                         |                        |             Disabled |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "QGXTSJyYllKv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        },
        "cellView": "form",
        "outputId": "2af44e42-0c26-4239-93d9-d712c78e1694"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n# ì´ ì…€ì€ FastAPI ì„œë²„ ì½”ë“œë¥¼ main_server.py íŒŒì¼ë¡œ ì €ìž¥í•©ë‹ˆë‹¤.\\n%%writefile main_server.py\\nimport os\\nimport io\\nimport logging\\nimport gc\\nfrom contextlib import asynccontextmanager\\nfrom typing import Optional, Any\\n\\nimport torch\\nimport uvicorn\\nfrom fastapi import FastAPI, HTTPException, UploadFile, File, Form\\nfrom fastapi.responses import Response\\nfrom pydantic import BaseModel\\nfrom PIL import Image\\nfrom pyngrok import ngrok\\n\\n# Diffusers - Flux + ControlNet\\nfrom diffusers import FluxPipeline, FluxControlNetModel, FluxControlNetPipeline\\nfrom controlnet_aux import CannyDetector\\n\\n# IP-Adapter\\nfrom pipeline_flux_ipa import FluxPipeline as FluxPipelineIP\\nfrom transformer_flux import FluxTransformer2DModel\\nfrom infer_flux_ipa_siglip import resize_img, IPAdapter\\n\\n# --- Configuration ---\\nBASE_MODEL_ID = \"black-forest-labs/FLUX.1-dev\"\\nCONTROLNET_MODEL_ID = \"Shakker-Labs/FLUX.1-dev-ControlNet-Union-Pro-2.0\"\\nIMAGE_ENCODER_PATH = \"google/siglip-so400m-patch14-384\"\\nIPADAPTER_PATH = \"./ip-adapter.bin\"\\n\\nDEFAULT_STEPS = 30\\nDEFAULT_GUIDANCE_SCALE = 3.5\\nDEFAULT_CONTROLNET_SCALE = 0.7\\nDEFAULT_IPADAPTER_SCALE = 0.7\\nIMAGE_WIDTH = 1024\\nIMAGE_HEIGHT = 1024\\nPORT = 8000\\n\\n# --- Global State ---\\nbase_pipe: Optional[FluxPipeline] = None\\ncontrolnet_pipe: Optional[FluxControlNetPipeline] = None\\ncontrolnet_preprocessor: Optional[Any] = None\\npipe_ip: Optional[FluxPipelineIP] = None\\nip_model: Optional[IPAdapter] = None\\ndevice: Optional[str] = None\\n\\nlogging.basicConfig(level=logging.INFO)\\nlogger = logging.getLogger(__name__)\\n\\n# --- Helper Functions ---\\ndef load_pil_image(image_bytes: bytes) -> Image.Image:\\n    return Image.open(io.BytesIO(image_bytes)).convert(\"RGB\")\\n\\ndef image_to_bytes(image: Image.Image) -> bytes:\\n    byte_arr = io.BytesIO()\\n    image.save(byte_arr, format=\\'PNG\\')\\n    byte_arr.seek(0)\\n    return byte_arr.getvalue()\\n\\ndef get_generator(seed: Optional[int] = None) -> torch.Generator:\\n    if seed is None:\\n        seed = torch.randint(0, 2**32 - 1, (1,)).item()\\n    logger.info(f\"Using seed: {seed}\")\\n    return torch.Generator(device=device).manual_seed(seed)\\n\\nasync def prepare_reference_image(uploaded_image: UploadFile) -> Image.Image:\\n    image = load_pil_image(await uploaded_image.read())\\n    return resize_img(image)\\n\\ndef prepare_control_image(uploaded_image: UploadFile) -> Image.Image:\\n    if controlnet_preprocessor is None:\\n        raise RuntimeError(\"ControlNet preprocessor not loaded.\")\\n    image = load_pil_image(uploaded_image.file.read())\\n    control_image = controlnet_preprocessor(image)\\n    return control_image\\n\\n# --- Model Loading ---\\ndef load_models():\\n    global base_pipe, controlnet_pipe, controlnet_preprocessor, pipe_ip, ip_model, device\\n\\n    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\\n    logger.info(f\"Using device: {device}\")\\n    dtype = torch.bfloat16 if device == \"cuda\" and torch.cuda.is_bf16_supported() else torch.float16\\n\\n    colab_log_flag = \\'O\\'\\n    try:\\n        logger.info(\"Loading Flux base model...\")\\n        base_pipe = FluxPipeline.from_pretrained(BASE_MODEL_ID, torch_dtype=dtype)\\n        base_pipe.to(\"cpu\")\\n        base_pipe.enable_model_cpu_offload()\\n        base_pipe.enable_attention_slicing()\\n\\n        logger.info(\"Loading Flux ControlNet model...\")\\n        controlnet_model = FluxControlNetModel.from_pretrained(CONTROLNET_MODEL_ID, torch_dtype=dtype)\\n        controlnet_pipe = FluxControlNetPipeline.from_pretrained(BASE_MODEL_ID, controlnet=controlnet_model, torch_dtype=dtype)\\n        controlnet_pipe.to(\"cpu\")\\n        controlnet_pipe.enable_model_cpu_offload()\\n        controlnet_pipe.enable_attention_slicing()\\n\\n        logger.info(\"Loading Canny Preprocessor...\")\\n        controlnet_preprocessor = CannyDetector()\\n\\n        logger.info(\"Loading Flux IP-Adapter model...\")\\n        transformer = FluxTransformer2DModel.from_pretrained(BASE_MODEL_ID, subfolder=\"transformer\", torch_dtype=dtype)\\n        pipe_ip = FluxPipelineIP.from_pretrained(BASE_MODEL_ID, transformer=transformer, torch_dtype=dtype)\\n        ip_model = IPAdapter(\\n            pipe_ip,\\n            IMAGE_ENCODER_PATH,\\n            IPADAPTER_PATH,\\n            device=device,\\n            num_tokens=128\\n        )\\n        pipe_ip.to(\"cpu\")\\n        pipe_ip.enable_model_cpu_offload()\\n        pipe_ip.enable_attention_slicing()\\n\\n    except Exception as e:\\n        logger.exception(\"Fatal error during model loading\")\\n        colab_log_flag = \\'X\\'\\n        raise RuntimeError(f\"Failed to load models: {e}\")\\n\\n    logger.info(\"âœ… All models loaded.\")\\n    print(colab_log_flag)\\n\\n# --- FastAPI Setup ---\\n@asynccontextmanager\\nasync def lifespan(app: FastAPI):\\n    logger.info(\"Application startup...\")\\n    load_models()\\n    ngrok_auth_token = os.environ.get(\"NGROK_AUTHTOKEN\")\\n    if ngrok_auth_token:\\n        public_url = ngrok.connect(PORT, \"http\")\\n        logger.info(f\"Ngrok tunnel active at: {public_url}\")\\n        app.state.ngrok_url = public_url\\n    else:\\n        app.state.ngrok_url = None\\n    yield\\n    logger.info(\"Application shutdown...\")\\n    ngrok.kill()\\n    global base_pipe, controlnet_pipe, pipe_ip\\n    del base_pipe, controlnet_pipe, pipe_ip\\n    if device == \"cuda\":\\n        torch.cuda.empty_cache()\\n\\napp = FastAPI(lifespan=lifespan, title=\"Flux Unified API\")\\n\\n# --- Pydantic Model ---\\nclass TextToImageRequest(BaseModel):\\n    prompt: str\\n    negative_prompt: Optional[str] = \"\"\\n    num_inference_steps: int = DEFAULT_STEPS\\n    guidance_scale: float = DEFAULT_GUIDANCE_SCALE\\n    seed: Optional[int] = None\\n\\n# --- API Endpoints ---\\n@app.get(\"/\")\\nasync def read_root():\\n    return {\\n        \"message\": \"Flux Unified API running.\",\\n        \"device\": device,\\n        \"ngrok_url\": app.state.ngrok_url if hasattr(app.state, \"ngrok_url\") else None\\n    }\\n\\n@app.post(\"/generate/text-to-image\", response_class=Response)\\nasync def generate_text_to_image(request: TextToImageRequest):\\n    if base_pipe is None:\\n        raise HTTPException(status_code=503, detail=\"Base pipeline not ready.\")\\n    base_pipe.to(\"cuda\")\\n    generator = get_generator(request.seed)\\n\\n    try:\\n        with torch.inference_mode():\\n            result = base_pipe(\\n                prompt=request.prompt,\\n                negative_prompt=request.negative_prompt,\\n                width=IMAGE_WIDTH,\\n                height=IMAGE_HEIGHT,\\n                num_inference_steps=request.num_inference_steps,\\n                guidance_scale=request.guidance_scale,\\n                generator=generator,\\n            )\\n        output_image = result.images[0]\\n        img_bytes = image_to_bytes(output_image)\\n        return Response(content=img_bytes, media_type=\"image/png\")\\n    finally:\\n        base_pipe.to(\"cpu\")\\n        torch.cuda.empty_cache()\\n        gc.collect()\\n\\n@app.post(\"/generate/image-to-image\", response_class=Response)\\nasync def generate_image_to_image(\\n    prompt: str = Form(...),\\n    negative_prompt: Optional[str] = Form(\"\"),\\n    controlnet_scale: float = Form(DEFAULT_CONTROLNET_SCALE),\\n    num_inference_steps: int = Form(DEFAULT_STEPS),\\n    guidance_scale: float = Form(DEFAULT_GUIDANCE_SCALE),\\n    seed: Optional[int] = Form(None),\\n    image: UploadFile = File(...)\\n):\\n    if controlnet_pipe is None:\\n        raise HTTPException(status_code=503, detail=\"ControlNet pipeline not ready.\")\\n\\n    controlnet_pipe.to(\"cuda\")\\n    control_image = prepare_control_image(image)\\n    generator = get_generator(seed)\\n\\n    try:\\n        with torch.inference_mode():\\n            result = controlnet_pipe(\\n                prompt=prompt,\\n                image=control_image,\\n                width=IMAGE_WIDTH,\\n                height=IMAGE_HEIGHT,\\n                num_inference_steps=num_inference_steps,\\n                guidance_scale=guidance_scale,\\n                controlnet_conditioning_scale=controlnet_scale,\\n                generator=generator,\\n            )\\n        output_image = result.images[0]\\n        img_bytes = image_to_bytes(output_image)\\n        return Response(content=img_bytes, media_type=\"image/png\")\\n    finally:\\n        controlnet_pipe.to(\"cpu\")\\n        torch.cuda.empty_cache()\\n        gc.collect()\\n\\n@app.post(\"/generate/ip-adapter-image\", response_class=Response)\\nasync def generate_ip_adapter_image(\\n    prompt: str = Form(...),\\n    ipadapter_scale: float = Form(DEFAULT_IPADAPTER_SCALE),\\n    num_inference_steps: int = Form(DEFAULT_STEPS),\\n    seed: Optional[int] = Form(None),\\n    image: UploadFile = File(...)\\n):\\n    if ip_model is None:\\n        raise HTTPException(status_code=503, detail=\"IP-Adapter not ready.\")\\n\\n    pipe_ip.to(\"cuda\")\\n    reference_image = await prepare_reference_image(image)\\n\\n    try:\\n        with torch.inference_mode():\\n            output_images = ip_model.generate(\\n                pil_image=reference_image,\\n                prompt=prompt,\\n                scale=ipadapter_scale,\\n                width=IMAGE_WIDTH,\\n                height=IMAGE_HEIGHT,\\n                seed=seed,\\n            )\\n        output_image = output_images[0]\\n        img_bytes = image_to_bytes(output_image)\\n        return Response(content=img_bytes, media_type=\"image/png\")\\n    finally:\\n        pipe_ip.to(\"cpu\")\\n        torch.cuda.empty_cache()\\n        gc.collect()\\n\\n# --- Main ---\\nif __name__ == \"__main__\":\\n    logger.info(\"Starting Uvicorn server...\")\\n    uvicorn.run(\"main_server:app\", host=\"0.0.0.0\", port=PORT, reload=False)\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "# @title [êµ¬ë²„ì „ - to('cuda') ì‚¬ìš© ì¤‘] FastAPI ì• í”Œë¦¬ì¼€ì´ì…˜ ì½”ë“œ ìž‘ì„± (main_server.py íŒŒì¼ ìƒì„±)\n",
        "'''\n",
        "# ì´ ì…€ì€ FastAPI ì„œë²„ ì½”ë“œë¥¼ main_server.py íŒŒì¼ë¡œ ì €ìž¥í•©ë‹ˆë‹¤.\n",
        "%%writefile main_server.py\n",
        "import os\n",
        "import io\n",
        "import logging\n",
        "import gc\n",
        "from contextlib import asynccontextmanager\n",
        "from typing import Optional, Any\n",
        "\n",
        "import torch\n",
        "import uvicorn\n",
        "from fastapi import FastAPI, HTTPException, UploadFile, File, Form\n",
        "from fastapi.responses import Response\n",
        "from pydantic import BaseModel\n",
        "from PIL import Image\n",
        "from pyngrok import ngrok\n",
        "\n",
        "# Diffusers - Flux + ControlNet\n",
        "from diffusers import FluxPipeline, FluxControlNetModel, FluxControlNetPipeline\n",
        "from controlnet_aux import CannyDetector\n",
        "\n",
        "# IP-Adapter\n",
        "from pipeline_flux_ipa import FluxPipeline as FluxPipelineIP\n",
        "from transformer_flux import FluxTransformer2DModel\n",
        "from infer_flux_ipa_siglip import resize_img, IPAdapter\n",
        "\n",
        "# --- Configuration ---\n",
        "BASE_MODEL_ID = \"black-forest-labs/FLUX.1-dev\"\n",
        "CONTROLNET_MODEL_ID = \"Shakker-Labs/FLUX.1-dev-ControlNet-Union-Pro-2.0\"\n",
        "IMAGE_ENCODER_PATH = \"google/siglip-so400m-patch14-384\"\n",
        "IPADAPTER_PATH = \"./ip-adapter.bin\"\n",
        "\n",
        "DEFAULT_STEPS = 30\n",
        "DEFAULT_GUIDANCE_SCALE = 3.5\n",
        "DEFAULT_CONTROLNET_SCALE = 0.7\n",
        "DEFAULT_IPADAPTER_SCALE = 0.7\n",
        "IMAGE_WIDTH = 1024\n",
        "IMAGE_HEIGHT = 1024\n",
        "PORT = 8000\n",
        "\n",
        "# --- Global State ---\n",
        "base_pipe: Optional[FluxPipeline] = None\n",
        "controlnet_pipe: Optional[FluxControlNetPipeline] = None\n",
        "controlnet_preprocessor: Optional[Any] = None\n",
        "pipe_ip: Optional[FluxPipelineIP] = None\n",
        "ip_model: Optional[IPAdapter] = None\n",
        "device: Optional[str] = None\n",
        "\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# --- Helper Functions ---\n",
        "def load_pil_image(image_bytes: bytes) -> Image.Image:\n",
        "    return Image.open(io.BytesIO(image_bytes)).convert(\"RGB\")\n",
        "\n",
        "def image_to_bytes(image: Image.Image) -> bytes:\n",
        "    byte_arr = io.BytesIO()\n",
        "    image.save(byte_arr, format='PNG')\n",
        "    byte_arr.seek(0)\n",
        "    return byte_arr.getvalue()\n",
        "\n",
        "def get_generator(seed: Optional[int] = None) -> torch.Generator:\n",
        "    if seed is None:\n",
        "        seed = torch.randint(0, 2**32 - 1, (1,)).item()\n",
        "    logger.info(f\"Using seed: {seed}\")\n",
        "    return torch.Generator(device=device).manual_seed(seed)\n",
        "\n",
        "async def prepare_reference_image(uploaded_image: UploadFile) -> Image.Image:\n",
        "    image = load_pil_image(await uploaded_image.read())\n",
        "    return resize_img(image)\n",
        "\n",
        "def prepare_control_image(uploaded_image: UploadFile) -> Image.Image:\n",
        "    if controlnet_preprocessor is None:\n",
        "        raise RuntimeError(\"ControlNet preprocessor not loaded.\")\n",
        "    image = load_pil_image(uploaded_image.file.read())\n",
        "    control_image = controlnet_preprocessor(image)\n",
        "    return control_image\n",
        "\n",
        "# --- Model Loading ---\n",
        "def load_models():\n",
        "    global base_pipe, controlnet_pipe, controlnet_preprocessor, pipe_ip, ip_model, device\n",
        "\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    logger.info(f\"Using device: {device}\")\n",
        "    dtype = torch.bfloat16 if device == \"cuda\" and torch.cuda.is_bf16_supported() else torch.float16\n",
        "\n",
        "    colab_log_flag = 'O'\n",
        "    try:\n",
        "        logger.info(\"Loading Flux base model...\")\n",
        "        base_pipe = FluxPipeline.from_pretrained(BASE_MODEL_ID, torch_dtype=dtype)\n",
        "        base_pipe.to(\"cpu\")\n",
        "        base_pipe.enable_model_cpu_offload()\n",
        "        base_pipe.enable_attention_slicing()\n",
        "\n",
        "        logger.info(\"Loading Flux ControlNet model...\")\n",
        "        controlnet_model = FluxControlNetModel.from_pretrained(CONTROLNET_MODEL_ID, torch_dtype=dtype)\n",
        "        controlnet_pipe = FluxControlNetPipeline.from_pretrained(BASE_MODEL_ID, controlnet=controlnet_model, torch_dtype=dtype)\n",
        "        controlnet_pipe.to(\"cpu\")\n",
        "        controlnet_pipe.enable_model_cpu_offload()\n",
        "        controlnet_pipe.enable_attention_slicing()\n",
        "\n",
        "        logger.info(\"Loading Canny Preprocessor...\")\n",
        "        controlnet_preprocessor = CannyDetector()\n",
        "\n",
        "        logger.info(\"Loading Flux IP-Adapter model...\")\n",
        "        transformer = FluxTransformer2DModel.from_pretrained(BASE_MODEL_ID, subfolder=\"transformer\", torch_dtype=dtype)\n",
        "        pipe_ip = FluxPipelineIP.from_pretrained(BASE_MODEL_ID, transformer=transformer, torch_dtype=dtype)\n",
        "        ip_model = IPAdapter(\n",
        "            pipe_ip,\n",
        "            IMAGE_ENCODER_PATH,\n",
        "            IPADAPTER_PATH,\n",
        "            device=device,\n",
        "            num_tokens=128\n",
        "        )\n",
        "        pipe_ip.to(\"cpu\")\n",
        "        pipe_ip.enable_model_cpu_offload()\n",
        "        pipe_ip.enable_attention_slicing()\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.exception(\"Fatal error during model loading\")\n",
        "        colab_log_flag = 'X'\n",
        "        raise RuntimeError(f\"Failed to load models: {e}\")\n",
        "\n",
        "    logger.info(\"âœ… All models loaded.\")\n",
        "    print(colab_log_flag)\n",
        "\n",
        "# --- FastAPI Setup ---\n",
        "@asynccontextmanager\n",
        "async def lifespan(app: FastAPI):\n",
        "    logger.info(\"Application startup...\")\n",
        "    load_models()\n",
        "    ngrok_auth_token = os.environ.get(\"NGROK_AUTHTOKEN\")\n",
        "    if ngrok_auth_token:\n",
        "        public_url = ngrok.connect(PORT, \"http\")\n",
        "        logger.info(f\"Ngrok tunnel active at: {public_url}\")\n",
        "        app.state.ngrok_url = public_url\n",
        "    else:\n",
        "        app.state.ngrok_url = None\n",
        "    yield\n",
        "    logger.info(\"Application shutdown...\")\n",
        "    ngrok.kill()\n",
        "    global base_pipe, controlnet_pipe, pipe_ip\n",
        "    del base_pipe, controlnet_pipe, pipe_ip\n",
        "    if device == \"cuda\":\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "app = FastAPI(lifespan=lifespan, title=\"Flux Unified API\")\n",
        "\n",
        "# --- Pydantic Model ---\n",
        "class TextToImageRequest(BaseModel):\n",
        "    prompt: str\n",
        "    negative_prompt: Optional[str] = \"\"\n",
        "    num_inference_steps: int = DEFAULT_STEPS\n",
        "    guidance_scale: float = DEFAULT_GUIDANCE_SCALE\n",
        "    seed: Optional[int] = None\n",
        "\n",
        "# --- API Endpoints ---\n",
        "@app.get(\"/\")\n",
        "async def read_root():\n",
        "    return {\n",
        "        \"message\": \"Flux Unified API running.\",\n",
        "        \"device\": device,\n",
        "        \"ngrok_url\": app.state.ngrok_url if hasattr(app.state, \"ngrok_url\") else None\n",
        "    }\n",
        "\n",
        "@app.post(\"/generate/text-to-image\", response_class=Response)\n",
        "async def generate_text_to_image(request: TextToImageRequest):\n",
        "    if base_pipe is None:\n",
        "        raise HTTPException(status_code=503, detail=\"Base pipeline not ready.\")\n",
        "    base_pipe.to(\"cuda\")\n",
        "    generator = get_generator(request.seed)\n",
        "\n",
        "    try:\n",
        "        with torch.inference_mode():\n",
        "            result = base_pipe(\n",
        "                prompt=request.prompt,\n",
        "                negative_prompt=request.negative_prompt,\n",
        "                width=IMAGE_WIDTH,\n",
        "                height=IMAGE_HEIGHT,\n",
        "                num_inference_steps=request.num_inference_steps,\n",
        "                guidance_scale=request.guidance_scale,\n",
        "                generator=generator,\n",
        "            )\n",
        "        output_image = result.images[0]\n",
        "        img_bytes = image_to_bytes(output_image)\n",
        "        return Response(content=img_bytes, media_type=\"image/png\")\n",
        "    finally:\n",
        "        base_pipe.to(\"cpu\")\n",
        "        torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "\n",
        "@app.post(\"/generate/image-to-image\", response_class=Response)\n",
        "async def generate_image_to_image(\n",
        "    prompt: str = Form(...),\n",
        "    negative_prompt: Optional[str] = Form(\"\"),\n",
        "    controlnet_scale: float = Form(DEFAULT_CONTROLNET_SCALE),\n",
        "    num_inference_steps: int = Form(DEFAULT_STEPS),\n",
        "    guidance_scale: float = Form(DEFAULT_GUIDANCE_SCALE),\n",
        "    seed: Optional[int] = Form(None),\n",
        "    image: UploadFile = File(...)\n",
        "):\n",
        "    if controlnet_pipe is None:\n",
        "        raise HTTPException(status_code=503, detail=\"ControlNet pipeline not ready.\")\n",
        "\n",
        "    controlnet_pipe.to(\"cuda\")\n",
        "    control_image = prepare_control_image(image)\n",
        "    generator = get_generator(seed)\n",
        "\n",
        "    try:\n",
        "        with torch.inference_mode():\n",
        "            result = controlnet_pipe(\n",
        "                prompt=prompt,\n",
        "                image=control_image,\n",
        "                width=IMAGE_WIDTH,\n",
        "                height=IMAGE_HEIGHT,\n",
        "                num_inference_steps=num_inference_steps,\n",
        "                guidance_scale=guidance_scale,\n",
        "                controlnet_conditioning_scale=controlnet_scale,\n",
        "                generator=generator,\n",
        "            )\n",
        "        output_image = result.images[0]\n",
        "        img_bytes = image_to_bytes(output_image)\n",
        "        return Response(content=img_bytes, media_type=\"image/png\")\n",
        "    finally:\n",
        "        controlnet_pipe.to(\"cpu\")\n",
        "        torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "\n",
        "@app.post(\"/generate/ip-adapter-image\", response_class=Response)\n",
        "async def generate_ip_adapter_image(\n",
        "    prompt: str = Form(...),\n",
        "    ipadapter_scale: float = Form(DEFAULT_IPADAPTER_SCALE),\n",
        "    num_inference_steps: int = Form(DEFAULT_STEPS),\n",
        "    seed: Optional[int] = Form(None),\n",
        "    image: UploadFile = File(...)\n",
        "):\n",
        "    if ip_model is None:\n",
        "        raise HTTPException(status_code=503, detail=\"IP-Adapter not ready.\")\n",
        "\n",
        "    pipe_ip.to(\"cuda\")\n",
        "    reference_image = await prepare_reference_image(image)\n",
        "\n",
        "    try:\n",
        "        with torch.inference_mode():\n",
        "            output_images = ip_model.generate(\n",
        "                pil_image=reference_image,\n",
        "                prompt=prompt,\n",
        "                scale=ipadapter_scale,\n",
        "                width=IMAGE_WIDTH,\n",
        "                height=IMAGE_HEIGHT,\n",
        "                seed=seed,\n",
        "            )\n",
        "        output_image = output_images[0]\n",
        "        img_bytes = image_to_bytes(output_image)\n",
        "        return Response(content=img_bytes, media_type=\"image/png\")\n",
        "    finally:\n",
        "        pipe_ip.to(\"cpu\")\n",
        "        torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "\n",
        "# --- Main ---\n",
        "if __name__ == \"__main__\":\n",
        "    logger.info(\"Starting Uvicorn server...\")\n",
        "    uvicorn.run(\"main_server:app\", host=\"0.0.0.0\", port=PORT, reload=False)\n",
        "'''"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}