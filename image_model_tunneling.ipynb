{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "BPe9QPfqlgwD"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "g_start = time.time()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "cellView": "form",
        "id": "w-k2065nuVgM"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "# @title 1. ÎùºÏù¥Î∏åÎü¨Î¶¨ ÏÑ§Ïπò\n",
        "%pip install -q fastapi uvicorn[standard]\n",
        "%pip install -q pyngrok>=7.0.0 # diffusers>=0.27.0 transformers accelerate\n",
        "%pip install -q controlnet_aux Pillow # torch>=2.0.0\n",
        "%pip install -q safetensors pydantic huggingface_hub python-multipart\n",
        "# %pip install -U diffusers transformers accelerate safetensors\n",
        "%pip install -q wget\n",
        "# %pip install -q peft requests\n",
        "\n",
        "# # xformers ÏÇ¨Ïö© Ïãú (Î≤ÑÏ†Ñ Ìò∏Ìôò ÌôïÏù∏ ÌïÑÏöî)\n",
        "# %pip install -q fastapi uvicorn[standard] pyngrok>=7.0.0  # 1) FastAPI / ÏÑúÎ≤ÑÏö©\n",
        "# %pip install -q diffusers==0.33.1 transformers==4.51.3 accelerate==1.6.0  # 2) Diffusers + Transformers + Accelerate\n",
        "# %pip install -q controlnet_aux Pillow safetensors pydantic huggingface_hub python-multipart # 3) FluxÏö© Î∂ÄÍ∞Ä Í∏∞Îä•Îì§\n",
        "# %pip install -q ninja wget  # 4) Í∏∞ÌÉÄ ÎèÑÍµ¨\n",
        "\n",
        "# %pip uninstall -y xformers  # 5) Í∏∞Ï°¥ xformers Ï†úÍ±∞\n",
        "# %pip install -q git+https://github.com/facebookresearch/xformers.git  # 6) Colab(PyTorch 2.6.0+cu124) ÌôòÍ≤ΩÏóê ÎßûÏ∂∞ xFormersÎ•º ÏÜåÏä§ÏóêÏÑú ÎπåÎìú ÏÑ§Ïπò\n",
        "# # %pip install -q peft requests\n",
        "\n",
        "print(\"‚úÖ ÎùºÏù¥Î∏åÎü¨Î¶¨ ÏÑ§Ïπò ÏôÑÎ£å\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7AY8e66XRJzV",
        "outputId": "43ef0033-28fc-4ab8-c0c4-dc7e03678f9e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ipython 7.34.0 requires jedi, which is not installed.\n"
          ]
        }
      ],
      "source": [
        "# ÎùºÏù¥Î∏åÎü¨Î¶¨ Ìò∏ÌôòÏÑ± ÌôïÏù∏\n",
        "# jediÎäî ÏÉÅÍ¥Ä ÏóÜÏùå (colab ÌäπÏÑ± ÏÉÅ, ÏÑ§Ïπò ÏïàÌïúÎã§Í≥† Ìï®)\n",
        "!pip check"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A0jI1SIruVeF",
        "outputId": "e751afaa-f7f2-4c5b-de02-469cea995a5e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîë Hugging Face Î°úÍ∑∏Ïù∏ Ï§ë...\n",
            "‚úÖ Ngrok Authtoken ÏÑ§Ï†ï ÏôÑÎ£å\n"
          ]
        }
      ],
      "source": [
        "# @title 2. Hugging Face Î°úÍ∑∏Ïù∏ / Ngrok ÏÑ§Ï†ï (Authtoken ÏãúÌÅ¨Î¶ø ÌÇ§)\n",
        "import os\n",
        "from google.colab import userdata\n",
        "from huggingface_hub import login\n",
        "from pyngrok import conf, ngrok\n",
        "\n",
        "# Hugging Face Î∞è ngrok ÌÜ†ÌÅ∞ (Colab SecretÏóêÏÑú Í∞ÄÏ†∏Ïò§Í∏∞)\n",
        "hf_token = userdata.get(\"HF_TOKEN\")        # Hugging Face Token\n",
        "civitai_token = userdata.get(\"CIVITAI_TOKEN\")   # CivitAI Token\n",
        "ngrok_token = userdata.get(\"google_ngrok_authtoken\")  # ngrok Token\n",
        "\n",
        "if hf_token == None:\n",
        "  print('x')\n",
        "if civitai_token == None:\n",
        "  print('x')\n",
        "if ngrok_token == None:\n",
        "  print('x')\n",
        "\n",
        "# Ìè¨Ìä∏ ÏÑ§Ï†ï\n",
        "PORT = 8000\n",
        "\n",
        "# hugging face Î°úÍ∑∏Ïù∏\n",
        "print(\"üîë Hugging Face Î°úÍ∑∏Ïù∏ Ï§ë...\")\n",
        "login(token=hf_token)\n",
        "\n",
        "# ngrok ÏÑ§Ï†ï\n",
        "ngrok.set_auth_token(ngrok_token)\n",
        "print(\"‚úÖ Ngrok Authtoken ÏÑ§Ï†ï ÏôÑÎ£å\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2R0izjLyvHfL",
        "outputId": "8b1ee9e4-a393-46e6-c71c-5c2e64392d26"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-05-27 07:59:43--  https://civitai.com/api/download/models/1215918?type=Model&format=SafeTensor&size=full&token=2ab1653b9d579f186d295f4540908157\n",
            "Resolving civitai.com (civitai.com)... 172.67.12.143, 104.22.19.237, 104.22.18.237, ...\n",
            "Connecting to civitai.com (civitai.com)|172.67.12.143|:443... connected.\n",
            "HTTP request sent, awaiting response... 307 Temporary Redirect\n",
            "Location: https://civitai-delivery-worker-prod.5ac0637cfd0766c97916cefa3764fbdf.r2.cloudflarestorage.com/model/686417/jGhibliV2FluxUltimate.FZyi.safetensors?X-Amz-Expires=86400&response-content-disposition=attachment%3B%20filename%3D%22IllustrationJuanerGhibli_v20.safetensors%22&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=e01358d793ad6966166af8b3064953ad/20250527/us-east-1/s3/aws4_request&X-Amz-Date=20250527T075943Z&X-Amz-SignedHeaders=host&X-Amz-Signature=25b9f1b92a6ee9c35a0a1b75d6c6e6212a2f6d39ee80f8b967bf28d3df17658d [following]\n",
            "--2025-05-27 07:59:44--  https://civitai-delivery-worker-prod.5ac0637cfd0766c97916cefa3764fbdf.r2.cloudflarestorage.com/model/686417/jGhibliV2FluxUltimate.FZyi.safetensors?X-Amz-Expires=86400&response-content-disposition=attachment%3B%20filename%3D%22IllustrationJuanerGhibli_v20.safetensors%22&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=e01358d793ad6966166af8b3064953ad/20250527/us-east-1/s3/aws4_request&X-Amz-Date=20250527T075943Z&X-Amz-SignedHeaders=host&X-Amz-Signature=25b9f1b92a6ee9c35a0a1b75d6c6e6212a2f6d39ee80f8b967bf28d3df17658d\n",
            "Resolving civitai-delivery-worker-prod.5ac0637cfd0766c97916cefa3764fbdf.r2.cloudflarestorage.com (civitai-delivery-worker-prod.5ac0637cfd0766c97916cefa3764fbdf.r2.cloudflarestorage.com)... 162.159.141.50, 172.66.1.46, 2606:4700:7::12e, ...\n",
            "Connecting to civitai-delivery-worker-prod.5ac0637cfd0766c97916cefa3764fbdf.r2.cloudflarestorage.com (civitai-delivery-worker-prod.5ac0637cfd0766c97916cefa3764fbdf.r2.cloudflarestorage.com)|162.159.141.50|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 11901497256 (11G)\n",
            "Saving to: ‚Äòillustration_juaner_flux.safetensors‚Äô\n",
            "\n",
            "illustration_juaner 100%[===================>]  11.08G  34.3MB/s    in 3m 14s  \n",
            "\n",
            "2025-05-27 08:02:58 (58.4 MB/s) - ‚Äòillustration_juaner_flux.safetensors‚Äô saved [11901497256/11901497256]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# @title 3. CivitAI Checkpoint Îã§Ïö¥Î°úÎìú\n",
        "import os\n",
        "os.environ[\"CIVITAI_API_TOKEN\"] = civitai_token\n",
        "\n",
        "import wget\n",
        "!wget -O illustration_juaner_flux.safetensors \\\n",
        "\"https://civitai.com/api/download/models/1215918?type=Model&format=SafeTensor&size=full&token=$CIVITAI_API_TOKEN\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 4. Model Pipeline Ï†ïÏùò - model_loader.py\n",
        "# Ïù¥ ÏÖÄÏùÄ Model PipelineÏùÑ ÏÉùÏÑ±ÌïòÎäî ÏΩîÎìúÎ•º model_loader.py ÌååÏùºÎ°ú Ï†ÄÏû•Ìï©ÎãàÎã§.\n",
        "\n",
        "%%writefile model_loader.py\n",
        "from diffusers import FluxPipeline, DiffusionPipeline, FluxTransformer2DModel\n",
        "from transformers import T5EncoderModel\n",
        "\n",
        "# Ghibli (Flux)\n",
        "def load_ghibli_flux_model(dtype):\n",
        "    BASE_MODEL_ID = \"black-forest-labs/FLUX.1-dev\"\n",
        "    CHECKPOINT_MODEL_ID = \"illustration_juaner_flux.safetensors\"\n",
        "\n",
        "    transformer = FluxTransformer2DModel.from_single_file(\n",
        "        CHECKPOINT_MODEL_ID, torch_dtype=dtype\n",
        "    )\n",
        "    text_encoder_2 = T5EncoderModel.from_pretrained(\n",
        "        BASE_MODEL_ID, subfolder=\"text_encoder_2\", torch_dtype=dtype\n",
        "    )\n",
        "    pipe = FluxPipeline.from_pretrained(\n",
        "        BASE_MODEL_ID,\n",
        "        transformer=transformer,\n",
        "        text_encoder_2=text_encoder_2,\n",
        "        torch_dtype=dtype,\n",
        "    )\n",
        "    pipe.enable_model_cpu_offload()\n",
        "    pipe.enable_attention_slicing()\n",
        "    return pipe\n",
        "\n",
        "# AnythingXL (stable-diffusion-xl style)\n",
        "def load_anything_xl_model(dtype):\n",
        "    MODEL_ID = \"stabilityai/stable-diffusion-xl-base-1.0\"\n",
        "    pipe = DiffusionPipeline.from_pretrained(\n",
        "        MODEL_ID,\n",
        "        torch_dtype=dtype,\n",
        "        variant=\"fp16\"\n",
        "    )\n",
        "    pipe.enable_model_cpu_offload()\n",
        "    pipe.enable_attention_slicing()\n",
        "    return pipe\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EWf3DH_gKQcO",
        "outputId": "d6776f76-eb8e-42e7-cf7d-c8667823fc39"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing model_loader.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XXvUlWb9HRcD",
        "outputId": "70905741-5568-4ceb-b86c-870f24da26b3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing main_server.py\n"
          ]
        }
      ],
      "source": [
        "# @title 5. FastAPI ÏÑúÎ≤Ñ - main_server.py\n",
        "# Ïù¥ ÏÖÄÏùÄ FastAPI ÏÑúÎ≤Ñ ÏΩîÎìúÎ•º main_server.py ÌååÏùºÎ°ú Ï†ÄÏû•Ìï©ÎãàÎã§.\n",
        "\n",
        "%%writefile main_server.py\n",
        "import os, io, gc, logging\n",
        "from contextlib import asynccontextmanager\n",
        "from typing import Optional, Any\n",
        "\n",
        "import torch\n",
        "import uvicorn\n",
        "from fastapi import FastAPI, HTTPException\n",
        "from fastapi.responses import Response\n",
        "from pydantic import BaseModel\n",
        "from PIL import Image\n",
        "from pyngrok import ngrok\n",
        "\n",
        "\n",
        "# --- GPU ÏÑ§Ï†ï ---\n",
        "device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "dtype: torch.dtype = torch.bfloat16 if (device == \"cuda\" and torch.cuda.is_bf16_supported()) else torch.float16\n",
        "\n",
        "# --- ÏÑ§Ï†ï ÏÉÅÏàò ---\n",
        "DEFAULT_STEPS = 30\n",
        "DEFAULT_GUIDANCE_SCALE = 3.5\n",
        "IMAGE_WIDTH = 1024\n",
        "IMAGE_HEIGHT = 1024\n",
        "PORT = 8000\n",
        "\n",
        "# --- Î°úÍπÖ ÏÑ§Ï†ï ---\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# --- Ïú†Ìã∏ Ìï®Ïàò ---\n",
        "def load_pil_image(image_bytes: bytes) -> Image.Image:\n",
        "    return Image.open(io.BytesIO(image_bytes)).convert(\"RGB\")\n",
        "\n",
        "def image_to_bytes(image: Image.Image) -> bytes:\n",
        "    byte_arr = io.BytesIO()\n",
        "    image.save(byte_arr, format='PNG')\n",
        "    byte_arr.seek(0)\n",
        "    return byte_arr.getvalue()\n",
        "\n",
        "def get_generator(seed: Optional[int] = None) -> torch.Generator:\n",
        "    if seed is None:\n",
        "        seed = torch.randint(0, 2**32 - 1, (1,)).item()\n",
        "    logger.info(f\"Using seed: {seed}\")\n",
        "    return torch.Generator(device=\"cuda\").manual_seed(seed)\n",
        "\n",
        "# -------------------------- Î™®Îç∏ ÏÑ∏ÌåÖ Í¥ÄÎ†® --------------------------\n",
        "# Î™®Îç∏ Î°úÎî© Î™®Îìà import\n",
        "from model_loader import load_ghibli_flux_model, load_anything_xl_model\n",
        "\n",
        "# --- Ï†ÑÏó≠ pipe ÏÑ∏ÌåÖ ---\n",
        "ghibli_flux_pipe: Optional[Any] = None\n",
        "anything_xl_pipe: Optional[Any] = None\n",
        "\n",
        "# --- Î™®Îç∏ Î°úÎî© ---\n",
        "def load_models():\n",
        "    global ghibli_flux_pipe, anything_xl_pipe\n",
        "    ghibli_flux_pipe = load_ghibli_flux_model(dtype)\n",
        "    anything_xl_pipe = load_anything_xl_model(dtype)\n",
        "\n",
        "# --- Î™®Îç∏ Ìï¥Ï≤¥ ---\n",
        "# lifespanÏóêÏÑú del Ìò∏Ï∂ú ÌïÑÏöî\n",
        "\n",
        "# ----------------------------------------------------------------\n",
        "\n",
        "\n",
        "# --- ÏÑúÎ≤Ñ ÏÉùÎ™ÖÏ£ºÍ∏∞ ---\n",
        "@asynccontextmanager\n",
        "async def lifespan(app: FastAPI):\n",
        "    logger.info(\"App starting...\")\n",
        "    load_models()\n",
        "\n",
        "    public_url = os.environ.get(\"NGROK_STATIC_URL\", None)\n",
        "    app.state.ngrok_url = public_url\n",
        "    logger.info(f\"Ngrok (external) URL registered: {public_url}\")\n",
        "\n",
        "    yield\n",
        "\n",
        "    logger.info(\"App shutting down...\")\n",
        "    del ghibli_flux_pipe, anything_xl_pipe\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "# --- FastAPI Ïï± ÏÉùÏÑ± ---\n",
        "app = FastAPI(lifespan=lifespan)\n",
        "\n",
        "# --- ÏöîÏ≤≠ Î™®Îç∏ ---\n",
        "class TextToImageRequest(BaseModel):\n",
        "    model_name: str\n",
        "    prompt: str\n",
        "    negative_prompt: Optional[str] = \"\"\n",
        "    num_inference_steps: int = DEFAULT_STEPS\n",
        "    guidance_scale: float = DEFAULT_GUIDANCE_SCALE\n",
        "    seed: Optional[int] = None\n",
        "\n",
        "# --- Î£®Ìä∏ ÌôïÏù∏ ---\n",
        "@app.get(\"/\")\n",
        "async def read_root():\n",
        "    return {\n",
        "        \"message\": \"Image Model(text to image) API is running.\",\n",
        "        \"device\": device,\n",
        "        \"ngrok_url\": app.state.ngrok_url if hasattr(app.state, \"ngrok_url\") else None,\n",
        "    }\n",
        "\n",
        "# --- ÌÖçÏä§Ìä∏ ‚Üí Ïù¥ÎØ∏ÏßÄ ---\n",
        "@app.post(\"/generate/text-to-image\", response_class=Response)\n",
        "async def generate_text_to_image(request: TextToImageRequest):\n",
        "    if request.model_name == \"ghibli-flux\":\n",
        "        if ghibli_flux_pipe is None:\n",
        "            raise HTTPException(status_code=503, detail=\"Ghibli-Flux pipeline not loaded.\")\n",
        "        pipe = ghibli_flux_pipe\n",
        "    elif request.model_name == \"anything-xl\":\n",
        "        if anything_xl_pipe is None:\n",
        "            raise HTTPException(status_code=503, detail=\"Anything-XL pipeline not loaded.\")\n",
        "        pipe = anything_xl_pipe\n",
        "    else:\n",
        "        raise HTTPException(status_code=400, detail=f\"Invalid mode: {request.mode}\")\n",
        "\n",
        "    generator = get_generator(request.seed)\n",
        "\n",
        "    try:\n",
        "        with torch.inference_mode():\n",
        "            result = pipe(\n",
        "                prompt=request.prompt,\n",
        "                negative_prompt=request.negative_prompt,\n",
        "                width=IMAGE_WIDTH,\n",
        "                height=IMAGE_HEIGHT,\n",
        "                num_inference_steps=request.num_inference_steps,\n",
        "                guidance_scale=request.guidance_scale,\n",
        "                generator=generator,\n",
        "            )\n",
        "            image_bytes = image_to_bytes(result.images[0])\n",
        "    finally:\n",
        "        del result\n",
        "        torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "\n",
        "    return Response(content=image_bytes, media_type=\"image/png\")\n",
        "\n",
        "# --- ÏÑúÎ≤Ñ Ïã§Ìñâ ---\n",
        "if __name__ == \"__main__\":\n",
        "    logger.info(\"Starting Uvicorn server...\")\n",
        "    uvicorn.run(\"main_server:app\", host=\"0.0.0.0\", port=PORT, reload=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9hmI4LjHXoTF",
        "outputId": "77e6b4c7-fd43-4e63-bdad-3e62f9798fce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ ngrok ÌîÑÎ°úÏÑ∏Ïä§ Ï¢ÖÎ£å ÏôÑÎ£å\n",
            "üßπ GPU Î©îÎ™®Î¶¨ Î∞è Ï∫êÏãú Ï†ïÎ¶¨ ÏôÑÎ£å\n",
            "‚úÖ FastAPI ÏÑúÎ≤Ñ ÌîÑÎ°úÏÑ∏Ïä§ Ï¢ÖÎ£å ÏôÑÎ£å\n"
          ]
        }
      ],
      "source": [
        "# @title Fin.\n",
        "import subprocess\n",
        "from pyngrok import ngrok\n",
        "\n",
        "# ngrok ÌîÑÎ°úÏÑ∏Ïä§ Ï¢ÖÎ£å\n",
        "try:\n",
        "    ngrok.kill()\n",
        "    print(\"‚úÖ ngrok ÌîÑÎ°úÏÑ∏Ïä§ Ï¢ÖÎ£å ÏôÑÎ£å\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ö†Ô∏è ngrok Ï¢ÖÎ£å Ï§ë Ïò§Î•ò (Î¨¥Ïãú Í∞ÄÎä•): {e}\")\n",
        "\n",
        "# uvicorn ÏÑúÎ≤Ñ ÌîÑÎ°úÏÑ∏Ïä§ Ï¢ÖÎ£å\n",
        "subprocess.run(['pkill', '-f', 'uvicorn main_server:app'], stderr=subprocess.DEVNULL)\n",
        "subprocess.run(['pkill', '-f', 'ngrok'], stderr=subprocess.DEVNULL)\n",
        "\n",
        "import gc\n",
        "import torch\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "\n",
        "print(\"üßπ GPU Î©îÎ™®Î¶¨ Î∞è Ï∫êÏãú Ï†ïÎ¶¨ ÏôÑÎ£å\")\n",
        "\n",
        "\n",
        "!ps -ef | grep -E \"main_server.py|ngrok\" | grep -v grep\n",
        "\n",
        "\n",
        "\n",
        "import subprocess\n",
        "\n",
        "# main_server.py Ï¢ÖÎ£å\n",
        "subprocess.run(['pkill', '-f', 'main_server.py'])\n",
        "print(\"‚úÖ FastAPI ÏÑúÎ≤Ñ ÌîÑÎ°úÏÑ∏Ïä§ Ï¢ÖÎ£å ÏôÑÎ£å\")\n",
        "\n",
        "!ps -ef | grep -E \"main_server.py\" | grep -v grep\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "oCqXd-e1uVX7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ed44abf3-ffe9-4d05-d0bf-5868d129a0c6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "env: PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True\n",
            "üõ† Í∏∞Ï°¥ ngrok / ÏÑúÎ≤Ñ Ï¢ÖÎ£å ÏãúÎèÑ...\n",
            "‚úÖ ngrok Ï¢ÖÎ£å ÏôÑÎ£å\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:pyngrok.ngrok:Opening tunnel named: http-8000-edf0a590-8e40-4bcc-a40c-4d4603db2c36\n",
            "INFO:pyngrok.process.ngrok:t=2025-05-27T08:03:05+0000 lvl=info msg=\"no configuration paths supplied\"\n",
            "INFO:pyngrok.process.ngrok:t=2025-05-27T08:03:05+0000 lvl=info msg=\"using configuration at default config path\" path=/root/.config/ngrok/ngrok.yml\n",
            "INFO:pyngrok.process.ngrok:t=2025-05-27T08:03:05+0000 lvl=info msg=\"open config file\" path=/root/.config/ngrok/ngrok.yml err=nil\n",
            "INFO:pyngrok.process.ngrok:t=2025-05-27T08:03:05+0000 lvl=info msg=\"starting web service\" obj=web addr=127.0.0.1:4040 allow_hosts=[]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üåê ngrok Í≥†Ï†ï ÎèÑÎ©îÏù∏ Ïó∞Í≤∞ ÏãúÎèÑ: clam-talented-promptly.ngrok-free.app...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:pyngrok.process.ngrok:t=2025-05-27T08:03:06+0000 lvl=info msg=\"client session established\" obj=tunnels.session\n",
            "INFO:pyngrok.process.ngrok:t=2025-05-27T08:03:06+0000 lvl=info msg=\"tunnel session started\" obj=tunnels.session\n",
            "INFO:pyngrok.process.ngrok:t=2025-05-27T08:03:06+0000 lvl=info msg=start pg=/api/tunnels id=09d7b73e5be89387\n",
            "INFO:pyngrok.process.ngrok:t=2025-05-27T08:03:06+0000 lvl=info msg=end pg=/api/tunnels id=09d7b73e5be89387 status=200 dur=300.023¬µs\n",
            "INFO:pyngrok.process.ngrok:t=2025-05-27T08:03:06+0000 lvl=info msg=start pg=/api/tunnels id=a32c1be786dbd981\n",
            "INFO:pyngrok.process.ngrok:t=2025-05-27T08:03:06+0000 lvl=info msg=end pg=/api/tunnels id=a32c1be786dbd981 status=200 dur=112.174¬µs\n",
            "INFO:pyngrok.process.ngrok:t=2025-05-27T08:03:06+0000 lvl=info msg=start pg=/api/tunnels id=e7cad0985e1cd2d8\n",
            "INFO:pyngrok.process.ngrok:t=2025-05-27T08:03:06+0000 lvl=info msg=\"started tunnel\" obj=tunnels name=http-8000-edf0a590-8e40-4bcc-a40c-4d4603db2c36 addr=http://localhost:8000 url=https://clam-talented-promptly.ngrok-free.app\n",
            "INFO:pyngrok.process.ngrok:t=2025-05-27T08:03:06+0000 lvl=info msg=end pg=/api/tunnels id=e7cad0985e1cd2d8 status=201 dur=247.304805ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ ngrok Ïó∞Í≤∞ ÏôÑÎ£å: https://clam-talented-promptly.ngrok-free.app\n",
            "üöÄ FastAPI ÏÑúÎ≤Ñ Ïã§Ìñâ Ï§ë... Î°úÍ∑∏ ÌååÏùº: uvicorn_server.log\n",
            "‚è≥ Î™®Îç∏ Î°úÎî© ÎåÄÍ∏∞ Ï§ë... ('Uvicorn running on' Í∞êÏßÄ)\n",
            "1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 \n",
            "31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 \n",
            "61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 \n",
            "‚úÖ ÏÑúÎ≤Ñ Ï§ÄÎπÑ ÏôÑÎ£å!\n",
            "‚åõ ÏïàÏ†ïÌôîÎ•º ÏúÑÌï¥ 10Ï¥à ÎåÄÍ∏∞ Ï§ë...\n",
            "\n",
            "üéØ ÏÑúÎ≤Ñ Ïã§Ìñâ ÏôÑÎ£å! Ïô∏Î∂Ä Ï†ëÏÜç URL:\n",
            "üîó https://clam-talented-promptly.ngrok-free.app\n",
            "\n",
            "üìã ÌòÑÏû¨ Ïã§Ìñâ Ï§ëÏù∏ ÌîÑÎ°úÏÑ∏Ïä§:\n",
            "root        3922    2408  0 08:03 ?        00:00:00 /root/.config/ngrok/ngrok start --none --log stdout\n",
            "root        3951       1 99 08:03 ?        00:02:32 python3 main_server.py\n",
            "\n",
            "Total time : 398.92 seconds\n"
          ]
        }
      ],
      "source": [
        "# @title üöÄ ngrok Ïó∞Í≤∞ Î∞è FastAPI ÏÑúÎ≤Ñ Ïã§Ìñâ\n",
        "# ÌôòÍ≤Ω Î≥ÄÏàò ÏÑ§Ï†ï\n",
        "%env PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True\n",
        "\n",
        "import time\n",
        "import os\n",
        "import subprocess\n",
        "import logging\n",
        "from pyngrok import ngrok\n",
        "\n",
        "# --- Î°úÍπÖ ÏÑ§Ï†ï ---\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "pyngrok_logger = logging.getLogger(\"pyngrok\")\n",
        "pyngrok_logger.setLevel(logging.INFO)\n",
        "\n",
        "# --- ÏÑ§Ï†ïÍ∞í ---\n",
        "PORT = 8000\n",
        "LOG_FILE = \"uvicorn_server.log\"\n",
        "STATIC_NGROK_DOMAIN = \"clam-talented-promptly.ngrok-free.app\"  # ÏÇ¨Ïö©Ïûê Í≥†Ï†ï ÎèÑÎ©îÏù∏\n",
        "MAX_WAIT_SECONDS = 300  # ÏµúÎåÄ ÎåÄÍ∏∞ ÏãúÍ∞Ñ\n",
        "READY_KEYWORD = \"Uvicorn running on\"\n",
        "\n",
        "# --- ngrok Î∞è ÏÑúÎ≤Ñ Ï¢ÖÎ£å ---\n",
        "print(\"üõ† Í∏∞Ï°¥ ngrok / ÏÑúÎ≤Ñ Ï¢ÖÎ£å ÏãúÎèÑ...\")\n",
        "try:\n",
        "    ngrok.kill()\n",
        "    print(\"‚úÖ ngrok Ï¢ÖÎ£å ÏôÑÎ£å\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ö†Ô∏è ngrok Ï¢ÖÎ£å Ï§ë Ïò§Î•ò (Î¨¥Ïãú Í∞ÄÎä•): {e}\")\n",
        "\n",
        "subprocess.run(['pkill', '-f', 'uvicorn main_server:app'], stderr=subprocess.DEVNULL)\n",
        "subprocess.run(['pkill', '-f', f'ngrok.*http.*{PORT}'], stderr=subprocess.DEVNULL)\n",
        "subprocess.run(['pkill', '-f', '/root/.config/ngrok/ngrok'], stderr=subprocess.DEVNULL)\n",
        "time.sleep(3)\n",
        "\n",
        "# --- ngrok Í≥†Ï†ï ÎèÑÎ©îÏù∏ Ïó∞Í≤∞ ---\n",
        "print(f\"üåê ngrok Í≥†Ï†ï ÎèÑÎ©îÏù∏ Ïó∞Í≤∞ ÏãúÎèÑ: {STATIC_NGROK_DOMAIN}...\")\n",
        "public_url = ngrok.connect(\n",
        "    addr=PORT,\n",
        "    proto=\"http\",\n",
        "    domain=STATIC_NGROK_DOMAIN\n",
        ").public_url\n",
        "print(f\"‚úÖ ngrok Ïó∞Í≤∞ ÏôÑÎ£å: {public_url}\")\n",
        "\n",
        "# ‚úÖ ngrok URLÏùÑ ÌôòÍ≤Ω Î≥ÄÏàòÎ°ú Îì±Î°ù (main_server.pyÏóêÏÑú ÏÇ¨Ïö©Ìï®)\n",
        "os.environ[\"NGROK_STATIC_URL\"] = public_url\n",
        "\n",
        "# --- FastAPI ÏÑúÎ≤Ñ Î∞±Í∑∏ÎùºÏö¥Îìú Ïã§Ìñâ ---\n",
        "print(f\"üöÄ FastAPI ÏÑúÎ≤Ñ Ïã§Ìñâ Ï§ë... Î°úÍ∑∏ ÌååÏùº: {LOG_FILE}\")\n",
        "if os.path.exists(LOG_FILE):\n",
        "    os.remove(LOG_FILE)\n",
        "\n",
        "subprocess.Popen(\n",
        "    f\"nohup python main_server.py > {LOG_FILE} 2>&1 &\",\n",
        "    shell=True\n",
        ")\n",
        "time.sleep(5)  # Ï¥àÍ∏∞ Î∂ÄÌåÖ ÎåÄÍ∏∞\n",
        "\n",
        "# --- ÏÑúÎ≤Ñ Ï§ÄÎπÑ ÎåÄÍ∏∞ ---\n",
        "print(f\"‚è≥ Î™®Îç∏ Î°úÎî© ÎåÄÍ∏∞ Ï§ë... ('{READY_KEYWORD}' Í∞êÏßÄ)\")\n",
        "ready_detected = False\n",
        "\n",
        "for i in range(MAX_WAIT_SECONDS):\n",
        "    print(f\"{i+1}\", end=\" \", flush=True)\n",
        "    if (i + 1) % 30 == 0:\n",
        "        print()\n",
        "\n",
        "    if (i + 1) % 5 == 0 and os.path.exists(LOG_FILE):\n",
        "        try:\n",
        "            with open(LOG_FILE, 'r') as f:\n",
        "                if READY_KEYWORD in f.read():\n",
        "                    print(\"\\n‚úÖ ÏÑúÎ≤Ñ Ï§ÄÎπÑ ÏôÑÎ£å!\")\n",
        "                    ready_detected = True\n",
        "                    break\n",
        "        except Exception as e:\n",
        "            print(f\"\\n‚ö†Ô∏è Î°úÍ∑∏ ÌôïÏù∏ Ïò§Î•ò: {e}\")\n",
        "\n",
        "    time.sleep(1)\n",
        "\n",
        "if not ready_detected:\n",
        "    print(\"\\n‚ö†Ô∏è ÏµúÎåÄ ÎåÄÍ∏∞ ÏãúÍ∞Ñ Ï¥àÍ≥º: Î™®Îç∏ Î°úÎî© Ïã§Ìå® Í∞ÄÎä•ÏÑ± ÏûàÏùå\")\n",
        "\n",
        "# --- ÏïàÏ†ïÌôî ÎåÄÍ∏∞ ---\n",
        "print(\"‚åõ ÏïàÏ†ïÌôîÎ•º ÏúÑÌï¥ 10Ï¥à ÎåÄÍ∏∞ Ï§ë...\")\n",
        "time.sleep(10)\n",
        "\n",
        "# --- Í≤∞Í≥º Ï∂úÎ†• ---\n",
        "print(f\"\\nüéØ ÏÑúÎ≤Ñ Ïã§Ìñâ ÏôÑÎ£å! Ïô∏Î∂Ä Ï†ëÏÜç URL:\")\n",
        "print(f\"üîó {public_url}\")\n",
        "\n",
        "print(\"\\nüìã ÌòÑÏû¨ Ïã§Ìñâ Ï§ëÏù∏ ÌîÑÎ°úÏÑ∏Ïä§:\")\n",
        "!ps -ef | grep -E \"main_server.py|ngrok\" | grep -v grep\n",
        "\n",
        "g_end = time.time()\n",
        "print(f\"\\nTotal time : {(g_end - g_start):.2f} seconds\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qrax2DLnOrYR",
        "outputId": "c311dd6b-89ee-4fb1-c8a8-25f247f24026"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tue May 27 08:04:45 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  NVIDIA A100-SXM4-40GB          Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   33C    P0             46W /  400W |       5MiB /  40960MiB |      0%      Default |\n",
            "|                                         |                        |             Disabled |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "QGXTSJyYllKv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        },
        "cellView": "form",
        "outputId": "2af44e42-0c26-4239-93d9-d712c78e1694"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n# Ïù¥ ÏÖÄÏùÄ FastAPI ÏÑúÎ≤Ñ ÏΩîÎìúÎ•º main_server.py ÌååÏùºÎ°ú Ï†ÄÏû•Ìï©ÎãàÎã§.\\n%%writefile main_server.py\\nimport os\\nimport io\\nimport logging\\nimport gc\\nfrom contextlib import asynccontextmanager\\nfrom typing import Optional, Any\\n\\nimport torch\\nimport uvicorn\\nfrom fastapi import FastAPI, HTTPException, UploadFile, File, Form\\nfrom fastapi.responses import Response\\nfrom pydantic import BaseModel\\nfrom PIL import Image\\nfrom pyngrok import ngrok\\n\\n# Diffusers - Flux + ControlNet\\nfrom diffusers import FluxPipeline, FluxControlNetModel, FluxControlNetPipeline\\nfrom controlnet_aux import CannyDetector\\n\\n# IP-Adapter\\nfrom pipeline_flux_ipa import FluxPipeline as FluxPipelineIP\\nfrom transformer_flux import FluxTransformer2DModel\\nfrom infer_flux_ipa_siglip import resize_img, IPAdapter\\n\\n# --- Configuration ---\\nBASE_MODEL_ID = \"black-forest-labs/FLUX.1-dev\"\\nCONTROLNET_MODEL_ID = \"Shakker-Labs/FLUX.1-dev-ControlNet-Union-Pro-2.0\"\\nIMAGE_ENCODER_PATH = \"google/siglip-so400m-patch14-384\"\\nIPADAPTER_PATH = \"./ip-adapter.bin\"\\n\\nDEFAULT_STEPS = 30\\nDEFAULT_GUIDANCE_SCALE = 3.5\\nDEFAULT_CONTROLNET_SCALE = 0.7\\nDEFAULT_IPADAPTER_SCALE = 0.7\\nIMAGE_WIDTH = 1024\\nIMAGE_HEIGHT = 1024\\nPORT = 8000\\n\\n# --- Global State ---\\nbase_pipe: Optional[FluxPipeline] = None\\ncontrolnet_pipe: Optional[FluxControlNetPipeline] = None\\ncontrolnet_preprocessor: Optional[Any] = None\\npipe_ip: Optional[FluxPipelineIP] = None\\nip_model: Optional[IPAdapter] = None\\ndevice: Optional[str] = None\\n\\nlogging.basicConfig(level=logging.INFO)\\nlogger = logging.getLogger(__name__)\\n\\n# --- Helper Functions ---\\ndef load_pil_image(image_bytes: bytes) -> Image.Image:\\n    return Image.open(io.BytesIO(image_bytes)).convert(\"RGB\")\\n\\ndef image_to_bytes(image: Image.Image) -> bytes:\\n    byte_arr = io.BytesIO()\\n    image.save(byte_arr, format=\\'PNG\\')\\n    byte_arr.seek(0)\\n    return byte_arr.getvalue()\\n\\ndef get_generator(seed: Optional[int] = None) -> torch.Generator:\\n    if seed is None:\\n        seed = torch.randint(0, 2**32 - 1, (1,)).item()\\n    logger.info(f\"Using seed: {seed}\")\\n    return torch.Generator(device=device).manual_seed(seed)\\n\\nasync def prepare_reference_image(uploaded_image: UploadFile) -> Image.Image:\\n    image = load_pil_image(await uploaded_image.read())\\n    return resize_img(image)\\n\\ndef prepare_control_image(uploaded_image: UploadFile) -> Image.Image:\\n    if controlnet_preprocessor is None:\\n        raise RuntimeError(\"ControlNet preprocessor not loaded.\")\\n    image = load_pil_image(uploaded_image.file.read())\\n    control_image = controlnet_preprocessor(image)\\n    return control_image\\n\\n# --- Model Loading ---\\ndef load_models():\\n    global base_pipe, controlnet_pipe, controlnet_preprocessor, pipe_ip, ip_model, device\\n\\n    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\\n    logger.info(f\"Using device: {device}\")\\n    dtype = torch.bfloat16 if device == \"cuda\" and torch.cuda.is_bf16_supported() else torch.float16\\n\\n    colab_log_flag = \\'O\\'\\n    try:\\n        logger.info(\"Loading Flux base model...\")\\n        base_pipe = FluxPipeline.from_pretrained(BASE_MODEL_ID, torch_dtype=dtype)\\n        base_pipe.to(\"cpu\")\\n        base_pipe.enable_model_cpu_offload()\\n        base_pipe.enable_attention_slicing()\\n\\n        logger.info(\"Loading Flux ControlNet model...\")\\n        controlnet_model = FluxControlNetModel.from_pretrained(CONTROLNET_MODEL_ID, torch_dtype=dtype)\\n        controlnet_pipe = FluxControlNetPipeline.from_pretrained(BASE_MODEL_ID, controlnet=controlnet_model, torch_dtype=dtype)\\n        controlnet_pipe.to(\"cpu\")\\n        controlnet_pipe.enable_model_cpu_offload()\\n        controlnet_pipe.enable_attention_slicing()\\n\\n        logger.info(\"Loading Canny Preprocessor...\")\\n        controlnet_preprocessor = CannyDetector()\\n\\n        logger.info(\"Loading Flux IP-Adapter model...\")\\n        transformer = FluxTransformer2DModel.from_pretrained(BASE_MODEL_ID, subfolder=\"transformer\", torch_dtype=dtype)\\n        pipe_ip = FluxPipelineIP.from_pretrained(BASE_MODEL_ID, transformer=transformer, torch_dtype=dtype)\\n        ip_model = IPAdapter(\\n            pipe_ip,\\n            IMAGE_ENCODER_PATH,\\n            IPADAPTER_PATH,\\n            device=device,\\n            num_tokens=128\\n        )\\n        pipe_ip.to(\"cpu\")\\n        pipe_ip.enable_model_cpu_offload()\\n        pipe_ip.enable_attention_slicing()\\n\\n    except Exception as e:\\n        logger.exception(\"Fatal error during model loading\")\\n        colab_log_flag = \\'X\\'\\n        raise RuntimeError(f\"Failed to load models: {e}\")\\n\\n    logger.info(\"‚úÖ All models loaded.\")\\n    print(colab_log_flag)\\n\\n# --- FastAPI Setup ---\\n@asynccontextmanager\\nasync def lifespan(app: FastAPI):\\n    logger.info(\"Application startup...\")\\n    load_models()\\n    ngrok_auth_token = os.environ.get(\"NGROK_AUTHTOKEN\")\\n    if ngrok_auth_token:\\n        public_url = ngrok.connect(PORT, \"http\")\\n        logger.info(f\"Ngrok tunnel active at: {public_url}\")\\n        app.state.ngrok_url = public_url\\n    else:\\n        app.state.ngrok_url = None\\n    yield\\n    logger.info(\"Application shutdown...\")\\n    ngrok.kill()\\n    global base_pipe, controlnet_pipe, pipe_ip\\n    del base_pipe, controlnet_pipe, pipe_ip\\n    if device == \"cuda\":\\n        torch.cuda.empty_cache()\\n\\napp = FastAPI(lifespan=lifespan, title=\"Flux Unified API\")\\n\\n# --- Pydantic Model ---\\nclass TextToImageRequest(BaseModel):\\n    prompt: str\\n    negative_prompt: Optional[str] = \"\"\\n    num_inference_steps: int = DEFAULT_STEPS\\n    guidance_scale: float = DEFAULT_GUIDANCE_SCALE\\n    seed: Optional[int] = None\\n\\n# --- API Endpoints ---\\n@app.get(\"/\")\\nasync def read_root():\\n    return {\\n        \"message\": \"Flux Unified API running.\",\\n        \"device\": device,\\n        \"ngrok_url\": app.state.ngrok_url if hasattr(app.state, \"ngrok_url\") else None\\n    }\\n\\n@app.post(\"/generate/text-to-image\", response_class=Response)\\nasync def generate_text_to_image(request: TextToImageRequest):\\n    if base_pipe is None:\\n        raise HTTPException(status_code=503, detail=\"Base pipeline not ready.\")\\n    base_pipe.to(\"cuda\")\\n    generator = get_generator(request.seed)\\n\\n    try:\\n        with torch.inference_mode():\\n            result = base_pipe(\\n                prompt=request.prompt,\\n                negative_prompt=request.negative_prompt,\\n                width=IMAGE_WIDTH,\\n                height=IMAGE_HEIGHT,\\n                num_inference_steps=request.num_inference_steps,\\n                guidance_scale=request.guidance_scale,\\n                generator=generator,\\n            )\\n        output_image = result.images[0]\\n        img_bytes = image_to_bytes(output_image)\\n        return Response(content=img_bytes, media_type=\"image/png\")\\n    finally:\\n        base_pipe.to(\"cpu\")\\n        torch.cuda.empty_cache()\\n        gc.collect()\\n\\n@app.post(\"/generate/image-to-image\", response_class=Response)\\nasync def generate_image_to_image(\\n    prompt: str = Form(...),\\n    negative_prompt: Optional[str] = Form(\"\"),\\n    controlnet_scale: float = Form(DEFAULT_CONTROLNET_SCALE),\\n    num_inference_steps: int = Form(DEFAULT_STEPS),\\n    guidance_scale: float = Form(DEFAULT_GUIDANCE_SCALE),\\n    seed: Optional[int] = Form(None),\\n    image: UploadFile = File(...)\\n):\\n    if controlnet_pipe is None:\\n        raise HTTPException(status_code=503, detail=\"ControlNet pipeline not ready.\")\\n\\n    controlnet_pipe.to(\"cuda\")\\n    control_image = prepare_control_image(image)\\n    generator = get_generator(seed)\\n\\n    try:\\n        with torch.inference_mode():\\n            result = controlnet_pipe(\\n                prompt=prompt,\\n                image=control_image,\\n                width=IMAGE_WIDTH,\\n                height=IMAGE_HEIGHT,\\n                num_inference_steps=num_inference_steps,\\n                guidance_scale=guidance_scale,\\n                controlnet_conditioning_scale=controlnet_scale,\\n                generator=generator,\\n            )\\n        output_image = result.images[0]\\n        img_bytes = image_to_bytes(output_image)\\n        return Response(content=img_bytes, media_type=\"image/png\")\\n    finally:\\n        controlnet_pipe.to(\"cpu\")\\n        torch.cuda.empty_cache()\\n        gc.collect()\\n\\n@app.post(\"/generate/ip-adapter-image\", response_class=Response)\\nasync def generate_ip_adapter_image(\\n    prompt: str = Form(...),\\n    ipadapter_scale: float = Form(DEFAULT_IPADAPTER_SCALE),\\n    num_inference_steps: int = Form(DEFAULT_STEPS),\\n    seed: Optional[int] = Form(None),\\n    image: UploadFile = File(...)\\n):\\n    if ip_model is None:\\n        raise HTTPException(status_code=503, detail=\"IP-Adapter not ready.\")\\n\\n    pipe_ip.to(\"cuda\")\\n    reference_image = await prepare_reference_image(image)\\n\\n    try:\\n        with torch.inference_mode():\\n            output_images = ip_model.generate(\\n                pil_image=reference_image,\\n                prompt=prompt,\\n                scale=ipadapter_scale,\\n                width=IMAGE_WIDTH,\\n                height=IMAGE_HEIGHT,\\n                seed=seed,\\n            )\\n        output_image = output_images[0]\\n        img_bytes = image_to_bytes(output_image)\\n        return Response(content=img_bytes, media_type=\"image/png\")\\n    finally:\\n        pipe_ip.to(\"cpu\")\\n        torch.cuda.empty_cache()\\n        gc.collect()\\n\\n# --- Main ---\\nif __name__ == \"__main__\":\\n    logger.info(\"Starting Uvicorn server...\")\\n    uvicorn.run(\"main_server:app\", host=\"0.0.0.0\", port=PORT, reload=False)\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "# @title [Íµ¨Î≤ÑÏ†Ñ - to('cuda') ÏÇ¨Ïö© Ï§ë] FastAPI Ïï†ÌîåÎ¶¨ÏºÄÏù¥ÏÖò ÏΩîÎìú ÏûëÏÑ± (main_server.py ÌååÏùº ÏÉùÏÑ±)\n",
        "'''\n",
        "# Ïù¥ ÏÖÄÏùÄ FastAPI ÏÑúÎ≤Ñ ÏΩîÎìúÎ•º main_server.py ÌååÏùºÎ°ú Ï†ÄÏû•Ìï©ÎãàÎã§.\n",
        "%%writefile main_server.py\n",
        "import os\n",
        "import io\n",
        "import logging\n",
        "import gc\n",
        "from contextlib import asynccontextmanager\n",
        "from typing import Optional, Any\n",
        "\n",
        "import torch\n",
        "import uvicorn\n",
        "from fastapi import FastAPI, HTTPException, UploadFile, File, Form\n",
        "from fastapi.responses import Response\n",
        "from pydantic import BaseModel\n",
        "from PIL import Image\n",
        "from pyngrok import ngrok\n",
        "\n",
        "# Diffusers - Flux + ControlNet\n",
        "from diffusers import FluxPipeline, FluxControlNetModel, FluxControlNetPipeline\n",
        "from controlnet_aux import CannyDetector\n",
        "\n",
        "# IP-Adapter\n",
        "from pipeline_flux_ipa import FluxPipeline as FluxPipelineIP\n",
        "from transformer_flux import FluxTransformer2DModel\n",
        "from infer_flux_ipa_siglip import resize_img, IPAdapter\n",
        "\n",
        "# --- Configuration ---\n",
        "BASE_MODEL_ID = \"black-forest-labs/FLUX.1-dev\"\n",
        "CONTROLNET_MODEL_ID = \"Shakker-Labs/FLUX.1-dev-ControlNet-Union-Pro-2.0\"\n",
        "IMAGE_ENCODER_PATH = \"google/siglip-so400m-patch14-384\"\n",
        "IPADAPTER_PATH = \"./ip-adapter.bin\"\n",
        "\n",
        "DEFAULT_STEPS = 30\n",
        "DEFAULT_GUIDANCE_SCALE = 3.5\n",
        "DEFAULT_CONTROLNET_SCALE = 0.7\n",
        "DEFAULT_IPADAPTER_SCALE = 0.7\n",
        "IMAGE_WIDTH = 1024\n",
        "IMAGE_HEIGHT = 1024\n",
        "PORT = 8000\n",
        "\n",
        "# --- Global State ---\n",
        "base_pipe: Optional[FluxPipeline] = None\n",
        "controlnet_pipe: Optional[FluxControlNetPipeline] = None\n",
        "controlnet_preprocessor: Optional[Any] = None\n",
        "pipe_ip: Optional[FluxPipelineIP] = None\n",
        "ip_model: Optional[IPAdapter] = None\n",
        "device: Optional[str] = None\n",
        "\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# --- Helper Functions ---\n",
        "def load_pil_image(image_bytes: bytes) -> Image.Image:\n",
        "    return Image.open(io.BytesIO(image_bytes)).convert(\"RGB\")\n",
        "\n",
        "def image_to_bytes(image: Image.Image) -> bytes:\n",
        "    byte_arr = io.BytesIO()\n",
        "    image.save(byte_arr, format='PNG')\n",
        "    byte_arr.seek(0)\n",
        "    return byte_arr.getvalue()\n",
        "\n",
        "def get_generator(seed: Optional[int] = None) -> torch.Generator:\n",
        "    if seed is None:\n",
        "        seed = torch.randint(0, 2**32 - 1, (1,)).item()\n",
        "    logger.info(f\"Using seed: {seed}\")\n",
        "    return torch.Generator(device=device).manual_seed(seed)\n",
        "\n",
        "async def prepare_reference_image(uploaded_image: UploadFile) -> Image.Image:\n",
        "    image = load_pil_image(await uploaded_image.read())\n",
        "    return resize_img(image)\n",
        "\n",
        "def prepare_control_image(uploaded_image: UploadFile) -> Image.Image:\n",
        "    if controlnet_preprocessor is None:\n",
        "        raise RuntimeError(\"ControlNet preprocessor not loaded.\")\n",
        "    image = load_pil_image(uploaded_image.file.read())\n",
        "    control_image = controlnet_preprocessor(image)\n",
        "    return control_image\n",
        "\n",
        "# --- Model Loading ---\n",
        "def load_models():\n",
        "    global base_pipe, controlnet_pipe, controlnet_preprocessor, pipe_ip, ip_model, device\n",
        "\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    logger.info(f\"Using device: {device}\")\n",
        "    dtype = torch.bfloat16 if device == \"cuda\" and torch.cuda.is_bf16_supported() else torch.float16\n",
        "\n",
        "    colab_log_flag = 'O'\n",
        "    try:\n",
        "        logger.info(\"Loading Flux base model...\")\n",
        "        base_pipe = FluxPipeline.from_pretrained(BASE_MODEL_ID, torch_dtype=dtype)\n",
        "        base_pipe.to(\"cpu\")\n",
        "        base_pipe.enable_model_cpu_offload()\n",
        "        base_pipe.enable_attention_slicing()\n",
        "\n",
        "        logger.info(\"Loading Flux ControlNet model...\")\n",
        "        controlnet_model = FluxControlNetModel.from_pretrained(CONTROLNET_MODEL_ID, torch_dtype=dtype)\n",
        "        controlnet_pipe = FluxControlNetPipeline.from_pretrained(BASE_MODEL_ID, controlnet=controlnet_model, torch_dtype=dtype)\n",
        "        controlnet_pipe.to(\"cpu\")\n",
        "        controlnet_pipe.enable_model_cpu_offload()\n",
        "        controlnet_pipe.enable_attention_slicing()\n",
        "\n",
        "        logger.info(\"Loading Canny Preprocessor...\")\n",
        "        controlnet_preprocessor = CannyDetector()\n",
        "\n",
        "        logger.info(\"Loading Flux IP-Adapter model...\")\n",
        "        transformer = FluxTransformer2DModel.from_pretrained(BASE_MODEL_ID, subfolder=\"transformer\", torch_dtype=dtype)\n",
        "        pipe_ip = FluxPipelineIP.from_pretrained(BASE_MODEL_ID, transformer=transformer, torch_dtype=dtype)\n",
        "        ip_model = IPAdapter(\n",
        "            pipe_ip,\n",
        "            IMAGE_ENCODER_PATH,\n",
        "            IPADAPTER_PATH,\n",
        "            device=device,\n",
        "            num_tokens=128\n",
        "        )\n",
        "        pipe_ip.to(\"cpu\")\n",
        "        pipe_ip.enable_model_cpu_offload()\n",
        "        pipe_ip.enable_attention_slicing()\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.exception(\"Fatal error during model loading\")\n",
        "        colab_log_flag = 'X'\n",
        "        raise RuntimeError(f\"Failed to load models: {e}\")\n",
        "\n",
        "    logger.info(\"‚úÖ All models loaded.\")\n",
        "    print(colab_log_flag)\n",
        "\n",
        "# --- FastAPI Setup ---\n",
        "@asynccontextmanager\n",
        "async def lifespan(app: FastAPI):\n",
        "    logger.info(\"Application startup...\")\n",
        "    load_models()\n",
        "    ngrok_auth_token = os.environ.get(\"NGROK_AUTHTOKEN\")\n",
        "    if ngrok_auth_token:\n",
        "        public_url = ngrok.connect(PORT, \"http\")\n",
        "        logger.info(f\"Ngrok tunnel active at: {public_url}\")\n",
        "        app.state.ngrok_url = public_url\n",
        "    else:\n",
        "        app.state.ngrok_url = None\n",
        "    yield\n",
        "    logger.info(\"Application shutdown...\")\n",
        "    ngrok.kill()\n",
        "    global base_pipe, controlnet_pipe, pipe_ip\n",
        "    del base_pipe, controlnet_pipe, pipe_ip\n",
        "    if device == \"cuda\":\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "app = FastAPI(lifespan=lifespan, title=\"Flux Unified API\")\n",
        "\n",
        "# --- Pydantic Model ---\n",
        "class TextToImageRequest(BaseModel):\n",
        "    prompt: str\n",
        "    negative_prompt: Optional[str] = \"\"\n",
        "    num_inference_steps: int = DEFAULT_STEPS\n",
        "    guidance_scale: float = DEFAULT_GUIDANCE_SCALE\n",
        "    seed: Optional[int] = None\n",
        "\n",
        "# --- API Endpoints ---\n",
        "@app.get(\"/\")\n",
        "async def read_root():\n",
        "    return {\n",
        "        \"message\": \"Flux Unified API running.\",\n",
        "        \"device\": device,\n",
        "        \"ngrok_url\": app.state.ngrok_url if hasattr(app.state, \"ngrok_url\") else None\n",
        "    }\n",
        "\n",
        "@app.post(\"/generate/text-to-image\", response_class=Response)\n",
        "async def generate_text_to_image(request: TextToImageRequest):\n",
        "    if base_pipe is None:\n",
        "        raise HTTPException(status_code=503, detail=\"Base pipeline not ready.\")\n",
        "    base_pipe.to(\"cuda\")\n",
        "    generator = get_generator(request.seed)\n",
        "\n",
        "    try:\n",
        "        with torch.inference_mode():\n",
        "            result = base_pipe(\n",
        "                prompt=request.prompt,\n",
        "                negative_prompt=request.negative_prompt,\n",
        "                width=IMAGE_WIDTH,\n",
        "                height=IMAGE_HEIGHT,\n",
        "                num_inference_steps=request.num_inference_steps,\n",
        "                guidance_scale=request.guidance_scale,\n",
        "                generator=generator,\n",
        "            )\n",
        "        output_image = result.images[0]\n",
        "        img_bytes = image_to_bytes(output_image)\n",
        "        return Response(content=img_bytes, media_type=\"image/png\")\n",
        "    finally:\n",
        "        base_pipe.to(\"cpu\")\n",
        "        torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "\n",
        "@app.post(\"/generate/image-to-image\", response_class=Response)\n",
        "async def generate_image_to_image(\n",
        "    prompt: str = Form(...),\n",
        "    negative_prompt: Optional[str] = Form(\"\"),\n",
        "    controlnet_scale: float = Form(DEFAULT_CONTROLNET_SCALE),\n",
        "    num_inference_steps: int = Form(DEFAULT_STEPS),\n",
        "    guidance_scale: float = Form(DEFAULT_GUIDANCE_SCALE),\n",
        "    seed: Optional[int] = Form(None),\n",
        "    image: UploadFile = File(...)\n",
        "):\n",
        "    if controlnet_pipe is None:\n",
        "        raise HTTPException(status_code=503, detail=\"ControlNet pipeline not ready.\")\n",
        "\n",
        "    controlnet_pipe.to(\"cuda\")\n",
        "    control_image = prepare_control_image(image)\n",
        "    generator = get_generator(seed)\n",
        "\n",
        "    try:\n",
        "        with torch.inference_mode():\n",
        "            result = controlnet_pipe(\n",
        "                prompt=prompt,\n",
        "                image=control_image,\n",
        "                width=IMAGE_WIDTH,\n",
        "                height=IMAGE_HEIGHT,\n",
        "                num_inference_steps=num_inference_steps,\n",
        "                guidance_scale=guidance_scale,\n",
        "                controlnet_conditioning_scale=controlnet_scale,\n",
        "                generator=generator,\n",
        "            )\n",
        "        output_image = result.images[0]\n",
        "        img_bytes = image_to_bytes(output_image)\n",
        "        return Response(content=img_bytes, media_type=\"image/png\")\n",
        "    finally:\n",
        "        controlnet_pipe.to(\"cpu\")\n",
        "        torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "\n",
        "@app.post(\"/generate/ip-adapter-image\", response_class=Response)\n",
        "async def generate_ip_adapter_image(\n",
        "    prompt: str = Form(...),\n",
        "    ipadapter_scale: float = Form(DEFAULT_IPADAPTER_SCALE),\n",
        "    num_inference_steps: int = Form(DEFAULT_STEPS),\n",
        "    seed: Optional[int] = Form(None),\n",
        "    image: UploadFile = File(...)\n",
        "):\n",
        "    if ip_model is None:\n",
        "        raise HTTPException(status_code=503, detail=\"IP-Adapter not ready.\")\n",
        "\n",
        "    pipe_ip.to(\"cuda\")\n",
        "    reference_image = await prepare_reference_image(image)\n",
        "\n",
        "    try:\n",
        "        with torch.inference_mode():\n",
        "            output_images = ip_model.generate(\n",
        "                pil_image=reference_image,\n",
        "                prompt=prompt,\n",
        "                scale=ipadapter_scale,\n",
        "                width=IMAGE_WIDTH,\n",
        "                height=IMAGE_HEIGHT,\n",
        "                seed=seed,\n",
        "            )\n",
        "        output_image = output_images[0]\n",
        "        img_bytes = image_to_bytes(output_image)\n",
        "        return Response(content=img_bytes, media_type=\"image/png\")\n",
        "    finally:\n",
        "        pipe_ip.to(\"cpu\")\n",
        "        torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "\n",
        "# --- Main ---\n",
        "if __name__ == \"__main__\":\n",
        "    logger.info(\"Starting Uvicorn server...\")\n",
        "    uvicorn.run(\"main_server:app\", host=\"0.0.0.0\", port=PORT, reload=False)\n",
        "'''"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}