# ai/run_unified_llm_worker.py

"""
í†µí•© LLM ì›Œì»¤ ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸
ëª¨ë“  ìœ í˜•ì˜ LLM ì‘ì—…ì„ ì²˜ë¦¬í•˜ëŠ” í†µí•© ì›Œì»¤ë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤.
- llm_request_queue: ì¼ë°˜ LLM ì‘ì—… (ê¸°ì¡´ ì›Œí¬í”Œë¡œìš°)
- llm_chat_queue: ì±„íŒ… ì‘ì—…  
- llm_summary_queue: ëŒ€í™” ìš”ì•½ ì‘ì—…
"""

import asyncio
import sys
import os
from pathlib import Path

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ì¶”ê°€
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

from app.workers.llm_worker import main

if __name__ == "__main__":
    print("ğŸš€ Unified LLM Worker ì‹œì‘...")
    print("ì¤‘ì§€í•˜ë ¤ë©´ Ctrl+Cë¥¼ ëˆ„ë¥´ì„¸ìš”.")
    print("ì²˜ë¦¬ ëŒ€ìƒ í:")
    print("  - llm_chat_queue (ì±„íŒ… ì‘ì—…)")
    print("  - llm_summary_queue (ìš”ì•½ ì‘ì—…)")  
    print("  - llm_request_queue (ì¼ë°˜ LLM ì‘ì—…)")
    print()
    
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        print("\nâš ï¸ í†µí•© LLM ì›Œì»¤ê°€ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"\nâŒ ì›Œì»¤ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}")
        sys.exit(1)
