{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "# @title 1. ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜\n",
        "%pip install -q fastapi uvicorn[standard]\n",
        "%pip install -q pyngrok>=7.0.0 diffusers>=0.27.0 transformers accelerate\n",
        "%pip install -q peft controlnet_aux torch>=2.0.0 opencv-python-headless Pillow\n",
        "%pip install -q requests safetensors pydantic huggingface_hub python-multipart\n",
        "\n",
        "print(\"âœ… ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜ ì™„ë£Œ\")"
      ],
      "metadata": {
        "id": "w-k2065nuVgM",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 2. Hugging Face ë¡œê·¸ì¸ / Ngrok ì„¤ì • (Authtoken ì‹œí¬ë¦¿ í‚¤)\n",
        "import os\n",
        "from google.colab import userdata\n",
        "from huggingface_hub import login\n",
        "from pyngrok import conf, ngrok\n",
        "\n",
        "# Hugging Face ë° ngrok í† í° (Colab Secretì—ì„œ ê°€ì ¸ì˜¤ê¸°)\n",
        "hf_token = userdata.get(\"HF_TOKEN\")        # Hugging Face Token\n",
        "ngrok_token = userdata.get(\"NGROK_TOKEN\")  # ngrok Token\n",
        "\n",
        "if hf_token == None:\n",
        "  print('x')\n",
        "if ngrok_token == None:\n",
        "  print('x')\n",
        "\n",
        "# í¬íŠ¸ ì„¤ì •\n",
        "PORT = 8000\n",
        "\n",
        "# hugging face ë¡œê·¸ì¸\n",
        "print(\"ğŸ”‘ Hugging Face ë¡œê·¸ì¸ ì¤‘...\")\n",
        "login(token=hf_token)\n",
        "\n",
        "# ngrok ì„¤ì •\n",
        "if ngrok_token == \"YOUR_NGROK_AUTHTOKEN\":\n",
        "  print(\"âš ï¸ ê²½ê³ : Ngrok Authtokenì„ ì…ë ¥í•˜ì„¸ìš”. ì—†ìœ¼ë©´ Ngrok í„°ë„ì´ ì‘ë™í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.\")\n",
        "  print(\"Ngrok í† í°ì€ https://dashboard.ngrok.com/get-started/your-authtoken ì—ì„œ í™•ì¸ ê°€ëŠ¥í•©ë‹ˆë‹¤.\")\n",
        "else:\n",
        "  os.environ['NGROK_AUTHTOKEN'] = ngrok_token\n",
        "  conf.get_default().auth_token = ngrok_token\n",
        "  print(\"âœ… Ngrok Authtoken ì„¤ì • ì™„ë£Œ\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A0jI1SIruVeF",
        "outputId": "1cd95e60-cc36-4881-ef37-9548c85416a4",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ”‘ Hugging Face ë¡œê·¸ì¸ ì¤‘...\n",
            "âœ… Ngrok Authtoken ì„¤ì • ì™„ë£Œ\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 3. FastAPI ì• í”Œë¦¬ì¼€ì´ì…˜ ì½”ë“œ ì‘ì„± (main_server.py íŒŒì¼ ìƒì„±)\n",
        "# ì´ ì…€ì€ FastAPI ì„œë²„ ì½”ë“œë¥¼ main_server.py íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤.\n",
        "\n",
        "%%writefile main_server.py\n",
        "import os\n",
        "import io\n",
        "import logging\n",
        "from contextlib import asynccontextmanager\n",
        "from typing import Optional, Any\n",
        "\n",
        "import torch\n",
        "import uvicorn\n",
        "from fastapi import FastAPI, HTTPException, UploadFile, File, Form\n",
        "from fastapi.responses import Response\n",
        "from PIL import Image\n",
        "from diffusers import FluxControlNetModel, FluxControlNetPipeline\n",
        "from controlnet_aux import CannyDetector\n",
        "\n",
        "# --- Configuration ---\n",
        "BASE_MODEL_ID = \"black-forest-labs/FLUX.1-dev\"\n",
        "CONTROLNET_MODEL_ID = \"Shakker-Labs/FLUX.1-dev-ControlNet-Union-Pro-2.0\"\n",
        "DEFAULT_STEPS = 30\n",
        "DEFAULT_GUIDANCE_SCALE = 3.5\n",
        "DEFAULT_CONTROLNET_SCALE = 0.7\n",
        "DEFAULT_LORA_SCALE = 0.8\n",
        "IMAGE_WIDTH = 1024\n",
        "IMAGE_HEIGHT = 1024\n",
        "PORT = 8000\n",
        "\n",
        "# --- Global State ---\n",
        "controlnet_pipe: Optional[FluxControlNetPipeline] = None\n",
        "controlnet_preprocessor: Optional[Any] = None\n",
        "device: Optional[str] = None\n",
        "\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# --- Helper Functions ---\n",
        "def load_pil_image(image_bytes: bytes) -> Image.Image:\n",
        "    image = Image.open(io.BytesIO(image_bytes)).convert(\"RGB\")\n",
        "    return image\n",
        "\n",
        "def image_to_bytes(image: Image.Image) -> bytes:\n",
        "    byte_arr = io.BytesIO()\n",
        "    image.save(byte_arr, format='PNG')\n",
        "    byte_arr.seek(0)\n",
        "    return byte_arr.getvalue()\n",
        "\n",
        "def get_generator(seed: Optional[int] = None) -> torch.Generator:\n",
        "    if seed is None:\n",
        "        seed = torch.randint(0, 2**32 - 1, (1,)).item()\n",
        "    logger.info(f\"Using seed: {seed}\")\n",
        "    return torch.Generator(device=device).manual_seed(seed)\n",
        "\n",
        "def prepare_control_image(uploaded_image: UploadFile) -> Image.Image:\n",
        "    if controlnet_preprocessor is None:\n",
        "        raise RuntimeError(\"ControlNet preprocessor not loaded.\")\n",
        "    image = load_pil_image(uploaded_image.file.read())\n",
        "    control_image = controlnet_preprocessor(image)\n",
        "    return control_image\n",
        "\n",
        "# --- Model Loading ---\n",
        "def load_models():\n",
        "    global controlnet_pipe, controlnet_preprocessor, device\n",
        "\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    logger.info(f\"Using device: {device}\")\n",
        "    dtype = torch.bfloat16 if device == \"cuda\" and torch.cuda.is_bf16_supported() else torch.float16\n",
        "\n",
        "    try:\n",
        "        logger.info(\"Loading ControlNet model...\")\n",
        "        controlnet_model = FluxControlNetModel.from_pretrained(CONTROLNET_MODEL_ID, torch_dtype=dtype)\n",
        "        controlnet_pipe = FluxControlNetPipeline.from_pretrained(BASE_MODEL_ID, controlnet=controlnet_model, torch_dtype=dtype)\n",
        "        controlnet_pipe.to(device)\n",
        "        controlnet_pipe.enable_model_cpu_offload()\n",
        "        controlnet_pipe.enable_attention_slicing()\n",
        "\n",
        "        logger.info(\"Loading Canny preprocessor...\")\n",
        "        controlnet_preprocessor = CannyDetector()\n",
        "        if hasattr(controlnet_preprocessor, 'to'):\n",
        "            controlnet_preprocessor.to(device)\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.exception(\"Fatal error during model loading\")\n",
        "        raise RuntimeError(f\"Failed to load models: {e}\")\n",
        "\n",
        "    logger.info(\"âœ… Model loading complete.\")\n",
        "\n",
        "# --- FastAPI Setup ---\n",
        "@asynccontextmanager\n",
        "async def lifespan(app: FastAPI):\n",
        "    logger.info(\"Application startup...\")\n",
        "    load_models()\n",
        "    yield\n",
        "    logger.info(\"Application shutdown...\")\n",
        "    global controlnet_pipe\n",
        "    del controlnet_pipe\n",
        "    if device == 'cuda':\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "app = FastAPI(lifespan=lifespan, title=\"Flux ControlNet Image-to-Image API\")\n",
        "\n",
        "# --- API Endpoint ---\n",
        "@app.post(\"/generate/image-to-image\", summary=\"Generate Image using ControlNet\", response_class=Response)\n",
        "async def generate_image_to_image(\n",
        "    prompt: str = Form(...),\n",
        "    negative_prompt: Optional[str] = Form(\"\"),\n",
        "    lora_scale: Optional[float] = Form(DEFAULT_LORA_SCALE),\n",
        "    controlnet_scale: float = Form(DEFAULT_CONTROLNET_SCALE),\n",
        "    num_inference_steps: int = Form(DEFAULT_STEPS),\n",
        "    guidance_scale: float = Form(DEFAULT_GUIDANCE_SCALE),\n",
        "    seed: Optional[int] = Form(None),\n",
        "    image: UploadFile = File(...)\n",
        "):\n",
        "    if controlnet_pipe is None:\n",
        "        raise HTTPException(status_code=503, detail=\"ControlNet pipeline not ready.\")\n",
        "\n",
        "    control_image = prepare_control_image(image)\n",
        "\n",
        "    generator = get_generator(seed)\n",
        "\n",
        "    try:\n",
        "        with torch.inference_mode():\n",
        "            result = controlnet_pipe(\n",
        "                prompt=prompt,\n",
        "                control_image=control_image,  # âœ¨ ì£¼ì˜: FluxControlNetPipelineì€ control_imageë¥¼ ë°›ì„ ê²ƒ\n",
        "                width=IMAGE_WIDTH,\n",
        "                height=IMAGE_HEIGHT,\n",
        "                num_inference_steps=num_inference_steps,\n",
        "                guidance_scale=guidance_scale,\n",
        "                controlnet_conditioning_scale=controlnet_scale,\n",
        "                generator=generator,\n",
        "            )\n",
        "\n",
        "        if hasattr(result, 'images'):\n",
        "            output_image = result.images[0]\n",
        "        elif isinstance(result, list) and isinstance(result[0], Image.Image):\n",
        "            output_image = result[0]\n",
        "        else:\n",
        "            raise ValueError(\"Could not extract image from pipeline result.\")\n",
        "\n",
        "        img_bytes = image_to_bytes(output_image)\n",
        "        return Response(content=img_bytes, media_type=\"image/png\")\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.exception(\"Image-to-Image generation failed\")\n",
        "        raise HTTPException(status_code=500, detail=f\"Generation failed: {e}\")\n",
        "\n",
        "# --- Main ---\n",
        "if __name__ == \"__main__\":\n",
        "    logger.info(\"Starting Uvicorn server...\")\n",
        "    uvicorn.run(\"main_server:app\", host=\"0.0.0.0\", port=PORT, reload=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3kbHyC1SuVaD",
        "outputId": "f905ed05-11b4-4ae7-b47f-27f99ed7696f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing main_server.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 4. ngrok í„°ë„ë§ ë° api ì œê³µ\n",
        "# í™˜ê²½ ë³€ìˆ˜ ì„¤ì •\n",
        "%env PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True\n",
        "\n",
        "import time\n",
        "import os\n",
        "import subprocess\n",
        "from pyngrok import ngrok\n",
        "import logging\n",
        "\n",
        "# ë¡œê¹… ì„¤ì •\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "pyngrok_logger = logging.getLogger(\"pyngrok\")\n",
        "pyngrok_logger.setLevel(logging.INFO)\n",
        "\n",
        "# --- ì„¤ì • ---\n",
        "LOG_FILE = \"uvicorn_server.log\"\n",
        "PORT = 8000  # main_server.py ë‚´ë¶€ í¬íŠ¸ì™€ ì¼ì¹˜\n",
        "STATIC_NGROK_DOMAIN = \"publicly-capable-monkfish.ngrok-free.app\"  # ê³ ì • ë„ë©”ì¸\n",
        "\n",
        "# --- ê¸°ì¡´ ì„œë²„ ë° ngrok í”„ë¡œì„¸ìŠ¤ ì¢…ë£Œ ---\n",
        "print(\"â„¹ï¸ ê¸°ì¡´ Uvicorn/Ngrok í”„ë¡œì„¸ìŠ¤ ì¢…ë£Œ ì‹œë„...\")\n",
        "\n",
        "try:\n",
        "    ngrok.kill()\n",
        "    print(\"   - ngrok í”„ë¡œì„¸ìŠ¤ ì¢…ë£Œ ì™„ë£Œ (pyngrok).\")\n",
        "    time.sleep(2)\n",
        "except Exception as e:\n",
        "    pyngrok_logger.warning(f\"ngrok.kill() ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ (ë¬´ì‹œ ê°€ëŠ¥): {e}\")\n",
        "\n",
        "subprocess.run(['pkill', '-f', 'uvicorn main_server:app'], stderr=subprocess.DEVNULL)\n",
        "subprocess.run(['pkill', '-f', f'ngrok.*http.*{PORT}'], stderr=subprocess.DEVNULL)\n",
        "subprocess.run(['pkill', '-f', '/root/.config/ngrok/ngrok'], stderr=subprocess.DEVNULL)\n",
        "time.sleep(3)\n",
        "\n",
        "# --- FastAPI ì„œë²„ ë°±ê·¸ë¼ìš´ë“œ ì‹¤í–‰ ---\n",
        "print(f\"ğŸš€ FastAPI ì„œë²„ë¥¼ ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì‹œì‘í•©ë‹ˆë‹¤... ë¡œê·¸ íŒŒì¼: {LOG_FILE}\")\n",
        "nohup_cmd = f\"nohup python main_server.py > {LOG_FILE} 2>&1 &\"\n",
        "subprocess.Popen(nohup_cmd, shell=True)\n",
        "time.sleep(5)  # ì„œë²„ ì´ˆê¸° ë¶€íŒ… ëŒ€ê¸°\n",
        "\n",
        "# --- ngrok static ë„ë©”ì¸ìœ¼ë¡œ ì—°ê²° ---\n",
        "print(f\"ğŸŒ ngrok static domain ì—°ê²° ì‹œë„ ({STATIC_NGROK_DOMAIN})...\")\n",
        "public_url = ngrok.connect(\n",
        "    addr=PORT,\n",
        "    proto=\"http\",\n",
        "    domain=STATIC_NGROK_DOMAIN\n",
        ")\n",
        "print(f\"âœ… ê³ ì • URL ì—°ê²° ì™„ë£Œ: {public_url}\")\n",
        "\n",
        "# --- ì„œë²„ ì¤€ë¹„ ëŒ€ê¸° (ëª¨ë¸ ë¡œë”© ì‹œê°„ í™•ë³´) ---\n",
        "wait_seconds = 200  # í•„ìš”ì— ë”°ë¼ ì¡°ì •\n",
        "print(f\"â³ ì„œë²„ ë° ëª¨ë¸ ì¤€ë¹„ ëŒ€ê¸° ì¤‘ ({wait_seconds}ì´ˆ)...\")\n",
        "for i in range(wait_seconds):\n",
        "    print(str(i), end=\" \", flush=True)\n",
        "    if (i + 1) % 30 == 0:\n",
        "        print()\n",
        "    time.sleep(1)\n",
        "print(\"\\nâœ… ì„œë²„ ì¤€ë¹„ ëŒ€ê¸° ì™„ë£Œ.\")\n",
        "\n",
        "# --- ê²°ê³¼ ì¶œë ¥ ---\n",
        "print(f\"\\nğŸ¯ ì„œë²„ê°€ ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì‹¤í–‰ë˜ê³  ìˆìœ¼ë©° ì™¸ë¶€ ì ‘ì† URLì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:\")\n",
        "print(f\"ğŸ”— {public_url}\")\n",
        "\n",
        "print(\"\\n--- ì‹¤í–‰ ì¤‘ì¸ ê´€ë ¨ í”„ë¡œì„¸ìŠ¤ (ì°¸ê³ ìš©) ---\")\n",
        "!ps -ef | grep -E \"main_server.py|ngrok\" | grep -v -E \"grep|pkill|colab\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oCqXd-e1uVX7",
        "outputId": "753c0c72-cb92-4a36-ba74-6d80ac64baa7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "env: PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True\n",
            "â„¹ï¸ ê¸°ì¡´ Uvicorn/Ngrok í”„ë¡œì„¸ìŠ¤ ì¢…ë£Œ ì‹œë„...\n",
            "   - ngrok í”„ë¡œì„¸ìŠ¤ ì¢…ë£Œ ì™„ë£Œ (pyngrok).\n",
            "ğŸš€ FastAPI ì„œë²„ë¥¼ ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì‹œì‘í•©ë‹ˆë‹¤... ë¡œê·¸ íŒŒì¼: uvicorn_server.log\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:pyngrok.ngrok:Opening tunnel named: http-8000-dce04423-330f-4cb6-8795-06f1757b8d37\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸŒ ngrok static domain ì—°ê²° ì‹œë„ (publicly-capable-monkfish.ngrok-free.app)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:pyngrok.process:Overriding default auth token\n",
            "INFO:pyngrok.process.ngrok:t=2025-04-23T10:45:14+0000 lvl=info msg=\"no configuration paths supplied\"\n",
            "INFO:pyngrok.process.ngrok:t=2025-04-23T10:45:14+0000 lvl=info msg=\"using configuration at default config path\" path=/root/.config/ngrok/ngrok.yml\n",
            "INFO:pyngrok.process.ngrok:t=2025-04-23T10:45:14+0000 lvl=info msg=\"open config file\" path=/root/.config/ngrok/ngrok.yml err=nil\n",
            "INFO:pyngrok.process.ngrok:t=2025-04-23T10:45:14+0000 lvl=info msg=\"starting web service\" obj=web addr=127.0.0.1:4040 allow_hosts=[]\n",
            "INFO:pyngrok.process.ngrok:t=2025-04-23T10:45:14+0000 lvl=info msg=\"client session established\" obj=tunnels.session\n",
            "INFO:pyngrok.process.ngrok:t=2025-04-23T10:45:14+0000 lvl=info msg=\"tunnel session started\" obj=tunnels.session\n",
            "INFO:pyngrok.process.ngrok:t=2025-04-23T10:45:14+0000 lvl=info msg=start pg=/api/tunnels id=7d2243fd67d7674d\n",
            "INFO:pyngrok.process.ngrok:t=2025-04-23T10:45:14+0000 lvl=info msg=end pg=/api/tunnels id=7d2243fd67d7674d status=200 dur=414.807Âµs\n",
            "INFO:pyngrok.process.ngrok:t=2025-04-23T10:45:14+0000 lvl=info msg=start pg=/api/tunnels id=cba4ee7582d8fab2\n",
            "INFO:pyngrok.process.ngrok:t=2025-04-23T10:45:14+0000 lvl=info msg=end pg=/api/tunnels id=cba4ee7582d8fab2 status=200 dur=109.434Âµs\n",
            "INFO:pyngrok.process.ngrok:t=2025-04-23T10:45:14+0000 lvl=info msg=start pg=/api/tunnels id=8963d9893abb570b\n",
            "INFO:pyngrok.process.ngrok:t=2025-04-23T10:45:14+0000 lvl=info msg=\"started tunnel\" obj=tunnels name=http-8000-dce04423-330f-4cb6-8795-06f1757b8d37 addr=http://localhost:8000 url=https://publicly-capable-monkfish.ngrok-free.app\n",
            "INFO:pyngrok.process.ngrok:t=2025-04-23T10:45:14+0000 lvl=info msg=end pg=/api/tunnels id=8963d9893abb570b status=201 dur=32.321713ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… ê³ ì • URL ì—°ê²° ì™„ë£Œ: NgrokTunnel: \"https://publicly-capable-monkfish.ngrok-free.app\" -> \"http://localhost:8000\"\n",
            "â³ ì„œë²„ ë° ëª¨ë¸ ì¤€ë¹„ ëŒ€ê¸° ì¤‘ (200ì´ˆ)...\n",
            "0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 \n",
            "30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 \n",
            "60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 \n",
            "90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 \n",
            "120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 \n",
            "150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 \n",
            "180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 \n",
            "âœ… ì„œë²„ ì¤€ë¹„ ëŒ€ê¸° ì™„ë£Œ.\n",
            "\n",
            "ğŸ¯ ì„œë²„ê°€ ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì‹¤í–‰ë˜ê³  ìˆìœ¼ë©° ì™¸ë¶€ ì ‘ì† URLì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:\n",
            "ğŸ”— NgrokTunnel: \"https://publicly-capable-monkfish.ngrok-free.app\" -> \"http://localhost:8000\"\n",
            "\n",
            "--- ì‹¤í–‰ ì¤‘ì¸ ê´€ë ¨ í”„ë¡œì„¸ìŠ¤ (ì°¸ê³ ìš©) ---\n",
            "root        5042       1 75 10:45 ?        00:02:35 python3 main_server.py\n",
            "root        5083    4474  0 10:45 ?        00:00:00 /root/.config/ngrok/ngrok start --none --log=stdout --authtoken=2w407BMGqxf9I8D2pqcULgTqTQ3_5kTpwTjAKu8ummsSXG1kn\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # @title 5. API ì„œë²„ ìš”ì²­, CSV ê¸°ë°˜ ë‹¤ìˆ˜ í”„ë¡¬í”„íŠ¸ ì‚¬ìš© ë° Google Drive ì €ì¥\n",
        "# import requests\n",
        "# import base64\n",
        "# import io\n",
        "# from PIL import Image\n",
        "# from IPython.display import display, HTML\n",
        "# import time\n",
        "# import json\n",
        "# import os # os ëª¨ë“ˆ ì¶”ê°€\n",
        "# import pandas as pd # pandas ë¼ì´ë¸ŒëŸ¬ë¦¬ ì¶”ê°€\n",
        "# from google.colab import drive # Google Drive ì—°ë™ìš©\n",
        "\n",
        "# # --- Google Drive ë§ˆìš´íŠ¸ ---\n",
        "# try:\n",
        "#     drive.mount('/content/drive')\n",
        "#     drive_mounted = True\n",
        "#     print(\"âœ… Google Drive ë§ˆìš´íŠ¸ ì„±ê³µ!\")\n",
        "# except Exception as e:\n",
        "#     print(f\"âš ï¸ Google Drive ë§ˆìš´íŠ¸ ì‹¤íŒ¨: {e}\")\n",
        "#     print(\"   ì´ë¯¸ì§€ëŠ” Google Driveì— ì €ì¥ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤.\")\n",
        "#     drive_mounted = False\n",
        "\n",
        "# # --- ì„¤ì • ---\n",
        "# # 5ë²ˆ ì…€ ì¶œë ¥ì—ì„œ í™•ì¸í•œ Ngrok URL (ì´ì „ ì„±ê³µ ì‹œ ì‚¬ìš©í•œ URL ìœ ì§€)\n",
        "# NGROK_URL = \"https://7178-34-139-109-175.ngrok-free.app\" # ì´ì „ ì„±ê³µ ì‹œ ì‚¬ìš©í•œ URL\n",
        "\n",
        "# # --- Google Drive ì €ì¥ ê²½ë¡œ ì„¤ì • ---\n",
        "# SAVE_DIR = \"/content/drive/MyDrive/FluxComicOutput\" # ì›í•˜ëŠ” ê²½ë¡œë¡œ ìˆ˜ì •í•˜ì„¸ìš”.\n",
        "# if drive_mounted:\n",
        "#     try:\n",
        "#         os.makedirs(SAVE_DIR, exist_ok=True)\n",
        "#         print(f\"âœ… ì´ë¯¸ì§€ë¥¼ ì €ì¥í•  ê²½ë¡œ: {SAVE_DIR}\")\n",
        "#     except Exception as e:\n",
        "#         print(f\"âš ï¸ Google Drive ì €ì¥ ê²½ë¡œ ìƒì„± ì‹¤íŒ¨ ({SAVE_DIR}): {e}\")\n",
        "#         print(\"   ì´ë¯¸ì§€ëŠ” Google Driveì— ì €ì¥ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤.\")\n",
        "#         drive_mounted = False\n",
        "\n",
        "# # --- CSVì—ì„œ í”„ë¡¬í”„íŠ¸ ë¡œë“œ ---\n",
        "# CSV_FILE_PATH = 'news_based_comic_prompts_sample.csv' # ì—…ë¡œë“œí•œ CSV íŒŒì¼ ì´ë¦„\n",
        "\n",
        "# # --- ìƒì„±í•  Case ID ë¦¬ìŠ¤íŠ¸ ---\n",
        "# # ì•„ë˜ ë¦¬ìŠ¤íŠ¸ì— ìƒì„±í•˜ê³  ì‹¶ì€ ë§Œí™”ì˜ case_id ë²ˆí˜¸ë“¤ì„ ë„£ìœ¼ì„¸ìš”.\n",
        "# # ì˜ˆ: [1, 5, 10] ë˜ëŠ” list(range(1, 6)) -> 1ë²ˆë¶€í„° 5ë²ˆê¹Œì§€ ìƒì„±\n",
        "# case_ids_to_generate = [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20] # <--- ìƒì„±í•  case_id ë¦¬ìŠ¤íŠ¸ë¥¼ ì—¬ê¸°ì— ì§€ì •í•˜ì„¸ìš”.\n",
        "\n",
        "# try:\n",
        "#     df_all_panels = pd.read_csv(CSV_FILE_PATH) # ì „ì²´ CSV íŒŒì¼ì„ í•œë²ˆë§Œ ì½ìŒ\n",
        "#     print(f\"âœ… '{CSV_FILE_PATH}' íŒŒì¼ ë¡œë“œ ì„±ê³µ!\")\n",
        "#     csv_loaded = True\n",
        "# except FileNotFoundError:\n",
        "#     print(f\"âŒ ì˜¤ë¥˜: '{CSV_FILE_PATH}' íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. íŒŒì¼ì„ Colab í™˜ê²½ì— ì—…ë¡œë“œí–ˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.\")\n",
        "#     csv_loaded = False\n",
        "# except Exception as e:\n",
        "#     print(f\"âŒ CSV íŒŒì¼ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}\")\n",
        "#     csv_loaded = False\n",
        "# # --------------------------\n",
        "\n",
        "# if NGROK_URL == \"YOUR_NGROK_PUBLIC_URL\":\n",
        "#     print(\"âš ï¸ ê²½ê³ : NGROK_URL ë³€ìˆ˜ì— 5ë²ˆ ì…€ì—ì„œ ì–»ì€ ì‹¤ì œ Ngrok ì£¼ì†Œë¥¼ ì…ë ¥í•´ì•¼ í•©ë‹ˆë‹¤!\")\n",
        "# elif not csv_loaded: # CSV ë¡œë“œ ì‹¤íŒ¨ ì‹œ ì‹¤í–‰ ì¤‘ì§€\n",
        "#      print(\"âŒ CSV íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨ë¡œ ì´ë¯¸ì§€ ìƒì„±ì„ ì§„í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n",
        "# else:\n",
        "#     # API ì—”ë“œí¬ì¸íŠ¸ ì£¼ì†Œ ì„¤ì •\n",
        "#     TEXT_TO_IMAGE_URL = f\"{NGROK_URL}/generate/text-to-image\"\n",
        "#     # IMAGE_TO_IMAGE_URL = f\"{NGROK_URL}/generate/image-to-image\" # í˜„ì¬ ë¹„í™œì„±í™” ìƒíƒœ\n",
        "#     CHANGE_LORA_URL = f\"{NGROK_URL}/change-lora\"\n",
        "#     STATUS_URL = f\"{NGROK_URL}/\"\n",
        "\n",
        "#     # --- ê¸°ë³¸ ìƒì„± íŒŒë¼ë¯¸í„° ---\n",
        "#     negative_prompt = \"(worst quality, low quality, normal quality:1.2), deformed, blurry, text, signature\"\n",
        "#     num_inference_steps = 30\n",
        "#     guidance_scale = 3.5\n",
        "#     lora_scale_vintage = 0.0 # LoRA ì‚¬ìš© ì‹œ ê°’ ì¡°ì •\n",
        "\n",
        "#     # --- ì„œë²„ ìƒíƒœ í™•ì¸ (ì „ì²´ ì‹¤í–‰ ì „ í•œ ë²ˆë§Œ) ---\n",
        "#     print(f\"\\nAPI ì„œë²„ ({NGROK_URL}) ìƒíƒœ í™•ì¸ ì¤‘...\")\n",
        "#     server_ok = False\n",
        "#     try:\n",
        "#         status_response = requests.get(STATUS_URL, timeout=30)\n",
        "#         status_response.raise_for_status()\n",
        "#         print(\"\\n--- ì„œë²„ ìƒíƒœ ---\")\n",
        "#         print(json.dumps(status_response.json(), indent=2))\n",
        "#         print(\"------------------\\n\")\n",
        "#         server_ok = True\n",
        "#     except requests.exceptions.RequestException as e:\n",
        "#         print(f\"âŒ ì„œë²„ ìƒíƒœ í™•ì¸ ì‹¤íŒ¨: {e}. ì´ë¯¸ì§€ ìƒì„±ì„ ì§„í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n",
        "#     # ------------------------------------\n",
        "\n",
        "#     # --- ìƒì„± ì‹¤í–‰ (ì§€ì •ëœ ëª¨ë“  Case IDì— ëŒ€í•´ ë°˜ë³µ) ---\n",
        "#     if server_ok and csv_loaded:\n",
        "#         print(f\"ğŸš€ ì´ {len(case_ids_to_generate)}ê°œì˜ ë§Œí™” ìƒì„±ì„ ì‹œì‘í•©ë‹ˆë‹¤: {case_ids_to_generate}\")\n",
        "\n",
        "#         # ì§€ì •ëœ ëª¨ë“  case_idì— ëŒ€í•´ ì™¸ë¶€ ë£¨í”„ ì‹¤í–‰\n",
        "#         for selected_case_id in case_ids_to_generate:\n",
        "#             print(f\"\\n===================================\")\n",
        "#             print(f\"â³ Case ID {selected_case_id} ì²˜ë¦¬ ì‹œì‘...\")\n",
        "#             print(f\"===================================\")\n",
        "\n",
        "#             generated_images = [] # ê° ì¼€ì´ìŠ¤ë³„ ì´ë¯¸ì§€ ë¦¬ìŠ¤íŠ¸ ì´ˆê¸°í™” (ë©”ëª¨ë¦¬ ê´€ë¦¬)\n",
        "\n",
        "#             # --- í•´ë‹¹ Case IDì˜ íŒ¨ë„ ë°ì´í„° ì¤€ë¹„ ---\n",
        "#             selected_panels_df = df_all_panels[df_all_panels['case_id'] == selected_case_id].sort_values(by='panel')\n",
        "\n",
        "#             if selected_panels_df.empty:\n",
        "#                 print(f\"âŒ ì˜¤ë¥˜: CSV íŒŒì¼ì—ì„œ case_id {selected_case_id}ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë‹¤ìŒ case_idë¡œ ë„˜ì–´ê°‘ë‹ˆë‹¤.\")\n",
        "#                 continue # í˜„ì¬ case_id ê±´ë„ˆë›°ê³  ë‹¤ìŒ ë£¨í”„ ë°˜ë³µ ì‹¤í–‰\n",
        "\n",
        "#             comic_panels = selected_panels_df[['prompt', 'seed']].to_dict('records')\n",
        "#             print(f\"âœ… Case ID {selected_case_id}ì— ëŒ€í•œ íŒ¨ë„ ë°ì´í„° ë¡œë“œ ì™„ë£Œ ({len(comic_panels)}ê°œ íŒ¨ë„).\")\n",
        "#             # ------------------------------------\n",
        "\n",
        "#             # --- ê° íŒ¨ë„ ì´ë¯¸ì§€ ìƒì„± (ë‚´ë¶€ ë£¨í”„) ---\n",
        "#             for i, panel_data in enumerate(comic_panels):\n",
        "#                 if 'prompt' not in panel_data or 'seed' not in panel_data:\n",
        "#                      print(f\"âš ï¸ ì»· {i+1} ë°ì´í„° í˜•ì‹ ì˜¤ë¥˜. ê±´ë„ˆ<0xEB><0x9C><0x84>ë‹ˆë‹¤: {panel_data}\")\n",
        "#                      continue\n",
        "\n",
        "#                 panel_prompt = panel_data['prompt']\n",
        "#                 panel_seed = int(panel_data['seed'])\n",
        "\n",
        "#                 print(f\"  ì»· {i+1}/{len(comic_panels)} ìƒì„± ì¤‘: \\\"{panel_prompt[:50]}...\\\" (Seed: {panel_seed})\")\n",
        "\n",
        "#                 payload = {\n",
        "#                     \"prompt\": panel_prompt,\n",
        "#                     \"negative_prompt\": negative_prompt,\n",
        "#                     \"num_inference_steps\": num_inference_steps,\n",
        "#                     \"guidance_scale\": guidance_scale,\n",
        "#                     \"lora_scale\": lora_scale_vintage,\n",
        "#                     \"seed\": panel_seed\n",
        "#                 }\n",
        "\n",
        "#                 try:\n",
        "#                     start_req_time = time.time()\n",
        "#                     response = requests.post(TEXT_TO_IMAGE_URL, json=payload, timeout=300) # íƒ€ì„ì•„ì›ƒ ëŠ˜ë¦¼\n",
        "#                     end_req_time = time.time()\n",
        "\n",
        "#                     if response.status_code == 200:\n",
        "#                         print(f\"    âœ… ì»· {i+1} ìƒì„± ì„±ê³µ! (ì†Œìš” ì‹œê°„: {end_req_time - start_req_time:.2f}ì´ˆ)\")\n",
        "#                         try:\n",
        "#                             img = Image.open(io.BytesIO(response.content))\n",
        "#                             generated_images.append(img)\n",
        "#                             display(img)\n",
        "\n",
        "#                             # --- Google Drive ì €ì¥ ë¡œì§ ---\n",
        "#                             if drive_mounted:\n",
        "#                                 try:\n",
        "#                                     filename = f\"comic_case_{selected_case_id}_panel_{i+1}_seed_{panel_seed}.png\"\n",
        "#                                     save_path = os.path.join(SAVE_DIR, filename)\n",
        "#                                     img.save(save_path)\n",
        "#                                     print(f\"      ğŸ’¾ ì´ë¯¸ì§€ë¥¼ Google Driveì— ì €ì¥í–ˆìŠµë‹ˆë‹¤: {save_path}\")\n",
        "#                                 except Exception as e:\n",
        "#                                     print(f\"      âš ï¸ Google Drive ì €ì¥ ì‹¤íŒ¨ ({filename}): {e}\")\n",
        "#                             # ----------------------------\n",
        "\n",
        "#                         except Exception as e:\n",
        "#                             print(f\"    âš ï¸ ì´ë¯¸ì§€ë¥¼ í‘œì‹œ/ì²˜ë¦¬í•˜ëŠ” ì¤‘ ì˜¤ë¥˜: {e}\")\n",
        "#                     else:\n",
        "#                         print(f\"    âŒ ì»· {i+1} ìƒì„± ì‹¤íŒ¨: Status Code {response.status_code}\")\n",
        "#                         try:\n",
        "#                             print(f\"       ì˜¤ë¥˜ ë©”ì‹œì§€: {response.json()}\")\n",
        "#                         except json.JSONDecodeError:\n",
        "#                             print(f\"       ì˜¤ë¥˜ ë‚´ìš©: {response.text}\")\n",
        "#                 except requests.exceptions.Timeout:\n",
        "#                      print(f\"  âŒ ì»· {i+1} ìš”ì²­ ì‹œê°„ ì´ˆê³¼ (Timeout). ë‹¤ìŒ íŒ¨ë„ë¡œ ë„˜ì–´ê°‘ë‹ˆë‹¤.\")\n",
        "#                 except requests.exceptions.RequestException as e:\n",
        "#                      print(f\"  âŒ ì»· {i+1} ìš”ì²­ ì‹¤íŒ¨: {e}\")\n",
        "#                 except Exception as e:\n",
        "#                      print(f\"  âŒ ì»· {i+1} ì²˜ë¦¬ ì¤‘ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {e}\")\n",
        "\n",
        "#                 time.sleep(1) # íŒ¨ë„ ê°„ ë”œë ˆì´\n",
        "\n",
        "#             print(f\"\\nğŸ‰ Case ID {selected_case_id} ë§Œí™” ì´ë¯¸ì§€ ìƒì„± ì™„ë£Œ!\")\n",
        "#             # time.sleep(5) # í•„ìš”í•˜ë‹¤ë©´ ì¼€ì´ìŠ¤ ì‚¬ì´ì— ì¶”ê°€ ë”œë ˆì´\n",
        "\n",
        "#         print(f\"\\n===================================\")\n",
        "#         print(f\"âœ… ëª¨ë“  ìš”ì²­ëœ ë§Œí™” ìƒì„± ì‘ì—… ì™„ë£Œ!\")\n",
        "#         print(f\"===================================\")\n",
        "#         # --- ìƒì„± ì‹¤í–‰ (Outer Loop for Case IDs) --- ë ---\n",
        "\n",
        "#     # --- (ì„ íƒ ì‚¬í•­) ë‹¤ë¥¸ LoRA ë¡œ ìŠ¤íƒ€ì¼ ë³€ê²½ í›„ ì¬ìƒì„± ---\n",
        "#     # (ì´ ë¶€ë¶„ì€ ê·¸ëŒ€ë¡œ ìœ ì§€)"
      ],
      "metadata": {
        "id": "WbBRc0ye4_NW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}