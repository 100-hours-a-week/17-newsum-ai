{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "# @title 1. 라이브러리 설치\n",
        "%pip install -q fastapi uvicorn[standard]\n",
        "%pip install -q pyngrok>=7.0.0 diffusers>=0.27.0 transformers accelerate\n",
        "%pip install -q peft controlnet_aux torch>=2.0.0 opencv-python-headless Pillow\n",
        "%pip install -q requests safetensors pydantic huggingface_hub python-multipart\n",
        "%pip install -U diffusers transformers accelerate safetensors\n",
        "%pip install -q wget\n",
        "\n",
        "print(\"✅ 라이브러리 설치 완료\")"
      ],
      "metadata": {
        "id": "w-k2065nuVgM",
        "cellView": "form"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title InstantX 코드 파일 다운로드\n",
        "import wget\n",
        "import os\n",
        "\n",
        "# Flux InstantX 파일 리스트\n",
        "files = [\n",
        "    (\"pipeline_flux_ipa.py\", \"https://huggingface.co/InstantX/FLUX.1-dev-IP-Adapter/resolve/main/pipeline_flux_ipa.py?download=true\"),\n",
        "    (\"transformer_flux.py\", \"https://huggingface.co/InstantX/FLUX.1-dev-IP-Adapter/resolve/main/transformer_flux.py?download=true\"),\n",
        "    (\"attention_processor.py\", \"https://huggingface.co/InstantX/FLUX.1-dev-IP-Adapter/resolve/main/attention_processor.py?download=true\"),\n",
        "    (\"infer_flux_ipa_siglip.py\", \"https://huggingface.co/InstantX/FLUX.1-dev-IP-Adapter/resolve/main/infer_flux_ipa_siglip.py?download=true\"),\n",
        "    (\"ip-adapter.bin\", \"https://huggingface.co/InstantX/FLUX.1-dev-IP-Adapter/resolve/main/ip-adapter.bin?download=true\"),\n",
        "]\n",
        "\n",
        "for filename, url in files:\n",
        "    if not os.path.exists(filename):\n",
        "        wget.download(url, out=filename)\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "4k7jefWBHbLf"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 2. Hugging Face 로그인 / Ngrok 설정 (Authtoken 시크릿 키)\n",
        "import os\n",
        "from google.colab import userdata\n",
        "from huggingface_hub import login\n",
        "from pyngrok import conf, ngrok\n",
        "\n",
        "# Hugging Face 및 ngrok 토큰 (Colab Secret에서 가져오기)\n",
        "hf_token = userdata.get(\"HF_TOKEN\")        # Hugging Face Token\n",
        "ngrok_token = userdata.get(\"NGROK_TOKEN\")  # ngrok Token\n",
        "\n",
        "if hf_token == None:\n",
        "  print('x')\n",
        "if ngrok_token == None:\n",
        "  print('x')\n",
        "\n",
        "# 포트 설정\n",
        "PORT = 8000\n",
        "\n",
        "# hugging face 로그인\n",
        "print(\"🔑 Hugging Face 로그인 중...\")\n",
        "login(token=hf_token)\n",
        "\n",
        "# ngrok 설정\n",
        "if ngrok_token == \"YOUR_NGROK_AUTHTOKEN\":\n",
        "  print(\"⚠️ 경고: Ngrok Authtoken을 입력하세요. 없으면 Ngrok 터널이 작동하지 않습니다.\")\n",
        "  print(\"Ngrok 토큰은 https://dashboard.ngrok.com/get-started/your-authtoken 에서 확인 가능합니다.\")\n",
        "else:\n",
        "  os.environ['NGROK_AUTHTOKEN'] = ngrok_token\n",
        "  conf.get_default().auth_token = ngrok_token\n",
        "  print(\"✅ Ngrok Authtoken 설정 완료\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A0jI1SIruVeF",
        "outputId": "641fe405-5a48-4dc1-ec84-8f4bcc7355c3",
        "cellView": "form"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔑 Hugging Face 로그인 중...\n",
            "✅ Ngrok Authtoken 설정 완료\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 3. FastAPI 애플리케이션 코드 작성 (main_server.py 파일 생성)\n",
        "# 이 셀은 FastAPI 서버 코드를 main_server.py 파일로 저장합니다.\n",
        "\n",
        "%%writefile main_server.py\n",
        "import os\n",
        "import io\n",
        "import logging\n",
        "from contextlib import asynccontextmanager\n",
        "from typing import Optional, Any\n",
        "\n",
        "import torch\n",
        "import uvicorn\n",
        "from fastapi import FastAPI, HTTPException, UploadFile, File, Form\n",
        "from fastapi.responses import Response\n",
        "from PIL import Image\n",
        "import gc\n",
        "\n",
        "# IP-Adapter 관련 모듈\n",
        "from pipeline_flux_ipa import FluxPipeline\n",
        "from transformer_flux import FluxTransformer2DModel\n",
        "from attention_processor import IPAFluxAttnProcessor2_0\n",
        "from transformers import AutoProcessor, SiglipVisionModel\n",
        "from infer_flux_ipa_siglip import resize_img, MLPProjModel, IPAdapter\n",
        "\n",
        "# --- Configuration ---\n",
        "BASE_MODEL_ID = \"black-forest-labs/FLUX.1-dev\"\n",
        "IMAGE_ENCODER_PATH = \"google/siglip-so400m-patch14-384\"\n",
        "IPADAPTER_PATH = \"./ip-adapter.bin\"\n",
        "\n",
        "DEFAULT_STEPS = 30\n",
        "# DEFAULT_GUIDANCE_SCALE = 3.5\n",
        "DEFAULT_IPADAPTER_SCALE = 0.7  # 변경: IP-Adapter 용 Scale 이름\n",
        "DEFAULT_LORA_SCALE = 0.8\n",
        "IMAGE_WIDTH = 1024\n",
        "IMAGE_HEIGHT = 1024\n",
        "PORT = 8000\n",
        "\n",
        "# --- Global State ---\n",
        "pipe: Optional[FluxPipeline] = None\n",
        "ip_model: Optional[IPAdapter] = None\n",
        "device: Optional[str] = None\n",
        "\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# --- Helper Functions ---\n",
        "def load_pil_image(image_bytes: bytes) -> Image.Image:\n",
        "    image = Image.open(io.BytesIO(image_bytes)).convert(\"RGB\")\n",
        "    return image\n",
        "\n",
        "def image_to_bytes(image: Image.Image) -> bytes:\n",
        "    byte_arr = io.BytesIO()\n",
        "    image.save(byte_arr, format='PNG')\n",
        "    byte_arr.seek(0)\n",
        "    return byte_arr.getvalue()\n",
        "\n",
        "def get_generator(seed: Optional[int] = None) -> torch.Generator:\n",
        "    if seed is None:\n",
        "        seed = torch.randint(0, 2**32 - 1, (1,)).item()\n",
        "    logger.info(f\"Using seed: {seed}\")\n",
        "    return torch.Generator(device=device).manual_seed(seed)\n",
        "\n",
        "async def prepare_reference_image(uploaded_image: UploadFile) -> Image.Image:\n",
        "    image = load_pil_image(await uploaded_image.read())\n",
        "    resized_image = resize_img(image)\n",
        "    return resized_image\n",
        "\n",
        "# --- Model Loading ---\n",
        "def load_models():\n",
        "    global pipe, ip_model, device\n",
        "\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    logger.info(f\"Using device: {device}\")\n",
        "    dtype = torch.bfloat16 if device == \"cuda\" and torch.cuda.is_bf16_supported() else torch.float16\n",
        "\n",
        "    try:\n",
        "        logger.info(\"Loading Flux Transformer...\")\n",
        "        transformer = FluxTransformer2DModel.from_pretrained(\n",
        "            BASE_MODEL_ID, subfolder=\"transformer\", torch_dtype=dtype\n",
        "        )\n",
        "\n",
        "        logger.info(\"Loading Flux Pipeline...\")\n",
        "        pipe = FluxPipeline.from_pretrained(\n",
        "            BASE_MODEL_ID, transformer=transformer, torch_dtype=dtype\n",
        "        )\n",
        "\n",
        "        logger.info(\"Loading IP-Adapter...\")\n",
        "        ip_model = IPAdapter(\n",
        "            pipe,\n",
        "            IMAGE_ENCODER_PATH,\n",
        "            IPADAPTER_PATH,\n",
        "            device=device,\n",
        "            num_tokens=128\n",
        "        )\n",
        "\n",
        "        pipe.to(device)\n",
        "        pipe.enable_model_cpu_offload()\n",
        "        pipe.enable_attention_slicing()\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.exception(\"Fatal error during model loading\")\n",
        "        raise RuntimeError(f\"Failed to load models: {e}\")\n",
        "\n",
        "    logger.info(\"✅ Model loading complete.\")\n",
        "\n",
        "# --- FastAPI Setup ---\n",
        "@asynccontextmanager\n",
        "async def lifespan(app: FastAPI):\n",
        "    logger.info(\"Application startup...\")\n",
        "    load_models()\n",
        "    yield\n",
        "    logger.info(\"Application shutdown...\")\n",
        "    global pipe\n",
        "    del pipe\n",
        "    if device == 'cuda':\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "app = FastAPI(lifespan=lifespan, title=\"Flux IP-Adapter Image API\")\n",
        "\n",
        "# --- API Endpoint ---\n",
        "@app.post(\"/generate/ip-adapter-image\", summary=\"Generate Image using IP-Adapter\", response_class=Response)\n",
        "async def generate_ip_adapter_image(\n",
        "    prompt: str = Form(...),\n",
        "    negative_prompt: Optional[str] = Form(\"\"),  # Currently unused.\n",
        "    lora_scale: Optional[float] = Form(DEFAULT_LORA_SCALE),\n",
        "    ipadapter_scale: float = Form(DEFAULT_IPADAPTER_SCALE),\n",
        "    num_inference_steps: int = Form(DEFAULT_STEPS),\n",
        "    seed: Optional[int] = Form(None),\n",
        "    image: UploadFile = File(...)\n",
        "):\n",
        "    if ip_model is None:\n",
        "        raise HTTPException(status_code=503, detail=\"IP-Adapter model not ready.\")\n",
        "\n",
        "    reference_image = await prepare_reference_image(image)\n",
        "\n",
        "    # Before Setting\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "    pipe.to(\"cuda\")\n",
        "\n",
        "    generator = get_generator(seed)\n",
        "\n",
        "    try:\n",
        "        with torch.inference_mode():\n",
        "            output_images = ip_model.generate(\n",
        "                pil_image=reference_image,\n",
        "                prompt=prompt,\n",
        "                scale=ipadapter_scale,\n",
        "                width=IMAGE_WIDTH,\n",
        "                height=IMAGE_HEIGHT,\n",
        "                seed=seed,\n",
        "            )\n",
        "\n",
        "        output_image = output_images[0]\n",
        "        img_bytes = image_to_bytes(output_image)\n",
        "        return Response(content=img_bytes, media_type=\"image/png\")\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.exception(\"Image generation with IP-Adapter failed\")\n",
        "        raise HTTPException(status_code=500, detail=f\"Generation failed: {e}\")\n",
        "\n",
        "    finally:\n",
        "        # After Setting (정상이든 에러든 무조건 실행)\n",
        "        torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "        pipe.to(\"cpu\")\n",
        "\n",
        "\n",
        "# --- Main ---\n",
        "if __name__ == \"__main__\":\n",
        "    logger.info(\"Starting Uvicorn server...\")\n",
        "    uvicorn.run(\"main_server:app\", host=\"0.0.0.0\", port=PORT, reload=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3kbHyC1SuVaD",
        "outputId": "bab892c0-681e-40eb-cde6-adf3052ed76a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing main_server.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 4. ngrok 터널링 및 api 제공\n",
        "# 환경 변수 설정\n",
        "%env PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True\n",
        "\n",
        "import time\n",
        "import os\n",
        "import subprocess\n",
        "from pyngrok import ngrok\n",
        "import logging\n",
        "\n",
        "# 로깅 설정\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "pyngrok_logger = logging.getLogger(\"pyngrok\")\n",
        "pyngrok_logger.setLevel(logging.INFO)\n",
        "\n",
        "# --- 설정 ---\n",
        "LOG_FILE = \"uvicorn_server.log\"\n",
        "PORT = 8000  # main_server.py 내부 포트와 일치\n",
        "STATIC_NGROK_DOMAIN = \"publicly-capable-monkfish.ngrok-free.app\"  # 고정 도메인\n",
        "\n",
        "# --- 기존 서버 및 ngrok 프로세스 종료 ---\n",
        "print(\"ℹ️ 기존 Uvicorn/Ngrok 프로세스 종료 시도...\")\n",
        "\n",
        "try:\n",
        "    ngrok.kill()\n",
        "    print(\"   - ngrok 프로세스 종료 완료 (pyngrok).\")\n",
        "    time.sleep(2)\n",
        "except Exception as e:\n",
        "    pyngrok_logger.warning(f\"ngrok.kill() 실행 중 오류 발생 (무시 가능): {e}\")\n",
        "\n",
        "subprocess.run(['pkill', '-f', 'uvicorn main_server:app'], stderr=subprocess.DEVNULL)\n",
        "subprocess.run(['pkill', '-f', f'ngrok.*http.*{PORT}'], stderr=subprocess.DEVNULL)\n",
        "subprocess.run(['pkill', '-f', '/root/.config/ngrok/ngrok'], stderr=subprocess.DEVNULL)\n",
        "time.sleep(3)\n",
        "\n",
        "# --- FastAPI 서버 백그라운드 실행 ---\n",
        "print(f\"🚀 FastAPI 서버를 백그라운드에서 시작합니다... 로그 파일: {LOG_FILE}\")\n",
        "nohup_cmd = f\"nohup python main_server.py > {LOG_FILE} 2>&1 &\"\n",
        "subprocess.Popen(nohup_cmd, shell=True)\n",
        "time.sleep(5)  # 서버 초기 부팅 대기\n",
        "\n",
        "# --- ngrok static 도메인으로 연결 ---\n",
        "print(f\"🌐 ngrok static domain 연결 시도 ({STATIC_NGROK_DOMAIN})...\")\n",
        "public_url = ngrok.connect(\n",
        "    addr=PORT,\n",
        "    proto=\"http\",\n",
        "    domain=STATIC_NGROK_DOMAIN\n",
        ")\n",
        "print(f\"✅ 고정 URL 연결 완료: {public_url}\")\n",
        "\n",
        "# --- 서버 준비 대기 (모델 로딩 시간 확보) ---\n",
        "wait_seconds = 200  # 필요에 따라 조정\n",
        "print(f\"⏳ 서버 및 모델 준비 대기 중 ({wait_seconds}초)...\")\n",
        "for i in range(wait_seconds):\n",
        "    print(str(i), end=\" \", flush=True)\n",
        "    if (i + 1) % 30 == 0:\n",
        "        print()\n",
        "    time.sleep(1)\n",
        "print(\"\\n✅ 서버 준비 대기 완료.\")\n",
        "\n",
        "# --- 결과 출력 ---\n",
        "print(f\"\\n🎯 서버가 백그라운드에서 실행되고 있으며 외부 접속 URL은 다음과 같습니다:\")\n",
        "print(f\"🔗 {public_url}\")\n",
        "\n",
        "print(\"\\n--- 실행 중인 관련 프로세스 (참고용) ---\")\n",
        "!ps -ef | grep -E \"main_server.py|ngrok\" | grep -v -E \"grep|pkill|colab\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oCqXd-e1uVX7",
        "outputId": "a7bf0fd3-64d4-4891-a39f-e7fcbcd83e3a"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "env: PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True\n",
            "ℹ️ 기존 Uvicorn/Ngrok 프로세스 종료 시도...\n",
            "   - ngrok 프로세스 종료 완료 (pyngrok).\n",
            "🚀 FastAPI 서버를 백그라운드에서 시작합니다... 로그 파일: uvicorn_server.log\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:pyngrok.ngrok:Opening tunnel named: http-8000-0f528c63-3f98-4fce-9532-47a1e19fdf2e\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🌐 ngrok static domain 연결 시도 (publicly-capable-monkfish.ngrok-free.app)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:pyngrok.process:Overriding default auth token\n",
            "INFO:pyngrok.process.ngrok:t=2025-04-29T02:11:56+0000 lvl=info msg=\"no configuration paths supplied\"\n",
            "INFO:pyngrok.process.ngrok:t=2025-04-29T02:11:56+0000 lvl=info msg=\"using configuration at default config path\" path=/root/.config/ngrok/ngrok.yml\n",
            "INFO:pyngrok.process.ngrok:t=2025-04-29T02:11:56+0000 lvl=info msg=\"open config file\" path=/root/.config/ngrok/ngrok.yml err=nil\n",
            "INFO:pyngrok.process.ngrok:t=2025-04-29T02:11:56+0000 lvl=info msg=\"starting web service\" obj=web addr=127.0.0.1:4040 allow_hosts=[]\n",
            "INFO:pyngrok.process.ngrok:t=2025-04-29T02:11:57+0000 lvl=info msg=\"client session established\" obj=tunnels.session\n",
            "INFO:pyngrok.process.ngrok:t=2025-04-29T02:11:57+0000 lvl=info msg=\"tunnel session started\" obj=tunnels.session\n",
            "INFO:pyngrok.process.ngrok:t=2025-04-29T02:11:57+0000 lvl=info msg=start pg=/api/tunnels id=5521723b951070b1\n",
            "INFO:pyngrok.process.ngrok:t=2025-04-29T02:11:57+0000 lvl=info msg=end pg=/api/tunnels id=5521723b951070b1 status=200 dur=356.132µs\n",
            "INFO:pyngrok.process.ngrok:t=2025-04-29T02:11:57+0000 lvl=info msg=start pg=/api/tunnels id=f98c31d4e52d5c57\n",
            "INFO:pyngrok.process.ngrok:t=2025-04-29T02:11:57+0000 lvl=info msg=end pg=/api/tunnels id=f98c31d4e52d5c57 status=200 dur=99.28µs\n",
            "INFO:pyngrok.process.ngrok:t=2025-04-29T02:11:57+0000 lvl=info msg=start pg=/api/tunnels id=ea98bab78c31fc40\n",
            "INFO:pyngrok.process.ngrok:t=2025-04-29T02:11:57+0000 lvl=info msg=\"started tunnel\" obj=tunnels name=http-8000-0f528c63-3f98-4fce-9532-47a1e19fdf2e addr=http://localhost:8000 url=https://publicly-capable-monkfish.ngrok-free.app\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ 고정 URL 연결 완료: NgrokTunnel: \"https://publicly-capable-monkfish.ngrok-free.app\" -> \"http://localhost:8000\"\n",
            "⏳ 서버 및 모델 준비 대기 중 (200초)...\n",
            "0 "
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:pyngrok.process.ngrok:t=2025-04-29T02:11:57+0000 lvl=info msg=end pg=/api/tunnels id=ea98bab78c31fc40 status=201 dur=42.515827ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 \n",
            "30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 \n",
            "60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 \n",
            "90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 \n",
            "120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 \n",
            "150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 \n",
            "180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 \n",
            "✅ 서버 준비 대기 완료.\n",
            "\n",
            "🎯 서버가 백그라운드에서 실행되고 있으며 외부 접속 URL은 다음과 같습니다:\n",
            "🔗 NgrokTunnel: \"https://publicly-capable-monkfish.ngrok-free.app\" -> \"http://localhost:8000\"\n",
            "\n",
            "--- 실행 중인 관련 프로세스 (참고용) ---\n",
            "root        1263       1 74 02:11 ?        00:02:33 python3 main_server.py\n",
            "root        1300     525  0 02:11 ?        00:00:00 /root/.config/ngrok/ngrok start --none --log=stdout --authtoken=2w407BMGqxf9I8D2pqcULgTqTQ3_5kTpwTjAKu8ummsSXG1kn\n"
          ]
        }
      ]
    }
  ]
}