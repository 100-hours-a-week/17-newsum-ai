{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "# @title 1. ÎùºÏù¥Î∏åÎü¨Î¶¨ ÏÑ§Ïπò\n",
        "%pip install -q fastapi uvicorn[standard]\n",
        "%pip install -q pyngrok>=7.0.0 diffusers>=0.27.0 transformers accelerate\n",
        "%pip install -q peft controlnet_aux torch>=2.0.0 opencv-python-headless Pillow\n",
        "%pip install -q requests safetensors pydantic huggingface_hub python-multipart\n",
        "%pip install -U diffusers transformers accelerate safetensors\n",
        "%pip install -q wget\n",
        "\n",
        "print(\"‚úÖ ÎùºÏù¥Î∏åÎü¨Î¶¨ ÏÑ§Ïπò ÏôÑÎ£å\")"
      ],
      "metadata": {
        "id": "w-k2065nuVgM",
        "cellView": "form"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title InstantX ÏΩîÎìú ÌååÏùº Îã§Ïö¥Î°úÎìú\n",
        "import wget\n",
        "import os\n",
        "\n",
        "# Flux InstantX ÌååÏùº Î¶¨Ïä§Ìä∏\n",
        "files = [\n",
        "    (\"pipeline_flux_ipa.py\", \"https://huggingface.co/InstantX/FLUX.1-dev-IP-Adapter/resolve/main/pipeline_flux_ipa.py?download=true\"),\n",
        "    (\"transformer_flux.py\", \"https://huggingface.co/InstantX/FLUX.1-dev-IP-Adapter/resolve/main/transformer_flux.py?download=true\"),\n",
        "    (\"attention_processor.py\", \"https://huggingface.co/InstantX/FLUX.1-dev-IP-Adapter/resolve/main/attention_processor.py?download=true\"),\n",
        "    (\"infer_flux_ipa_siglip.py\", \"https://huggingface.co/InstantX/FLUX.1-dev-IP-Adapter/resolve/main/infer_flux_ipa_siglip.py?download=true\"),\n",
        "    (\"ip-adapter.bin\", \"https://huggingface.co/InstantX/FLUX.1-dev-IP-Adapter/resolve/main/ip-adapter.bin?download=true\"),\n",
        "]\n",
        "\n",
        "for filename, url in files:\n",
        "    if not os.path.exists(filename):\n",
        "        wget.download(url, out=filename)\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "4k7jefWBHbLf"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 2. Hugging Face Î°úÍ∑∏Ïù∏ / Ngrok ÏÑ§Ï†ï (Authtoken ÏãúÌÅ¨Î¶ø ÌÇ§)\n",
        "import os\n",
        "from google.colab import userdata\n",
        "from huggingface_hub import login\n",
        "from pyngrok import conf, ngrok\n",
        "\n",
        "# Hugging Face Î∞è ngrok ÌÜ†ÌÅ∞ (Colab SecretÏóêÏÑú Í∞ÄÏ†∏Ïò§Í∏∞)\n",
        "hf_token = userdata.get(\"HF_TOKEN\")        # Hugging Face Token\n",
        "ngrok_token = userdata.get(\"NGROK_TOKEN\")  # ngrok Token\n",
        "\n",
        "if hf_token == None:\n",
        "  print('x')\n",
        "if ngrok_token == None:\n",
        "  print('x')\n",
        "\n",
        "# Ìè¨Ìä∏ ÏÑ§Ï†ï\n",
        "PORT = 8000\n",
        "\n",
        "# hugging face Î°úÍ∑∏Ïù∏\n",
        "print(\"üîë Hugging Face Î°úÍ∑∏Ïù∏ Ï§ë...\")\n",
        "login(token=hf_token)\n",
        "\n",
        "# ngrok ÏÑ§Ï†ï\n",
        "if ngrok_token == \"YOUR_NGROK_AUTHTOKEN\":\n",
        "  print(\"‚ö†Ô∏è Í≤ΩÍ≥†: Ngrok AuthtokenÏùÑ ÏûÖÎ†•ÌïòÏÑ∏Ïöî. ÏóÜÏúºÎ©¥ Ngrok ÌÑ∞ÎÑêÏù¥ ÏûëÎèôÌïòÏßÄ ÏïäÏäµÎãàÎã§.\")\n",
        "  print(\"Ngrok ÌÜ†ÌÅ∞ÏùÄ https://dashboard.ngrok.com/get-started/your-authtoken ÏóêÏÑú ÌôïÏù∏ Í∞ÄÎä•Ìï©ÎãàÎã§.\")\n",
        "else:\n",
        "  os.environ['NGROK_AUTHTOKEN'] = ngrok_token\n",
        "  conf.get_default().auth_token = ngrok_token\n",
        "  print(\"‚úÖ Ngrok Authtoken ÏÑ§Ï†ï ÏôÑÎ£å\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A0jI1SIruVeF",
        "outputId": "641fe405-5a48-4dc1-ec84-8f4bcc7355c3",
        "cellView": "form"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîë Hugging Face Î°úÍ∑∏Ïù∏ Ï§ë...\n",
            "‚úÖ Ngrok Authtoken ÏÑ§Ï†ï ÏôÑÎ£å\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 3. FastAPI Ïï†ÌîåÎ¶¨ÏºÄÏù¥ÏÖò ÏΩîÎìú ÏûëÏÑ± (main_server.py ÌååÏùº ÏÉùÏÑ±)\n",
        "# Ïù¥ ÏÖÄÏùÄ FastAPI ÏÑúÎ≤Ñ ÏΩîÎìúÎ•º main_server.py ÌååÏùºÎ°ú Ï†ÄÏû•Ìï©ÎãàÎã§.\n",
        "\n",
        "%%writefile main_server.py\n",
        "import os\n",
        "import io\n",
        "import logging\n",
        "from contextlib import asynccontextmanager\n",
        "from typing import Optional, Any\n",
        "\n",
        "import torch\n",
        "import uvicorn\n",
        "from fastapi import FastAPI, HTTPException, UploadFile, File, Form\n",
        "from fastapi.responses import Response\n",
        "from PIL import Image\n",
        "import gc\n",
        "\n",
        "# IP-Adapter Í¥ÄÎ†® Î™®Îìà\n",
        "from pipeline_flux_ipa import FluxPipeline\n",
        "from transformer_flux import FluxTransformer2DModel\n",
        "from attention_processor import IPAFluxAttnProcessor2_0\n",
        "from transformers import AutoProcessor, SiglipVisionModel\n",
        "from infer_flux_ipa_siglip import resize_img, MLPProjModel, IPAdapter\n",
        "\n",
        "# --- Configuration ---\n",
        "BASE_MODEL_ID = \"black-forest-labs/FLUX.1-dev\"\n",
        "IMAGE_ENCODER_PATH = \"google/siglip-so400m-patch14-384\"\n",
        "IPADAPTER_PATH = \"./ip-adapter.bin\"\n",
        "\n",
        "DEFAULT_STEPS = 30\n",
        "# DEFAULT_GUIDANCE_SCALE = 3.5\n",
        "DEFAULT_IPADAPTER_SCALE = 0.7  # Î≥ÄÍ≤Ω: IP-Adapter Ïö© Scale Ïù¥Î¶Ñ\n",
        "DEFAULT_LORA_SCALE = 0.8\n",
        "IMAGE_WIDTH = 1024\n",
        "IMAGE_HEIGHT = 1024\n",
        "PORT = 8000\n",
        "\n",
        "# --- Global State ---\n",
        "pipe: Optional[FluxPipeline] = None\n",
        "ip_model: Optional[IPAdapter] = None\n",
        "device: Optional[str] = None\n",
        "\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# --- Helper Functions ---\n",
        "def load_pil_image(image_bytes: bytes) -> Image.Image:\n",
        "    image = Image.open(io.BytesIO(image_bytes)).convert(\"RGB\")\n",
        "    return image\n",
        "\n",
        "def image_to_bytes(image: Image.Image) -> bytes:\n",
        "    byte_arr = io.BytesIO()\n",
        "    image.save(byte_arr, format='PNG')\n",
        "    byte_arr.seek(0)\n",
        "    return byte_arr.getvalue()\n",
        "\n",
        "def get_generator(seed: Optional[int] = None) -> torch.Generator:\n",
        "    if seed is None:\n",
        "        seed = torch.randint(0, 2**32 - 1, (1,)).item()\n",
        "    logger.info(f\"Using seed: {seed}\")\n",
        "    return torch.Generator(device=device).manual_seed(seed)\n",
        "\n",
        "async def prepare_reference_image(uploaded_image: UploadFile) -> Image.Image:\n",
        "    image = load_pil_image(await uploaded_image.read())\n",
        "    resized_image = resize_img(image)\n",
        "    return resized_image\n",
        "\n",
        "# --- Model Loading ---\n",
        "def load_models():\n",
        "    global pipe, ip_model, device\n",
        "\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    logger.info(f\"Using device: {device}\")\n",
        "    dtype = torch.bfloat16 if device == \"cuda\" and torch.cuda.is_bf16_supported() else torch.float16\n",
        "\n",
        "    try:\n",
        "        logger.info(\"Loading Flux Transformer...\")\n",
        "        transformer = FluxTransformer2DModel.from_pretrained(\n",
        "            BASE_MODEL_ID, subfolder=\"transformer\", torch_dtype=dtype\n",
        "        )\n",
        "\n",
        "        logger.info(\"Loading Flux Pipeline...\")\n",
        "        pipe = FluxPipeline.from_pretrained(\n",
        "            BASE_MODEL_ID, transformer=transformer, torch_dtype=dtype\n",
        "        )\n",
        "\n",
        "        logger.info(\"Loading IP-Adapter...\")\n",
        "        ip_model = IPAdapter(\n",
        "            pipe,\n",
        "            IMAGE_ENCODER_PATH,\n",
        "            IPADAPTER_PATH,\n",
        "            device=device,\n",
        "            num_tokens=128\n",
        "        )\n",
        "\n",
        "        pipe.to(device)\n",
        "        pipe.enable_model_cpu_offload()\n",
        "        pipe.enable_attention_slicing()\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.exception(\"Fatal error during model loading\")\n",
        "        raise RuntimeError(f\"Failed to load models: {e}\")\n",
        "\n",
        "    logger.info(\"‚úÖ Model loading complete.\")\n",
        "\n",
        "# --- FastAPI Setup ---\n",
        "@asynccontextmanager\n",
        "async def lifespan(app: FastAPI):\n",
        "    logger.info(\"Application startup...\")\n",
        "    load_models()\n",
        "    yield\n",
        "    logger.info(\"Application shutdown...\")\n",
        "    global pipe\n",
        "    del pipe\n",
        "    if device == 'cuda':\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "app = FastAPI(lifespan=lifespan, title=\"Flux IP-Adapter Image API\")\n",
        "\n",
        "# --- API Endpoint ---\n",
        "@app.post(\"/generate/ip-adapter-image\", summary=\"Generate Image using IP-Adapter\", response_class=Response)\n",
        "async def generate_ip_adapter_image(\n",
        "    prompt: str = Form(...),\n",
        "    negative_prompt: Optional[str] = Form(\"\"),  # Currently unused.\n",
        "    lora_scale: Optional[float] = Form(DEFAULT_LORA_SCALE),\n",
        "    ipadapter_scale: float = Form(DEFAULT_IPADAPTER_SCALE),\n",
        "    num_inference_steps: int = Form(DEFAULT_STEPS),\n",
        "    seed: Optional[int] = Form(None),\n",
        "    image: UploadFile = File(...)\n",
        "):\n",
        "    if ip_model is None:\n",
        "        raise HTTPException(status_code=503, detail=\"IP-Adapter model not ready.\")\n",
        "\n",
        "    reference_image = await prepare_reference_image(image)\n",
        "\n",
        "    # Before Setting\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "    pipe.to(\"cuda\")\n",
        "\n",
        "    generator = get_generator(seed)\n",
        "\n",
        "    try:\n",
        "        with torch.inference_mode():\n",
        "            output_images = ip_model.generate(\n",
        "                pil_image=reference_image,\n",
        "                prompt=prompt,\n",
        "                scale=ipadapter_scale,\n",
        "                width=IMAGE_WIDTH,\n",
        "                height=IMAGE_HEIGHT,\n",
        "                seed=seed,\n",
        "            )\n",
        "\n",
        "        output_image = output_images[0]\n",
        "        img_bytes = image_to_bytes(output_image)\n",
        "        return Response(content=img_bytes, media_type=\"image/png\")\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.exception(\"Image generation with IP-Adapter failed\")\n",
        "        raise HTTPException(status_code=500, detail=f\"Generation failed: {e}\")\n",
        "\n",
        "    finally:\n",
        "        # After Setting (Ï†ïÏÉÅÏù¥Îì† ÏóêÎü¨Îì† Î¨¥Ï°∞Í±¥ Ïã§Ìñâ)\n",
        "        torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "        pipe.to(\"cpu\")\n",
        "\n",
        "\n",
        "# --- Main ---\n",
        "if __name__ == \"__main__\":\n",
        "    logger.info(\"Starting Uvicorn server...\")\n",
        "    uvicorn.run(\"main_server:app\", host=\"0.0.0.0\", port=PORT, reload=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3kbHyC1SuVaD",
        "outputId": "bab892c0-681e-40eb-cde6-adf3052ed76a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing main_server.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 4. ngrok ÌÑ∞ÎÑêÎßÅ Î∞è api Ï†úÍ≥µ\n",
        "# ÌôòÍ≤Ω Î≥ÄÏàò ÏÑ§Ï†ï\n",
        "%env PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True\n",
        "\n",
        "import time\n",
        "import os\n",
        "import subprocess\n",
        "from pyngrok import ngrok\n",
        "import logging\n",
        "\n",
        "# Î°úÍπÖ ÏÑ§Ï†ï\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "pyngrok_logger = logging.getLogger(\"pyngrok\")\n",
        "pyngrok_logger.setLevel(logging.INFO)\n",
        "\n",
        "# --- ÏÑ§Ï†ï ---\n",
        "LOG_FILE = \"uvicorn_server.log\"\n",
        "PORT = 8000  # main_server.py ÎÇ¥Î∂Ä Ìè¨Ìä∏ÏôÄ ÏùºÏπò\n",
        "STATIC_NGROK_DOMAIN = \"publicly-capable-monkfish.ngrok-free.app\"  # Í≥†Ï†ï ÎèÑÎ©îÏù∏\n",
        "\n",
        "# --- Í∏∞Ï°¥ ÏÑúÎ≤Ñ Î∞è ngrok ÌîÑÎ°úÏÑ∏Ïä§ Ï¢ÖÎ£å ---\n",
        "print(\"‚ÑπÔ∏è Í∏∞Ï°¥ Uvicorn/Ngrok ÌîÑÎ°úÏÑ∏Ïä§ Ï¢ÖÎ£å ÏãúÎèÑ...\")\n",
        "\n",
        "try:\n",
        "    ngrok.kill()\n",
        "    print(\"   - ngrok ÌîÑÎ°úÏÑ∏Ïä§ Ï¢ÖÎ£å ÏôÑÎ£å (pyngrok).\")\n",
        "    time.sleep(2)\n",
        "except Exception as e:\n",
        "    pyngrok_logger.warning(f\"ngrok.kill() Ïã§Ìñâ Ï§ë Ïò§Î•ò Î∞úÏÉù (Î¨¥Ïãú Í∞ÄÎä•): {e}\")\n",
        "\n",
        "subprocess.run(['pkill', '-f', 'uvicorn main_server:app'], stderr=subprocess.DEVNULL)\n",
        "subprocess.run(['pkill', '-f', f'ngrok.*http.*{PORT}'], stderr=subprocess.DEVNULL)\n",
        "subprocess.run(['pkill', '-f', '/root/.config/ngrok/ngrok'], stderr=subprocess.DEVNULL)\n",
        "time.sleep(3)\n",
        "\n",
        "# --- FastAPI ÏÑúÎ≤Ñ Î∞±Í∑∏ÎùºÏö¥Îìú Ïã§Ìñâ ---\n",
        "print(f\"üöÄ FastAPI ÏÑúÎ≤ÑÎ•º Î∞±Í∑∏ÎùºÏö¥ÎìúÏóêÏÑú ÏãúÏûëÌï©ÎãàÎã§... Î°úÍ∑∏ ÌååÏùº: {LOG_FILE}\")\n",
        "nohup_cmd = f\"nohup python main_server.py > {LOG_FILE} 2>&1 &\"\n",
        "subprocess.Popen(nohup_cmd, shell=True)\n",
        "time.sleep(5)  # ÏÑúÎ≤Ñ Ï¥àÍ∏∞ Î∂ÄÌåÖ ÎåÄÍ∏∞\n",
        "\n",
        "# --- ngrok static ÎèÑÎ©îÏù∏ÏúºÎ°ú Ïó∞Í≤∞ ---\n",
        "print(f\"üåê ngrok static domain Ïó∞Í≤∞ ÏãúÎèÑ ({STATIC_NGROK_DOMAIN})...\")\n",
        "public_url = ngrok.connect(\n",
        "    addr=PORT,\n",
        "    proto=\"http\",\n",
        "    domain=STATIC_NGROK_DOMAIN\n",
        ")\n",
        "print(f\"‚úÖ Í≥†Ï†ï URL Ïó∞Í≤∞ ÏôÑÎ£å: {public_url}\")\n",
        "\n",
        "# --- ÏÑúÎ≤Ñ Ï§ÄÎπÑ ÎåÄÍ∏∞ (Î™®Îç∏ Î°úÎî© ÏãúÍ∞Ñ ÌôïÎ≥¥) ---\n",
        "wait_seconds = 200  # ÌïÑÏöîÏóê Îî∞Îùº Ï°∞Ï†ï\n",
        "print(f\"‚è≥ ÏÑúÎ≤Ñ Î∞è Î™®Îç∏ Ï§ÄÎπÑ ÎåÄÍ∏∞ Ï§ë ({wait_seconds}Ï¥à)...\")\n",
        "for i in range(wait_seconds):\n",
        "    print(str(i), end=\" \", flush=True)\n",
        "    if (i + 1) % 30 == 0:\n",
        "        print()\n",
        "    time.sleep(1)\n",
        "print(\"\\n‚úÖ ÏÑúÎ≤Ñ Ï§ÄÎπÑ ÎåÄÍ∏∞ ÏôÑÎ£å.\")\n",
        "\n",
        "# --- Í≤∞Í≥º Ï∂úÎ†• ---\n",
        "print(f\"\\nüéØ ÏÑúÎ≤ÑÍ∞Ä Î∞±Í∑∏ÎùºÏö¥ÎìúÏóêÏÑú Ïã§ÌñâÎêòÍ≥† ÏûàÏúºÎ©∞ Ïô∏Î∂Ä Ï†ëÏÜç URLÏùÄ Îã§ÏùåÍ≥º Í∞ôÏäµÎãàÎã§:\")\n",
        "print(f\"üîó {public_url}\")\n",
        "\n",
        "print(\"\\n--- Ïã§Ìñâ Ï§ëÏù∏ Í¥ÄÎ†® ÌîÑÎ°úÏÑ∏Ïä§ (Ï∞∏Í≥†Ïö©) ---\")\n",
        "!ps -ef | grep -E \"main_server.py|ngrok\" | grep -v -E \"grep|pkill|colab\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oCqXd-e1uVX7",
        "outputId": "a7bf0fd3-64d4-4891-a39f-e7fcbcd83e3a"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "env: PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True\n",
            "‚ÑπÔ∏è Í∏∞Ï°¥ Uvicorn/Ngrok ÌîÑÎ°úÏÑ∏Ïä§ Ï¢ÖÎ£å ÏãúÎèÑ...\n",
            "   - ngrok ÌîÑÎ°úÏÑ∏Ïä§ Ï¢ÖÎ£å ÏôÑÎ£å (pyngrok).\n",
            "üöÄ FastAPI ÏÑúÎ≤ÑÎ•º Î∞±Í∑∏ÎùºÏö¥ÎìúÏóêÏÑú ÏãúÏûëÌï©ÎãàÎã§... Î°úÍ∑∏ ÌååÏùº: uvicorn_server.log\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:pyngrok.ngrok:Opening tunnel named: http-8000-0f528c63-3f98-4fce-9532-47a1e19fdf2e\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üåê ngrok static domain Ïó∞Í≤∞ ÏãúÎèÑ (publicly-capable-monkfish.ngrok-free.app)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:pyngrok.process:Overriding default auth token\n",
            "INFO:pyngrok.process.ngrok:t=2025-04-29T02:11:56+0000 lvl=info msg=\"no configuration paths supplied\"\n",
            "INFO:pyngrok.process.ngrok:t=2025-04-29T02:11:56+0000 lvl=info msg=\"using configuration at default config path\" path=/root/.config/ngrok/ngrok.yml\n",
            "INFO:pyngrok.process.ngrok:t=2025-04-29T02:11:56+0000 lvl=info msg=\"open config file\" path=/root/.config/ngrok/ngrok.yml err=nil\n",
            "INFO:pyngrok.process.ngrok:t=2025-04-29T02:11:56+0000 lvl=info msg=\"starting web service\" obj=web addr=127.0.0.1:4040 allow_hosts=[]\n",
            "INFO:pyngrok.process.ngrok:t=2025-04-29T02:11:57+0000 lvl=info msg=\"client session established\" obj=tunnels.session\n",
            "INFO:pyngrok.process.ngrok:t=2025-04-29T02:11:57+0000 lvl=info msg=\"tunnel session started\" obj=tunnels.session\n",
            "INFO:pyngrok.process.ngrok:t=2025-04-29T02:11:57+0000 lvl=info msg=start pg=/api/tunnels id=5521723b951070b1\n",
            "INFO:pyngrok.process.ngrok:t=2025-04-29T02:11:57+0000 lvl=info msg=end pg=/api/tunnels id=5521723b951070b1 status=200 dur=356.132¬µs\n",
            "INFO:pyngrok.process.ngrok:t=2025-04-29T02:11:57+0000 lvl=info msg=start pg=/api/tunnels id=f98c31d4e52d5c57\n",
            "INFO:pyngrok.process.ngrok:t=2025-04-29T02:11:57+0000 lvl=info msg=end pg=/api/tunnels id=f98c31d4e52d5c57 status=200 dur=99.28¬µs\n",
            "INFO:pyngrok.process.ngrok:t=2025-04-29T02:11:57+0000 lvl=info msg=start pg=/api/tunnels id=ea98bab78c31fc40\n",
            "INFO:pyngrok.process.ngrok:t=2025-04-29T02:11:57+0000 lvl=info msg=\"started tunnel\" obj=tunnels name=http-8000-0f528c63-3f98-4fce-9532-47a1e19fdf2e addr=http://localhost:8000 url=https://publicly-capable-monkfish.ngrok-free.app\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Í≥†Ï†ï URL Ïó∞Í≤∞ ÏôÑÎ£å: NgrokTunnel: \"https://publicly-capable-monkfish.ngrok-free.app\" -> \"http://localhost:8000\"\n",
            "‚è≥ ÏÑúÎ≤Ñ Î∞è Î™®Îç∏ Ï§ÄÎπÑ ÎåÄÍ∏∞ Ï§ë (200Ï¥à)...\n",
            "0 "
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:pyngrok.process.ngrok:t=2025-04-29T02:11:57+0000 lvl=info msg=end pg=/api/tunnels id=ea98bab78c31fc40 status=201 dur=42.515827ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 \n",
            "30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 \n",
            "60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 \n",
            "90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 \n",
            "120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 \n",
            "150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 \n",
            "180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 \n",
            "‚úÖ ÏÑúÎ≤Ñ Ï§ÄÎπÑ ÎåÄÍ∏∞ ÏôÑÎ£å.\n",
            "\n",
            "üéØ ÏÑúÎ≤ÑÍ∞Ä Î∞±Í∑∏ÎùºÏö¥ÎìúÏóêÏÑú Ïã§ÌñâÎêòÍ≥† ÏûàÏúºÎ©∞ Ïô∏Î∂Ä Ï†ëÏÜç URLÏùÄ Îã§ÏùåÍ≥º Í∞ôÏäµÎãàÎã§:\n",
            "üîó NgrokTunnel: \"https://publicly-capable-monkfish.ngrok-free.app\" -> \"http://localhost:8000\"\n",
            "\n",
            "--- Ïã§Ìñâ Ï§ëÏù∏ Í¥ÄÎ†® ÌîÑÎ°úÏÑ∏Ïä§ (Ï∞∏Í≥†Ïö©) ---\n",
            "root        1263       1 74 02:11 ?        00:02:33 python3 main_server.py\n",
            "root        1300     525  0 02:11 ?        00:00:00 /root/.config/ngrok/ngrok start --none --log=stdout --authtoken=2w407BMGqxf9I8D2pqcULgTqTQ3_5kTpwTjAKu8ummsSXG1kn\n"
          ]
        }
      ]
    }
  ]
}