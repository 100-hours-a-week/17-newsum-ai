{"cells":[{"cell_type":"markdown","source":["# Setup -> Curate Image -> Tag Image ÍπåÏßÄÎäî ÌïÑÏàò"],"metadata":{"id":"q_WdwzCrFo26"}},{"cell_type":"markdown","metadata":{"id":"rmCPmqFL6hCQ"},"source":["# üìä Dataset Maker by Hollowstrawberry\n","\n","This is based on the work of [Kohya-ss](https://github.com/kohya-ss/sd-scripts) and [Linaqruf](https://colab.research.google.com/github/Linaqruf/kohya-trainer/blob/main/kohya-LoRA-dreambooth.ipynb). Thank you!"]},{"cell_type":"markdown","metadata":{"id":"UZ0f5TOg_iIl"},"source":["### ‚≠ï Disclaimer\n","The purpose of this document is to research bleeding-edge technologies in the field of machine learning.  \n","Please read and follow the [Google Colab guidelines](https://research.google.com/colaboratory/faq.html) and its [Terms of Service](https://research.google.com/colaboratory/tos_v3.html)."]},{"cell_type":"markdown","metadata":{"id":"-rdgF2AWLS2h"},"source":["| |GitHub|üá¨üáß English|üá™üá∏ Spanish|\n","|:--|:-:|:-:|:-:|\n","| üè† **Homepage** | [![GitHub](https://raw.githubusercontent.com/hollowstrawberry/kohya-colab/main/assets/github.svg)](https://github.com/hollowstrawberry/kohya-colab) | | |\n","| üìä **Dataset Maker** | [![GitHub](https://raw.githubusercontent.com/hollowstrawberry/kohya-colab/main/assets/github.svg)](https://github.com/hollowstrawberry/kohya-colab/blob/main/Dataset_Maker.ipynb) | [![Open in Colab](https://raw.githubusercontent.com/hollowstrawberry/kohya-colab/main/assets/colab-badge.svg)](https://colab.research.google.com/github/hollowstrawberry/kohya-colab/blob/main/Dataset_Maker.ipynb) | [![Abrir en Colab](https://raw.githubusercontent.com/hollowstrawberry/kohya-colab/main/assets/colab-badge-spanish.svg)](https://colab.research.google.com/github/hollowstrawberry/kohya-colab/blob/main/Spanish_Dataset_Maker.ipynb) |\n","| ‚≠ê **Lora Trainer** | [![GitHub](https://raw.githubusercontent.com/hollowstrawberry/kohya-colab/main/assets/github.svg)](https://github.com/hollowstrawberry/kohya-colab/blob/main/Lora_Trainer.ipynb) | [![Open in Colab](https://raw.githubusercontent.com/hollowstrawberry/kohya-colab/main/assets/colab-badge.svg)](https://colab.research.google.com/github/hollowstrawberry/kohya-colab/blob/main/Lora_Trainer.ipynb) | [![Abrir en Colab](https://raw.githubusercontent.com/hollowstrawberry/kohya-colab/main/assets/colab-badge-spanish.svg)](https://colab.research.google.com/github/hollowstrawberry/kohya-colab/blob/main/Spanish_Lora_Trainer.ipynb) |\n","| üåü **XL Lora Trainer** | [![GitHub](https://raw.githubusercontent.com/hollowstrawberry/kohya-colab/main/assets/github.svg)](https://github.com/hollowstrawberry/kohya-colab/blob/main/Lora_Trainer_XL.ipynb) | [![Open in Colab](https://raw.githubusercontent.com/hollowstrawberry/kohya-colab/main/assets/colab-badge.svg)](https://colab.research.google.com/github/hollowstrawberry/kohya-colab/blob/main/Lora_Trainer_XL.ipynb) |  |\n","| üåü **Legacy XL Trainer** | [![GitHub](https://raw.githubusercontent.com/hollowstrawberry/kohya-colab/main/assets/github.svg)](https://github.com/hollowstrawberry/kohya-colab/blob/main/Lora_Trainer_XL_Legacy.ipynb) | [![Open in Colab](https://raw.githubusercontent.com/hollowstrawberry/kohya-colab/main/assets/colab-badge.svg)](https://colab.research.google.com/github/hollowstrawberry/kohya-colab/blob/main/Lora_Trainer_XL_Legacy.ipynb) |  |"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"cBa7KdewQ4BU","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1744563146050,"user_tz":-540,"elapsed":13257,"user":{"displayName":"Ïù¥ÏãúÏö±","userId":"14216727192530419875"}},"outputId":"19fb9a04-ef3c-4c0b-8e3c-da7538d634f0"},"outputs":[{"output_type":"stream","name":"stdout","text":["üìÇ Connecting to Google Drive...\n","Mounted at /content/drive\n","‚úÖ Project Pastel_Dreamscape is ready!\n"]}],"source":["import os\n","from IPython import get_ipython\n","from IPython.display import display, Markdown\n","\n","COLAB = True\n","\n","if COLAB:\n","  from google.colab.output import clear as clear_output\n","else:\n","  from IPython.display import clear_output\n","\n","#@title ## üö© Start Here\n","\n","#@markdown ### 1Ô∏è‚É£ Setup\n","#@markdown This cell will load some requirements and create the necessary folders in your Google Drive. <p>\n","#@markdown Your project name can't contain spaces but it can contain a single / to make a subfolder in your dataset.\n","project_name = \"Pastel_Dreamscape\" #@param {type:\"string\"}\n","project_name = project_name.strip()\n","#@markdown The folder structure doesn't matter and is purely for comfort. Make sure to always pick the same one. I like organizing by project.\n","folder_structure = \"Organize by project (MyDrive/Loras/project_name/dataset)\" #@param [\"Organize by category (MyDrive/lora_training/datasets/project_name)\", \"Organize by project (MyDrive/Loras/project_name/dataset)\"]\n","\n","if not project_name or any(c in project_name for c in \" .()\\\"'\\\\\") or project_name.count(\"/\") > 1:\n","  print(\"Please write a valid project_name.\")\n","else:\n","  if COLAB and not os.path.exists('/content/drive'):\n","    from google.colab import drive\n","    print(\"üìÇ Connecting to Google Drive...\")\n","    drive.mount('/content/drive')\n","\n","  project_base = project_name if \"/\" not in project_name else project_name[:project_name.rfind(\"/\")]\n","  project_subfolder = project_name if \"/\" not in project_name else project_name[project_name.rfind(\"/\")+1:]\n","\n","  root_dir = \"/content\" if COLAB else \"~/Loras\"\n","  deps_dir = os.path.join(root_dir, \"deps\")\n","\n","  if \"/Loras\" in folder_structure:\n","    main_dir      = os.path.join(root_dir, \"drive/MyDrive/Loras\") if COLAB else root_dir\n","    config_folder = os.path.join(main_dir, project_base)\n","    images_folder = os.path.join(main_dir, project_base, \"dataset\")\n","    if \"/\" in project_name:\n","      images_folder = os.path.join(images_folder, project_subfolder)\n","  else:\n","    main_dir      = os.path.join(root_dir, \"drive/MyDrive/lora_training\") if COLAB else root_dir\n","    config_folder = os.path.join(main_dir, \"config\", project_name)\n","    images_folder = os.path.join(main_dir, \"datasets\", project_name)\n","\n","  for dir in [main_dir, deps_dir, images_folder, config_folder]:\n","    os.makedirs(dir, exist_ok=True)\n","\n","  print(f\"‚úÖ Project {project_name} is ready!\")\n","  step1_installed_flag = True\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"b218DEEMpwzB","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1744563244887,"user_tz":-540,"elapsed":91735,"user":{"displayName":"Ïù¥ÏãúÏö±","userId":"14216727192530419875"}},"outputId":"7063e8a6-1aeb-431b-c5d0-0c4656b1a262","cellView":"form"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","üíø Analyzing dataset...\n","\n"," 100% |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 25/25 [17.5ms elapsed, 0s remaining, 1.4K samples/s]      \n"]},{"output_type":"stream","name":"stderr","text":["INFO:eta.core.utils: 100% |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 25/25 [17.5ms elapsed, 0s remaining, 1.4K samples/s]      \n"]},{"output_type":"stream","name":"stdout","text":["Downloading model from 'https://openaipublic.azureedge.net/clip/models/40d365715913c9da98579312b702a82c18be219cc2a73407c4526f58eba950af/ViT-B-32.pt'...\n"]},{"output_type":"stream","name":"stderr","text":["INFO:fiftyone.core.models:Downloading model from 'https://openaipublic.azureedge.net/clip/models/40d365715913c9da98579312b702a82c18be219cc2a73407c4526f58eba950af/ViT-B-32.pt'...\n"]},{"output_type":"stream","name":"stdout","text":[" 100% |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà|    2.6Gb/2.6Gb [19.8s elapsed, 0s remaining, 122.4Mb/s]      \n"]},{"output_type":"stream","name":"stderr","text":["INFO:eta.core.utils: 100% |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà|    2.6Gb/2.6Gb [19.8s elapsed, 0s remaining, 122.4Mb/s]      \n"]},{"output_type":"stream","name":"stdout","text":["Downloading CLIP tokenizer...\n"]},{"output_type":"stream","name":"stderr","text":["INFO:fiftyone.utils.clip.zoo:Downloading CLIP tokenizer...\n"]},{"output_type":"stream","name":"stdout","text":[" 100% |‚ñà‚ñà‚ñà‚ñà‚ñà|   10.4Mb/10.4Mb [65.4ms elapsed, 0s remaining, 158.2Mb/s]     \n"]},{"output_type":"stream","name":"stderr","text":["INFO:eta.core.utils: 100% |‚ñà‚ñà‚ñà‚ñà‚ñà|   10.4Mb/10.4Mb [65.4ms elapsed, 0s remaining, 158.2Mb/s]     \n"]},{"output_type":"stream","name":"stdout","text":[" 100% |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 25/25 [8.6s elapsed, 0s remaining, 2.9 samples/s] \n"]},{"output_type":"stream","name":"stderr","text":["INFO:eta.core.utils: 100% |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 25/25 [8.6s elapsed, 0s remaining, 2.9 samples/s] \n"]},{"output_type":"stream","name":"stdout","text":[" 100% |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 25/25 [455.0ms elapsed, 0s remaining, 54.9 samples/s]      \n"]},{"output_type":"stream","name":"stderr","text":["INFO:eta.core.utils: 100% |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 25/25 [455.0ms elapsed, 0s remaining, 54.9 samples/s]      \n"]},{"output_type":"stream","name":"stdout","text":["\n","‚úÖ Removed 0 images from dataset. You now have 25 images.\n"]}],"source":["if \"step1_installed_flag\" not in globals():\n","  raise Exception(\"Please run step 1 first!\")\n","\n","#@markdown ### 3Ô∏è‚É£ Curate your images\n","#@markdown We will find duplicate images with the FiftyOne AI and delete them. <p>\n","#@markdown This is how similar 2 images must be to be marked for deletion. I recommend 0.97 to 0.99:\n","similarity_threshold = 0.985 #@param {type:\"number\"}\n","#@markdown You can choose between only deleting the duplicates, or additionally opening an interactive area below this cell that lets you visualize all your images and manually mark with `delete` to the ones you don't like. <p>\n","#@markdown If the interactive area appears blank for over a minute, try enabling cookies and removing tracking protection for the Google Colab website, as they may break it.\n","#@markdown Regardless, you can save your changes by sending Enter in the input box above the interactive area.<p>\n","action = \"Delete duplicates\" #@param [\"Delete duplicates\",\"Mark duplicates and open interactive area\",\"Open interactive area\"]\n","#@markdown To open the interactive area in a new tab INSTEAD of below, you need an ngrok account.\n","open_in_new_tab = False #@param {type:\"boolean\"}\n","ngrok_token = \"\" #@param {type:\"string\"}\n","\n","\n","os.chdir(root_dir)\n","model_name = \"clip-vit-base32-torch\"\n","supported_types = (\".png\", \".jpg\", \".jpeg\")\n","img_count = len(os.listdir(images_folder))\n","batch_size = min(250, img_count)\n","\n","if \"step3_installed_flag\" not in globals():\n","  print(\"üè≠ Installing dependencies...\\n\")\n","  !pip -q install fiftyone ftfy pyngrok\n","  !pip -q install fiftyone-db-ubuntu2204\n","  if not get_ipython().__dict__['user_ns']['_exit_code']:\n","    clear_output()\n","    step3_installed_flag = True\n","  else:\n","    print(\"‚ùå Error installing dependencies, attempting to continue anyway...\")\n","\n","os.environ[\"FIFTYONE_SERVER\"] = \"0\"\n","import numpy as np\n","import fiftyone as fo\n","import fiftyone.zoo as foz\n","from fiftyone import ViewField as F\n","from sklearn.metrics.pairwise import cosine_similarity\n","from pyngrok import ngrok, conf\n","from portpicker import pick_unused_port\n","\n","non_images = [f for f in os.listdir(images_folder) if not f.lower().endswith(supported_types)]\n","if non_images:\n","  print(f\"üí• Error: Found non-image file {non_images[0]} - This program doesn't allow it. Sorry! Use the Extras at the bottom to clean the folder.\")\n","elif img_count == 0:\n","  print(f\"üí• Error: No images found in {images_folder}\")\n","else:\n","  print(\"\\nüíø Analyzing dataset...\\n\")\n","  dataset = fo.Dataset.from_dir(images_folder, dataset_type=fo.types.ImageDirectory)\n","  if \"duplicates\" in action:\n","    model = foz.load_zoo_model(model_name)\n","    embeddings = dataset.compute_embeddings(model, batch_size=batch_size)\n","\n","    batch_embeddings = np.array_split(embeddings, batch_size)\n","    similarity_matrices = []\n","    max_size_x = max(array.shape[0] for array in batch_embeddings)\n","    max_size_y = max(array.shape[1] for array in batch_embeddings)\n","\n","    for i, batch_embedding in enumerate(batch_embeddings):\n","      similarity = cosine_similarity(batch_embedding)\n","      #Pad 0 for np.concatenate\n","      padded_array = np.zeros((max_size_x, max_size_y))\n","      padded_array[0:similarity.shape[0], 0:similarity.shape[1]] = similarity\n","      similarity_matrices.append(padded_array)\n","\n","    similarity_matrix = np.concatenate(similarity_matrices, axis=0)\n","    similarity_matrix = similarity_matrix[0:embeddings.shape[0], 0:embeddings.shape[0]]\n","\n","    similarity_matrix = cosine_similarity(embeddings)\n","    similarity_matrix -= np.identity(len(similarity_matrix))\n","\n","    dataset.match(F(\"max_similarity\") > similarity_threshold)\n","    dataset.tags = [\"delete\", \"has_duplicates\"]\n","\n","    id_map = [s.id for s in dataset.select_fields([\"id\"])]\n","    samples_to_remove = set()\n","    samples_to_keep = set()\n","\n","    for idx, sample in enumerate(dataset):\n","      if sample.id not in samples_to_remove:\n","        # Keep the first instance of two duplicates\n","        samples_to_keep.add(sample.id)\n","\n","        dup_idxs = np.where(similarity_matrix[idx] > similarity_threshold)[0]\n","        for dup in dup_idxs:\n","            # We kept the first instance so remove all other duplicates\n","            samples_to_remove.add(id_map[dup])\n","\n","        if len(dup_idxs) > 0:\n","            sample.tags.append(\"has_duplicates\")\n","            sample.save()\n","      else:\n","        sample.tags.append(\"delete\")\n","        sample.save()\n","\n","    sidebar_groups = fo.DatasetAppConfig.default_sidebar_groups(dataset)\n","    for group in sidebar_groups[1:]:\n","      group.expanded = False\n","    dataset.app_config.sidebar_groups = sidebar_groups\n","    dataset.save()\n","\n","  if \"interactive\" in action:\n","    clear_output()\n","    os.environ[\"FIFTYONE_SERVER\"] = \"1\"\n","    port = pick_unused_port()\n","    session = fo.launch_app(dataset, port=port, auto=not open_in_new_tab)\n","    if open_in_new_tab:\n","      conf.get_default().auth_token = ngrok_token\n","      public_url = ngrok.connect(port).public_url\n","      print(f\"üü¢ Session open at {public_url}\")\n","\n","    print(\"‚ùó Wait a minute for the session to load. If it doesn't, read above.\")\n","    print(\"‚ùó When it's ready, you'll see a grid of your images.\")\n","    print(\"‚ùó On the left side enable \\\"sample tags\\\" to visualize the images marked for deletion.\")\n","    print(\"‚ùó You can mark your own images with the \\\"delete\\\" label by selecting them and pressing the tag icon at the top.\")\n","    input(\"‚≠ï When you're done, enter something here to save your changes: \")\n","\n","    print(\"üíæ Saving...\")\n","\n","  marked = [s for s in dataset if \"delete\" in s.tags]\n","  dataset.delete_samples(marked)\n","  previous_folder = images_folder[:images_folder.rfind(\"/\")]\n","  dataset.export(export_dir=os.path.join(images_folder, project_subfolder), dataset_type=fo.types.ImageDirectory)\n","\n","  temp_suffix = \"_temp\"\n","  !mv {images_folder} {images_folder}{temp_suffix}\n","  !mv {images_folder}{temp_suffix}/{project_subfolder} {images_folder}\n","  !rm -r {images_folder}{temp_suffix}\n","\n","  if \"interactive\" in action:\n","    session.refresh()\n","    fo.close_app()\n","    clear_output()\n","\n","  print(f\"\\n‚úÖ Removed {len(marked)} images from dataset. You now have {len(os.listdir(images_folder))} images.\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"sl4FD7Mz-uea","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1744563959094,"user_tz":-540,"elapsed":18529,"user":{"displayName":"Ïù¥ÏãúÏö±","userId":"14216727192530419875"}},"outputId":"7af92f62-6587-4731-e47a-9c43548d7dfb"},"outputs":[{"output_type":"stream","name":"stdout","text":["üìä Captioning complete. Here are 10 example captions from your dataset:\n","a person is standing on a beach with a surfboard\n","\n","a purple sky with clouds and a pink sky\n","\n","a field of lavender with the sun setting in the background\n","\n","a beach with a pink sky and a few people\n","\n","a field with tall grass and a foggy sky\n","\n","a couple of people on a beach with a surfboard\n","\n","a person walking on a beach with a surfboard\n","\n","a boat is sitting on the frozen water\n","\n","a field of purple flowers next to a lake\n","\n","a mountain range with a pink sky in the background\n","\n"]}],"source":["if \"step1_installed_flag\" not in globals():\n","  raise Exception(\"Please run step 1 first!\")\n","\n","#@markdown ### 4Ô∏è‚É£ Tag your images\n","#@markdown We will be using AI to automatically tag your images, specifically [Waifu Diffusion](https://huggingface.co/SmilingWolf/wd-eva02-large-tagger-v3) in the case of anime and [BLIP](https://huggingface.co/spaces/Salesforce/BLIP) in the case of photos. <p>\n","#@markdown Giving tags/captions to your images allows for much better training. This process takes 5 minutes to install and 5 more minutes to tag a thousand images. It goes through all subfolders if you have any. <p>\n","method = \"Photo captions\" #@param [\"Anime tags\", \"Photo captions\"]\n","#@markdown **Anime:** Using both taggers will be more accurate than one or the other. Lower threshold will yield more tags, try 0.25 for concepts and 0.50 for styles. You should include character names if you're not training a character.\n","tagger = \"Both\" #@param [\"Both\",\"SmilingWolf/wd-eva02-large-tagger-v3\",\"SmilingWolf/wd-vit-large-tagger-v3\"]\n","tag_threshold = 0.25 #@param {type:\"slider\", min:0.0, max:1.0, step:0.01}\n","blacklist_tags = \"virtual youtuber, parody, style parody, official alternate costume, official alternate hairstyle, official alternate hair length, alternate costume, alternate hairstyle, alternate hair length, alternate hair color\" #@param {type:\"string\"}\n","include_character_names = False #@param {type:\"boolean\"}\n","#@markdown **Photos:** The minimum and maximum length of tokens/words in each caption.\n","caption_min = 10 #@param {type:\"number\"}\n","caption_max = 75 #@param {type:\"number\"}\n","\n","character_threshold = tag_threshold if include_character_names else 1.1\n","undesired_tags = '\"' + ','.join([t.strip() for t in blacklist_tags.split(\",\") if t.strip()]) + '\"'\n","\n","kohya_dir = \"/content/kohya\"\n","venv_python = os.path.join(kohya_dir, \"venv/bin/python\")\n","venv_pip = os.path.join(kohya_dir, \"venv/bin/pip\")\n","\n","if \"step4_installed_flag\" not in globals():\n","  print(\"\\nüè≠ Installing dependencies...\\n\")\n","  !apt install -y python3.10-venv -qq\n","  !git clone https://github.com/kohya-ss/sd-scripts {kohya_dir}\n","  os.chdir(kohya_dir)\n","  !git reset --hard e89653975ddf429cdf0c0fd268da0a5a3e8dba1f\n","  !python3.10 -m venv venv\n","  !{venv_pip} install -r requirements.txt\n","  !{venv_pip} install fairscale==0.4.13 timm==0.6.12\n","  !{venv_pip} install onnx onnxruntime-gpu==1.20.1 --extra-index-url https://aiinfra.pkgs.visualstudio.com/PublicPackages/_packaging/onnxruntime-cuda-12/pypi/simple/\n","  !{venv_pip} uninstall -y rich\n","  step4_installed_flag = True\n","\n","print(\"\\nüö∂‚Äç‚ôÇÔ∏è Launching program...\\n\")\n","os.chdir(kohya_dir)\n","\n","if \"Anime\" in method:\n","  tagger_models = [\n","    \"SmilingWolf/wd-eva02-large-tagger-v3\",\n","    \"SmilingWolf/wd-vit-large-tagger-v3\"\n","  ] if tagger == \"Both\" else [tagger]\n","\n","  for i, tagger_model in enumerate(tagger_models):\n","    append_tags = \"--append_tags\" if i > 0 else \"\"\n","    !{venv_python} finetune/tag_images_by_wd14_tagger.py \\\n","      {images_folder} \\\n","      --repo_id={tagger_model} \\\n","      --general_threshold={tag_threshold} \\\n","      --character_threshold={character_threshold} \\\n","      --batch_size=8 \\\n","      --max_data_loader_n_workers=2 \\\n","      --caption_extension=.txt \\\n","      --undesired_tags {undesired_tags} \\\n","      --onnx --recursive --remove_underscore {append_tags}\n","\n","  if not get_ipython().__dict__['user_ns']['_exit_code']:\n","    # Count tags\n","    from collections import Counter\n","    text_files = []\n","    for root, dirs, files in os.walk(images_folder):\n","      for file in files:\n","        if file.lower().endswith(\".txt\"):\n","          text_files.append(os.path.join(root, file))\n","    top_tags = Counter()\n","    for file in text_files:\n","      with open(file, 'r') as f:\n","        tags = [t.strip() for t in f.read().split(\",\")]\n","      top_tags.update(tags)\n","\n","    clear_output()\n","    print(f\"üìä Tagging complete. Here are the top 50 tags in your dataset:\")\n","    print(\"\\n\".join(f\"{k} ({v})\" for k, v in top_tags.most_common(50)))\n","\n","else:\n","  !{venv_python} finetune/make_captions.py \\\n","    {images_folder} \\\n","    --beam_search \\\n","    --max_data_loader_n_workers=2 \\\n","    --batch_size=8 \\\n","    --min_length={caption_min} \\\n","    --max_length={caption_max} \\\n","    --caption_extension=.txt \\\n","    --recursive\n","\n","  if not get_ipython().__dict__['user_ns']['_exit_code']:\n","    import os\n","    import random\n","    from IPython.display import clear_output\n","\n","    # ‚úÖ caption ÌååÏùº Í≤ΩÎ°ú ÏàòÏßë\n","    caption_files = []\n","    for root, dirs, files in os.walk(images_folder):\n","      for file in files:\n","        if file.lower().endswith(\".txt\"):\n","          caption_files.append(os.path.join(root, file))\n","\n","    # ‚úÖ 10Í∞ú ÏÉòÌîå Ï∂úÎ†•\n","    sample = []\n","    for txt_path in random.sample(caption_files, min(10, len(caption_files))):\n","      with open(txt_path, 'r') as f:\n","        sample.append(f.read())\n","\n","    clear_output()\n","    print(f\"üìä Captioning complete. Here are {len(sample)} example captions from your dataset:\")\n","    print(\"\\n\".join(sample))\n","\n","os.chdir(root_dir)"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"WBFik7accyDz"},"outputs":[],"source":["if \"step1_installed_flag\" not in globals():\n","  raise Exception(\"Please run step 1 first!\")\n","\n","# @markdown ### 5Ô∏è‚É£ Curate your tags - Optional for Keyword\n","# @markdown Modify your dataset's tags. You can run this cell multiple times with different parameters. <p>\n","\n","#@markdown Put an activation tag at the start of every text file. This is useful to make learning better and activate your Lora easier. Set `keep_tokens` to 1 when training.<p>\n","#@markdown Common tags that are removed such as hair color, etc. will be \"absorbed\" by your activation tag.\n","global_activation_tag = \"\" #@param {type:\"string\"}\n","remove_tags = \"\" #@param {type:\"string\"}\n","#@markdown &nbsp;\n","\n","#@markdown In this advanced section, you can search text files containing matching tags, and replace them with less/more/different tags. If you select the checkbox below, any extra tags will be put at the start of the file, letting you assign different activation tags to different parts of your dataset. Still, you may want a more advanced tool for this.\n","search_tags = \"\" #@param {type:\"string\"}\n","replace_with = \"\" #@param {type:\"string\"}\n","search_mode = \"OR\" #@param [\"OR\", \"AND\"]\n","new_becomes_activation_tag = False #@param {type:\"boolean\"}\n","#@markdown These may be useful sometimes. Will remove existing activation tags, be careful.\n","sort_alphabetically = False #@param {type:\"boolean\"}\n","remove_duplicates = False #@param {type:\"boolean\"}\n","\n","def split_tags(tagstr):\n","  return [s.strip() for s in tagstr.split(\",\") if s.strip()]\n","\n","activation_tag_list = split_tags(global_activation_tag)\n","remove_tags_list = split_tags(remove_tags)\n","search_tags_list = split_tags(search_tags)\n","replace_with_list = split_tags(replace_with)\n","replace_new_list = [t for t in replace_with_list if t not in search_tags_list]\n","\n","replace_with_list = [t for t in replace_with_list if t not in replace_new_list]\n","replace_new_list.reverse()\n","activation_tag_list.reverse()\n","\n","remove_count = 0\n","replace_count = 0\n","\n","text_files = []\n","for root, dirs, files in os.walk(images_folder):\n","  for file in files:\n","    if file.lower().endswith(\".txt\"):\n","      text_files.append(os.path.join(root, file))\n","\n","for txt in text_files:\n","\n","  with open(os.path.join(images_folder, txt), 'r') as f:\n","    tags = [s.strip() for s in f.read().split(\",\") if s.strip()]\n","\n","  if remove_duplicates:\n","    tags = list(set(tags))\n","  if sort_alphabetically:\n","    tags.sort()\n","\n","  for rem in remove_tags_list:\n","    if rem in tags:\n","      remove_count += 1\n","      tags.remove(rem)\n","\n","  if \"AND\" in search_mode and all(r in tags for r in search_tags_list) \\\n","      or \"OR\" in search_mode and any(r in tags for r in search_tags_list):\n","    replace_count += 1\n","    for rem in search_tags_list:\n","      if rem in tags:\n","        tags.remove(rem)\n","    for add in replace_with_list:\n","      if add not in tags:\n","        tags.append(add)\n","    for new in replace_new_list:\n","      if new_becomes_activation_tag:\n","        if new in tags:\n","          tags.remove(new)\n","        tags.insert(0, new)\n","      else:\n","        if new not in tags:\n","          tags.append(new)\n","\n","  for act in activation_tag_list:\n","    if act in tags:\n","      tags.remove(act)\n","    tags.insert(0, act)\n","\n","  with open(os.path.join(images_folder, txt), 'w') as f:\n","    f.write(\", \".join(tags))\n","\n","if global_activation_tag:\n","  print(f\"\\nüìé Applied new activation tag(s): {', '.join(activation_tag_list)}\")\n","if remove_tags:\n","  print(f\"\\nüöÆ Removed {remove_count} tags.\")\n","if search_tags:\n","  print(f\"\\nüí´ Replaced in {replace_count} files.\")\n","print(\"\\n‚úÖ Done! Check your updated tags in the Extras below.\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HuJB7BGAyZCw","colab":{"base_uri":"https://localhost:8080/","height":58},"executionInfo":{"status":"ok","timestamp":1744123660541,"user_tz":-540,"elapsed":17,"user":{"displayName":"Ïù¥ÏãúÏö±","userId":"14216727192530419875"}},"outputId":"66251068-49c0-460f-9f1f-67378f29423b","cellView":"form"},"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"### ü¶Ä [Click here to open the Lora trainer](https://colab.research.google.com/github/hollowstrawberry/kohya-colab/blob/main/Lora_Trainer.ipynb)"},"metadata":{}}],"source":["#@markdown ### 6Ô∏è‚É£ Ready\n","#@markdown You should be ready to [train your Lora](https://colab.research.google.com/github/hollowstrawberry/kohya-colab/blob/main/Lora_Trainer.ipynb)!\n","\n","from IPython.display import Markdown, display\n","display(Markdown(f\"### ü¶Ä [Click here to open the Lora trainer](https://colab.research.google.com/github/hollowstrawberry/kohya-colab/blob/main/Lora_Trainer.ipynb)\"))\n"]},{"cell_type":"markdown","metadata":{"id":"gDB9GXRONfiU"},"source":["## *Ô∏è‚É£ Extras"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"xEsqOglcc6hA"},"outputs":[],"source":["if \"step1_installed_flag\" not in globals():\n","  raise Exception(\"Please run step 1 first!\")\n","\n","#@markdown ### üìà Analyze Tags\n","#@markdown Perhaps you need another look at your dataset.\n","show_top_tags = 50 #@param {type:\"number\"}\n","\n","text_files = []\n","for root, dirs, files in os.walk(images_folder):\n","  for file in files:\n","    if file.lower().endswith(\".txt\"):\n","      text_files.append(os.path.join(root, file))\n","\n","from collections import Counter\n","top_tags = Counter()\n","for file in text_files:\n","  with open(file, 'r') as f:\n","    tags = [t.strip() for t in f.read().split(\",\")]\n","  top_tags.update(tags)\n","\n","print(f\"üìä Top {show_top_tags} tags:\")\n","for k, v in top_tags.most_common(show_top_tags):\n","  print(f\"{k} ({v})\")"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"x56xQYwuOz2V"},"outputs":[],"source":["#@markdown ### üìÇ Unzip dataset\n","#@markdown It's much slower to upload individual files to your Drive, so you may want to upload a zip if you have your dataset in your computer.\n","zip = \"/content/drive/MyDrive/Loras/example.zip\" #@param {type:\"string\"}\n","extract_to = \"/content/drive/MyDrive/Loras/example/dataset\" #@param {type:\"string\"}\n","\n","import os, zipfile\n","\n","if not os.path.exists('/content/drive'):\n","  from google.colab import drive\n","  print(\"üìÇ Connecting to Google Drive...\")\n","  drive.mount('/content/drive')\n","\n","os.makedirs(extract_to, exist_ok=True)\n","\n","with zipfile.ZipFile(zip, 'r') as f:\n","  f.extractall(extract_to)\n","\n","print(\"‚úÖ Done\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"dLetTcLVOvAE"},"outputs":[],"source":["#@markdown ### üî¢ Count datasets\n","#@markdown Google Drive makes it impossible to count the files in a folder, so this will show you the file counts in all folders and subfolders.\n","folder = \"/content/drive/MyDrive/Loras\" #@param {type:\"string\"}\n","\n","import os\n","from google.colab import drive\n","\n","if not os.path.exists('/content/drive'):\n","    print(\"üìÇ Connecting to Google Drive...\\n\")\n","    drive.mount('/content/drive')\n","\n","tree = {}\n","exclude = (\"_logs\", \"/output\")\n","for i, (root, dirs, files) in enumerate(os.walk(folder, topdown=True)):\n","  dirs[:] = [d for d in dirs if all(ex not in d for ex in exclude)]\n","  images = len([f for f in files if f.lower().endswith((\".png\", \".jpg\", \".jpeg\"))])\n","  captions = len([f for f in files if f.lower().endswith(\".txt\")])\n","  others = len(files) - images - captions\n","  path = root[folder.rfind(\"/\")+1:]\n","  tree[path] = None if not images else f\"{images:>4} images | {captions:>4} captions |\"\n","  if tree[path] and others:\n","    tree[path] += f\" {others:>4} other files\"\n","\n","pad = max(len(k) for k in tree)\n","print(\"\\n\".join(f\"üìÅ{k.ljust(pad)} | {v}\" for k, v in tree.items() if v))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"dt8PmSM5_iI0"},"outputs":[],"source":["if \"step1_installed_flag\" not in globals():\n","  raise Exception(\"Please run step 1 first!\")\n","\n","from PIL import Image\n","import os\n","Image.MAX_IMAGE_PIXELS = None\n","\n","#@markdown ### üñºÔ∏è Reduce dataset filesize\n","#@markdown This will convert all images in the project folder to jpeg, reducing filesize without affecting quality too much. This can also solve some errors.\n","location = images_folder\n","\n","for dir in [d[0] for d in os.walk(location)]:\n","    os.chdir(dir)\n","    converted = False\n","    for file_name in list(os.listdir(\".\")):\n","        try:\n","            # Convert png to jpeg\n","            if file_name.endswith(\".png\"):\n","                if not converted:\n","                    print(f\"Converting {dir}\")\n","                    converted = True\n","                im = Image.open(file_name)\n","                im = im.convert(\"RGB\")\n","                new_file_name = os.path.splitext(file_name)[0] + \".jpeg\"\n","                im.save(new_file_name, quality=95)\n","                os.remove(file_name)\n","                file_name = new_file_name\n","            # Resize large jpegs\n","            if file_name.endswith((\".jpeg\", \".jpg\")) and os.path.getsize(file_name) > 2000000:\n","                if not converted:\n","                    print(f\"Converting {dir}\")\n","                    converted = True\n","                im = Image.open(file_name)\n","                im = im.resize((int(im.width/2), int(im.height/2)))\n","                im.save(file_name, quality=95)\n","            # Rename jpg to jpeg\n","            if file_name.endswith(\".jpg\"):\n","                if not converted:\n","                    print(f\"Converting {dir}\")\n","                new_file_name = os.path.splitext(file_name)[0] + \".jpeg\"\n","                os.rename(file_name, new_file_name)\n","        except Exception as e:\n","            print(f\"An error occurred while processing {file_name}: {e}\")\n","    if converted:\n","        print(f\"Converted {dir}\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"y6PKW-LIr214"},"outputs":[],"source":["if \"step1_installed_flag\" not in globals():\n","  raise Exception(\"Please run step 1 first!\")\n","\n","#@markdown ### üöÆ Clean folder\n","#@markdown Careful! Deletes all non-image files in the project folder.\n","\n","!find {images_folder} -type f ! \\( -iname '*.png' -o -iname '*.jpg' -o -iname '*.jpeg' \\) -delete\n"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[{"file_id":"https://github.com/hollowstrawberry/kohya-colab/blob/main/Dataset_Maker.ipynb","timestamp":1743950890458}],"machine_shape":"hm","gpuType":"A100"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}