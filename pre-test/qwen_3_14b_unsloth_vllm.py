# -*- coding: utf-8 -*-
"""qwen_3_14b_unsloth_vllm.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1buc_WTrNsqaFIPd-5AQolvwZmfSHSjyD
"""

# @title 1. 라이브러리 설치 및 환경 설정
# ---------------------------------------------
# 중요: GPU 런타임 필요 (T4 이상 권장)
# ---------------------------------------------
import os
import torch
import subprocess
import threading
import time
import requests
from getpass import getpass
# from google.colab import drive


# GPU 확인
if not torch.cuda.is_available():
    raise RuntimeError("GPU 런타임을 사용해주세요. '런타임' -> '런타임 유형 변경' -> '하드웨어 가속기'에서 GPU를 선택하세요.")
else:
    print(f"GPU 감지됨: {torch.cuda.get_device_name(0)}")
    print(f"GPU VRAM: {torch.cuda.get_device_properties(0).total_memory / (1024**3):.2f} GB")

# Unsloth 및 기타 필수 라이브러리 설치 (Colab 환경 기준)
print("라이브러리 설치 중...")
os.environ['COLAB_RELEASE_TAG'] = 'Unsloth 2025.4' # 예시 버전, 필요시 최신 확인
#!pip install "unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git"
#!pip install --no-deps xformers # 버전 명시 제거 또는 최신 호환 버전 확인
#!pip install transformers accelerate bitsandbytes peft trl # 최신 호환 버전 시도
#!pip install huggingface_hub hf_transfer pyngrok packaging

#!pip install --upgrade huggingface_hub hf_transfer pyngrok packaging vllm  unsloth torch

print("라이브러리 설치 완료.")

# HF_TRANSFER 활성화 (다운로드 속도 향상)
os.environ["HF_HUB_ENABLE_HF_TRANSFER"] = "1"

# @title 2. 로그인 정보 수동 입력 및 Google Drive 마운트

# Hugging Face 로그인
print("--- Hugging Face 로그인 ---")
HF_TOKEN = getpass('Hugging Face 접근 토큰(write 권한)을 입력하세요: ')
if not HF_TOKEN:
    raise ValueError("Hugging Face 토큰이 필요합니다.")
from huggingface_hub import login
login(token=HF_TOKEN)
print("Hugging Face 로그인 성공.")

# Ngrok 로그인
print("\n--- Ngrok 설정 ---")
NGROK_AUTH_TOKEN = getpass('Ngrok 인증 토큰을 입력하세요: ')
if not NGROK_AUTH_TOKEN:
    raise ValueError("Ngrok 인증 토큰이 필요합니다.")

# Ngrok 고정 도메인 입력 (유료 플랜 필요)
NGROK_DOMAIN = input('Ngrok 고정 도메인(예: my-qwen-server.ngrok.app)을 입력하세요: ')
if not NGROK_DOMAIN:
    raise ValueError("Ngrok 고정 도메인이 필요합니다.")

from pyngrok import ngrok, conf
ngrok.set_auth_token(NGROK_AUTH_TOKEN)
print("Ngrok 인증 설정 완료.")

# Google Drive 마운트
print("\nGoogle Drive 마운트 중...")
# drive.mount('/content/drive')
print("Google Drive 마운트 완료.")

# 모델 및 로그 저장 경로 설정
GDRIVE_BASE_PATH = "/content/drive/MyDrive/unsloth_models/Qwen3" # 필요시 경로 수정
MODEL_SAVE_DIR = os.path.join(GDRIVE_BASE_PATH, "qwen3_14b_models")
LOG_DIR = os.path.join(GDRIVE_BASE_PATH, "vllm_logs")
os.makedirs(MODEL_SAVE_DIR, exist_ok=True)
os.makedirs(LOG_DIR, exist_ok=True)

VLLM_LOG_FILE = os.path.join(LOG_DIR, "vllm_server.log")
VLLM_ERR_FILE = os.path.join(LOG_DIR, "vllm_server.err")

print(f"모델 저장 경로: {MODEL_SAVE_DIR}")
print(f"vLLM 로그 저장 경로: {VLLM_LOG_FILE}")
print(f"vLLM 에러 로그 저장 경로: {VLLM_ERR_FILE}")


# 저장할 모델 경로 정의
MODEL_NAME = "unsloth/Qwen3-14B"
MODEL_SAVE_PATH_8BIT = os.path.join(MODEL_SAVE_DIR, "qwen3-14b-8bit")
MODEL_SAVE_PATH_4BIT = os.path.join(MODEL_SAVE_DIR, "qwen3-14b-4bit")

# @title 3. 모델 로드 및 양자화 저장 (Unsloth)
from unsloth import FastLanguageModel
import torch

# --- 4비트 모델 로드 및 저장 ---
if not os.path.exists(os.path.join(MODEL_SAVE_PATH_4BIT, "config.json")): # config.json 존재 여부로 확인
    print("="*30)
    print(f"'{MODEL_NAME}' 모델을 4비트로 로드 중...")
    print("="*30)
    model_4bit, tokenizer_4bit = FastLanguageModel.from_pretrained(
        model_name=MODEL_NAME,
        max_seq_length=2048,
        dtype=None,
        load_in_4bit=True,
        load_in_8bit=False,
    )
    print(f"\n4비트 모델을 '{MODEL_SAVE_PATH_4BIT}'에 저장 중...")
    model_4bit.save_pretrained(MODEL_SAVE_PATH_4BIT)
    tokenizer_4bit.save_pretrained(MODEL_SAVE_PATH_4BIT)
    print("4비트 모델 저장 완료.")
    del model_4bit, tokenizer_4bit
    torch.cuda.empty_cache()
    print("4비트 모델 메모리 해제 완료.")
else:
    print(f"4비트 모델이 이미 '{MODEL_SAVE_PATH_4BIT}'에 존재합니다. 저장을 건너<0xEB><0xA9>니다.")

# --- 8비트 모델 로드 및 저장 ---
if not os.path.exists(os.path.join(MODEL_SAVE_PATH_8BIT, "config.json")): # config.json 존재 여부로 확인
    print("="*30)
    print(f"'{MODEL_NAME}' 모델을 8비트로 로드 중...")
    print("="*30)
    model_8bit, tokenizer_8bit = FastLanguageModel.from_pretrained(
        model_name=MODEL_NAME,
        max_seq_length=2048,
        dtype=None,
        load_in_8bit=True,
        load_in_4bit=False,
    )
    print(f"\n8비트 모델을 '{MODEL_SAVE_PATH_8BIT}'에 저장 중...")
    model_8bit.save_pretrained(MODEL_SAVE_PATH_8BIT)
    tokenizer_8bit.save_pretrained(MODEL_SAVE_PATH_8BIT)
    print("8비트 모델 저장 완료.")
    del model_8bit, tokenizer_8bit
    torch.cuda.empty_cache()
    print("8비트 모델 메모리 해제 완료.")
else:
    print(f"8비트 모델이 이미 '{MODEL_SAVE_PATH_8BIT}'에 존재합니다. 저장을 건너<0xEB><0xA9>니다.")

#!pip install --upgrade transformers accelerate bitsandbytes peft trl # 최신 호환 버전 시도

# @title 4. vLLM 서빙 시작 및 Ngrok 터널링

# --- 서빙할 모델 선택 ---
# MODEL_TO_SERVE_PATH = MODEL_SAVE_PATH_4BIT # 기본 4비트
MODEL_TO_SERVE_PATH = MODEL_SAVE_PATH_8BIT # 8비트 서빙 시 주석 해제

print(f"서빙할 모델 경로: {MODEL_TO_SERVE_PATH}")
if not os.path.exists(MODEL_TO_SERVE_PATH):
     raise FileNotFoundError(f"선택된 모델 경로 '{MODEL_TO_SERVE_PATH}'가 존재하지 않습니다.")

# --- vLLM 서버 시작 ---
VLLM_HOST = "0.0.0.0"
VLLM_PORT = 8500
GPU_MEMORY_UTILIZATION = 0.4

vllm_command = [
    "python", "-m", "vllm.entrypoints.openai.api_server",
    "--model", MODEL_TO_SERVE_PATH,
    "--host", VLLM_HOST,
    "--port", str(VLLM_PORT),
    "--trust-remote-code",
    "--quantization", "bitsandbytes",  # 이 옵션 추가 또는 주석 해제
    "--gpu-memory-utilization", str(GPU_MEMORY_UTILIZATION),
    "--max-model-len", "2048",
]

print("\nvLLM 서버를 백그라운드에서 시작하고 로그를 파일에 저장합니다...")
print(f"명령어: {' '.join(vllm_command)}")
print(f"표준 출력 로그: {VLLM_LOG_FILE}")
print(f"표준 에러 로그: {VLLM_ERR_FILE}")

# 로그 파일을 쓰기 모드(w)로 열어 기존 내용 덮어쓰기
with open(VLLM_LOG_FILE, "w") as log_out, open(VLLM_ERR_FILE, "w") as log_err:
    vllm_server_process = subprocess.Popen(vllm_command, stdout=log_out, stderr=log_err)

print("vLLM 서버 프로세스가 시작되었습니다 (PID: {}).".format(vllm_server_process.pid))
print("서버가 완전히 시작될 때까지 잠시 대기합니다 (로그 파일을 확인하세요, 약 1-2분 소요)...")
time.sleep(90) # 대기 시간은 환경에 따라 조절

# 간단한 서버 상태 확인 (예: health 엔드포인트)
try:
    response = requests.get(f"http://localhost:{VLLM_PORT}/health", timeout=10)
    if response.status_code == 200:
        print("vLLM 서버가 성공적으로 시작된 것 같습니다 (HTTP 200).")
    else:
        print(f"vLLM 서버 상태 확인 응답 코드: {response.status_code}. {VLLM_ERR_FILE} 파일을 확인하세요.")
except requests.exceptions.RequestException as e:
    print(f"vLLM 서버 연결 실패: {e}. {VLLM_ERR_FILE} 파일을 확인하세요.")


# --- Ngrok 터널 시작 ---
print("\nNgrok 터널 시작 중...")
# 기존 터널 종료
for tunnel in ngrok.get_tunnels():
    ngrok.disconnect(tunnel.public_url)
    print(f"기존 Ngrok 터널 종료: {tunnel.public_url}")

try:
    public_url = ngrok.connect(VLLM_PORT, proto="http", hostname=NGROK_DOMAIN).public_url
    print("="*50)
    print(f"✅ vLLM 서버가 Ngrok을 통해 다음 주소에서 실행 중입니다:")
    print(public_url)
    print(f"(이 주소는 OpenAI API Base URL로 사용될 수 있습니다. 예: {public_url}/v1)")
    print("="*50)
    print(f"\nvLLM 서버 로그는 Google Drive의 다음 파일에서 확인 가능합니다:")
    print(f"  - 표준 출력: {VLLM_LOG_FILE}")
    print(f"  - 표준 에러: {VLLM_ERR_FILE}")
    print("\nColab 세션을 유지해야 서버가 계속 실행됩니다.")
    print("종료하려면 Colab 런타임을 중지하세요.")

    print("\n서버 실행 중... (Colab 자동 종료 방지)")
    while True:
        # vLLM 프로세스 상태 확인 (선택적)
        if vllm_server_process.poll() is not None:
            print(f"vLLM 서버 프로세스가 종료되었습니다. 종료 코드: {vllm_server_process.returncode}")
            print(f"자세한 내용은 {VLLM_LOG_FILE} 및 {VLLM_ERR_FILE} 파일을 확인하세요.")
            break
        time.sleep(60)

except Exception as e:
    print(f"Ngrok 터널 생성 실패: {e}")
    print("Ngrok 설정(인증 토큰, 고정 도메인 유효성) 및 vLLM 서버 상태를 확인하세요.")

# finally:
#     print("\n정리 작업 수행...")
#     ngrok.kill()
#     if 'vllm_server_process' in locals() and vllm_server_process.poll() is None:
#         vllm_server_process.terminate()
#         try:
#             vllm_server_process.wait(timeout=10) # 10초간 종료 대기
#             print("vLLM 서버 프로세스가 정상적으로 종료되었습니다.")
#         except subprocess.TimeoutExpired:
#             vllm_server_process.kill() # 강제 종료
#             print("vLLM 서버 프로세스를 강제 종료했습니다.")
#     print("스크립트 실행 완료.")

finally:
    print("\n정리 작업 수행...")
    ngrok.kill()
    if 'vllm_server_process' in locals() and vllm_server_process.poll() is None:
        vllm_server_process.terminate()
        try:
            vllm_server_process.wait(timeout=10)
            print("vLLM 서버 프로세스가 정상적으로 종료되었습니다.")
        except subprocess.TimeoutExpired:
            vllm_server_process.kill()
            print("vLLM 서버 프로세스를 강제 종료했습니다.")
    torch.cuda.empty_cache() # CUDA 캐시 비우기 추가
    print("스크립트 실행 완료.")

