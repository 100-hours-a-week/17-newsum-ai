{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# # -*- coding: utf-8 -*-\n",
        "# \"\"\"\n",
        "# FastAPI + Diffusers + Cloudflared API ë°°í¬ (Colab ê¸°ë°˜)\n",
        "\n",
        "# - FastAPI: API ì„œë²„ êµ¬ì¶•\n",
        "# - Diffusers: Stable Diffusion ëª¨ë¸ ì¶”ë¡ \n",
        "# - Cloudflared: ì™¸ë¶€ ì¸í„°ë„· ë…¸ì¶œ\n",
        "# - ë™ì  LoRA ë¡œë”©/ì–¸ë¡œë”© API í¬í•¨\n",
        "# \"\"\"\n",
        "\n",
        "# # ==============================================================================\n",
        "# # 0. í™˜ê²½ ì„¤ì • ë° ë³€ìˆ˜ ì •ì˜\n",
        "# # ==============================================================================\n",
        "# import os\n",
        "# import subprocess\n",
        "# import json\n",
        "# import time\n",
        "# import threading\n",
        "# import requests\n",
        "# from google.colab import drive, files\n",
        "# from IPython.display import display, Image as IPyImage, clear_output\n",
        "# import logging\n",
        "\n",
        "# # --- ì‚¬ìš©ì ì„¤ì • ---\n",
        "# # Hugging Face Hub ë˜ëŠ” ë¡œì»¬ ê²½ë¡œì—ì„œ ë¶ˆëŸ¬ì˜¬ Diffusers ëª¨ë¸ ID\n",
        "# MODEL_ID = \"stabilityai/stable-diffusion-xl-base-1.0\"\n",
        "\n",
        "# # (ì„ íƒ ì‚¬í•­) ì´ˆê¸° ë¡œë“œí•  LoRA ê°€ì¤‘ì¹˜ íŒŒì¼ ê²½ë¡œ (Google Drive ë‚´)\n",
        "# # ë¹„ì›Œë‘ë©´ ì´ˆê¸° LoRA ì—†ì´ ì‹œì‘\n",
        "# INITIAL_LORA_WEIGHTS_PATH = \"\" # ì˜ˆ: \"/content/drive/MyDrive/Loras/YourLora/output/YourLora-000010.safetensors\"\n",
        "\n",
        "# # API ì„œë²„ í¬íŠ¸ ì„¤ì •\n",
        "# API_PORT = \"9080\"\n",
        "\n",
        "# # --- ë‚´ë¶€ ì„¤ì • ---\n",
        "# FASTAPI_APP_FILE = \"/content/main.py\"\n",
        "# REQUIREMENTS_PATH = \"/content/requirements_fastapi.txt\"\n",
        "# UVICORN_LOG_FILE = \"/content/uvicorn.log\"\n",
        "# is_google_colab = 'google.colab' in str(get_ipython())\n",
        "# cloudflared_process = None\n",
        "# cloudflared_url = None\n",
        "# uvicorn_process = None # Uvicorn í”„ë¡œì„¸ìŠ¤ ì¶”ì ìš©\n",
        "\n",
        "# # ë¡œê¹… ì„¤ì •\n",
        "# logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
        "# logger = logging.getLogger(__name__)\n",
        "\n",
        "# # ==============================================================================\n",
        "# # 1. Google Drive ë§ˆìš´íŠ¸ (LoRA ì‚¬ìš© ì‹œ)\n",
        "# # ==============================================================================\n",
        "# if is_google_colab:\n",
        "#     logger.info(\"ğŸ“‚ Google Drive ë§ˆìš´íŠ¸ ì¤‘...\")\n",
        "#     try:\n",
        "#         drive.mount('/content/drive', force_remount=True)\n",
        "#         logger.info(\"âœ… Google Drive ë§ˆìš´íŠ¸ ì™„ë£Œ.\")\n",
        "#         if INITIAL_LORA_WEIGHTS_PATH and not os.path.exists(INITIAL_LORA_WEIGHTS_PATH):\n",
        "#              logger.error(f\"ì´ˆê¸° LoRA íŒŒì¼ ê²½ë¡œ ì˜¤ë¥˜: {INITIAL_LORA_WEIGHTS_PATH}\")\n",
        "#         elif INITIAL_LORA_WEIGHTS_PATH:\n",
        "#             logger.info(f\"âœ… ì´ˆê¸° LoRA íŒŒì¼ í™•ì¸: {INITIAL_LORA_WEIGHTS_PATH}\")\n",
        "#     except Exception as e:\n",
        "#         logger.error(f\"Google Drive ë§ˆìš´íŠ¸ ì˜¤ë¥˜: {e}\")\n",
        "# else:\n",
        "#     logger.warning(\"Colab í™˜ê²½ ì•„ë‹˜. Drive ë§ˆìš´íŠ¸ ìƒëµ.\")\n",
        "\n",
        "# # ==============================================================================\n",
        "# # 2. í•„ìš” íŒ¨í‚¤ì§€ ì„¤ì¹˜ ë° Cloudflared\n",
        "# # ==============================================================================\n",
        "# logger.info(\"\\nâš™ï¸ í•„ìš”í•œ íŒ¨í‚¤ì§€ ì„¤ì¹˜ ë° Cloudflared ì¤€ë¹„ ì¤‘...\")\n",
        "\n",
        "# # requirements_fastapi.txt ìƒì„±\n",
        "# requirements_content = \"\"\"\n",
        "# fastapi\n",
        "# uvicorn[standard] # ASGI ì„œë²„ (standardëŠ” ì¶”ê°€ ê¸°ëŠ¥ í¬í•¨)\n",
        "# python-multipart # FastAPIì—ì„œ í¼ ë°ì´í„° ì²˜ë¦¬ ë“±ì— í•„ìš”í•  ìˆ˜ ìˆìŒ\n",
        "# requests # í…ŒìŠ¤íŠ¸ìš©\n",
        "\n",
        "# torch>=2.0.0\n",
        "# # torchvision # í•¸ë“¤ëŸ¬ ì§ì ‘ ì‚¬ìš© ì•ˆ í•¨\n",
        "# # torchaudio # í•¸ë“¤ëŸ¬ ì§ì ‘ ì‚¬ìš© ì•ˆ í•¨\n",
        "# diffusers>=0.24.0\n",
        "# transformers>=4.30.0\n",
        "# accelerate>=0.20.0\n",
        "# safetensors>=0.3.0\n",
        "# invisible-watermark>=0.2.0\n",
        "# pillow\n",
        "# python-dotenv # í™˜ê²½ë³€ìˆ˜ ê´€ë¦¬ (ì„ íƒì )\n",
        "# \"\"\"\n",
        "# with open(REQUIREMENTS_PATH, 'w') as f: f.write(requirements_content)\n",
        "# logger.info(f\"âœ… requirements_fastapi.txt ìƒì„± ì™„ë£Œ: {REQUIREMENTS_PATH}\")\n",
        "\n",
        "# # pip install\n",
        "# logger.info(f\"   - pip install ì‹¤í–‰: {REQUIREMENTS_PATH}\")\n",
        "# try:\n",
        "#     # ì´ì „ ì‹¤í–‰ì˜ uvicorn ë“± í”„ë¡œì„¸ìŠ¤ ì¢…ë£Œ ì‹œë„ (ì„ íƒì )\n",
        "#     # subprocess.run(['pkill', '-f', 'uvicorn'], capture_output=True)\n",
        "#     pip_process = subprocess.run(['pip', 'install', '-r', REQUIREMENTS_PATH, '-qq'],\n",
        "#                                  check=True, capture_output=True, text=True, encoding='utf-8')\n",
        "#     logger.info(\"âœ… Python íŒ¨í‚¤ì§€ ì„¤ì¹˜ ì™„ë£Œ.\")\n",
        "# except subprocess.CalledProcessError as e:\n",
        "#     logger.error(f\"ğŸ’¥ pip install ì‹¤íŒ¨! (ì¢…ë£Œ ì½”ë“œ: {e.returncode})\")\n",
        "#     print(\"--- pip stdout ---\\n\", e.stdout)\n",
        "#     print(\"--- pip stderr ---\\n\", e.stderr)\n",
        "#     raise e\n",
        "\n",
        "# # Cloudflared ì„¤ì¹˜\n",
        "# if is_google_colab:\n",
        "#     logger.info(\"   - Cloudflared ë‹¤ìš´ë¡œë“œ ë° ì„¤ì¹˜ ì¤‘...\")\n",
        "#     try:\n",
        "#         if os.path.exists(\"cloudflared-linux-amd64.deb\"): os.remove(\"cloudflared-linux-amd64.deb\")\n",
        "#         subprocess.run(['wget', '-q', 'https://github.com/cloudflare/cloudflared/releases/latest/download/cloudflared-linux-amd64.deb'], check=True)\n",
        "#         time.sleep(1)\n",
        "#         subprocess.run(['dpkg', '-i', 'cloudflared-linux-amd64.deb'], check=True, capture_output=True)\n",
        "#         logger.info(\"âœ… Cloudflared ì„¤ì¹˜ ì™„ë£Œ.\")\n",
        "#     except Exception as e: logger.error(f\"Cloudflared ì„¤ì¹˜ ì˜¤ë¥˜: {e}\")\n",
        "# else: logger.warning(\"Colab í™˜ê²½ ì•„ë‹˜. Cloudflared ì„¤ì¹˜ ìƒëµ.\")\n",
        "\n",
        "# # ==============================================================================\n",
        "# # 3. FastAPI ì•± ìŠ¤í¬ë¦½íŠ¸ ìƒì„± (main.py)\n",
        "# # ==============================================================================\n",
        "# logger.info(f\"\\nğŸ“ FastAPI ì•± ìŠ¤í¬ë¦½íŠ¸ ìƒì„± ì¤‘ ({FASTAPI_APP_FILE})...\")\n",
        "\n",
        "# fastapi_app_content = '''\n",
        "# import logging\n",
        "# import os\n",
        "# import base64\n",
        "# import io\n",
        "# import time\n",
        "# import torch\n",
        "# from diffusers import DiffusionPipeline, DPMSolverMultistepScheduler\n",
        "# from fastapi import FastAPI, HTTPException, Request\n",
        "# from pydantic import BaseModel, Field\n",
        "# from PIL import Image\n",
        "# import asyncio # ë¹„ë™ê¸° Lock ë° ìŠ¤ë ˆë“œ ì‹¤í–‰ìš©\n",
        "# from contextlib import asynccontextmanager\n",
        "# from fastapi import FastAPI\n",
        "# from fastapi.responses import JSONResponse, Response\n",
        "\n",
        "# # --- ê¸°ë³¸ ì„¤ì • ---\n",
        "# # í™˜ê²½ ë³€ìˆ˜ ë˜ëŠ” ê¸°ë³¸ê°’ ì‚¬ìš© (ìŠ¤í¬ë¦½íŠ¸ ì‹œì‘ ì‹œ ì„¤ì •ëœ ê°’ í™œìš©)\n",
        "# MODEL_ID = os.environ.get(\"TS_MODEL_ID\", \"stabilityai/stable-diffusion-xl-base-1.0\")\n",
        "# INITIAL_LORA_PATH = os.environ.get(\"TS_INITIAL_LORA_PATH\", None)\n",
        "# ADAPTER_NAME = \"default\" # ì‚¬ìš©í•  LoRA ì–´ëŒ‘í„° ì´ë¦„\n",
        "\n",
        "# # ë¡œê¹… ì„¤ì •\n",
        "# logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
        "# logger = logging.getLogger(\"uvicorn\") # Uvicorn ë¡œê±° ì‚¬ìš© ê¶Œì¥\n",
        "\n",
        "# # --- ëª¨ë¸ ë° ìƒíƒœ ê´€ë¦¬ ---\n",
        "# # ì „ì—­ ë³€ìˆ˜ ëŒ€ì‹  context manager ë˜ëŠ” í´ë˜ìŠ¤ ì‚¬ìš© ê¶Œì¥ë˜ë‚˜, ì—¬ê¸°ì„œëŠ” ê°„ë‹¨í•˜ê²Œ ì „ì—­ ì‚¬ìš©\n",
        "# pipeline = None\n",
        "# current_lora_path = None\n",
        "# model_lock = asyncio.Lock() # íŒŒì´í”„ë¼ì¸ ì ‘ê·¼ ì œì–´ìš© Lock\n",
        "\n",
        "# # --- FastAPI ì•± ìƒëª…ì£¼ê¸° ê´€ë¦¬ (ëª¨ë¸ ë¡œë”©/ì–¸ë¡œë”©) ---\n",
        "# @asynccontextmanager\n",
        "# async def lifespan(app: FastAPI):\n",
        "#     global pipeline, current_lora_path, model_lock\n",
        "#     logger.info(\"ì• í”Œë¦¬ì¼€ì´ì…˜ ì‹œì‘: ëª¨ë¸ ë¡œë”© ì‹œì‘...\")\n",
        "#     start_time = time.time()\n",
        "#     try:\n",
        "#         if torch.cuda.is_available():\n",
        "#             device = \"cuda\"\n",
        "#             torch_dtype = torch.float16\n",
        "#             logger.info(f\"ì‚¬ìš© ê°€ëŠ¥ GPU ê°ì§€ë¨. Device: {device}, Dtype: {torch_dtype}\")\n",
        "#         else:\n",
        "#             device = \"cpu\"\n",
        "#             torch_dtype = torch.float32 # CPUì—ì„œëŠ” float32 ì‚¬ìš©\n",
        "#             logger.info(f\"GPU ì‚¬ìš© ë¶ˆê°€. Device: {device}, Dtype: {torch_dtype}\")\n",
        "\n",
        "#         if torch.cuda.is_available(): logger.info(f\"ë©”ëª¨ë¦¬ (ë¡œë”© ì „): í• ë‹¹ë¨ {torch.cuda.memory_allocated(device)/1e9:.2f} GB, ì˜ˆì•½ë¨ {torch.cuda.memory_reserved(device)/1e9:.2f} GB\")\n",
        "\n",
        "#         temp_pipeline = DiffusionPipeline.from_pretrained(\n",
        "#             MODEL_ID,\n",
        "#             torch_dtype=torch_dtype,\n",
        "#             variant=\"fp16\" if torch_dtype == torch.float16 else None, # GPU ì‚¬ìš© ì‹œ FP16 ë³€í˜• ì‚¬ìš©\n",
        "#             use_safetensors=True\n",
        "#         )\n",
        "#         temp_pipeline.scheduler = DPMSolverMultistepScheduler.from_config(temp_pipeline.scheduler.config, use_karras_sigmas=True)\n",
        "#         temp_pipeline.to(device)\n",
        "#         logger.info(f\"ëª¨ë¸ íŒŒì´í”„ë¼ì¸ ë¡œë”© ì™„ë£Œ. Device: {temp_pipeline.device}\")\n",
        "\n",
        "#         if torch.cuda.is_available(): logger.info(f\"ë©”ëª¨ë¦¬ (íŒŒì´í”„ë¼ì¸ ë¡œë”© í›„): í• ë‹¹ë¨ {torch.cuda.memory_allocated(device)/1e9:.2f} GB, ì˜ˆì•½ë¨ {torch.cuda.memory_reserved(device)/1e9:.2f} GB\")\n",
        "\n",
        "#         pipeline = temp_pipeline # ì „ì—­ ë³€ìˆ˜ì— í• ë‹¹\n",
        "\n",
        "#         # ì´ˆê¸° LoRA ë¡œë“œ\n",
        "#         if INITIAL_LORA_PATH and os.path.exists(INITIAL_LORA_PATH):\n",
        "#             logger.info(f\"ì´ˆê¸° LoRA ë¡œë”© ì‹œë„: {INITIAL_LORA_PATH}\")\n",
        "#             try:\n",
        "#                 async with model_lock: # Lock ì‚¬ìš©\n",
        "#                     # ë¹„ë™ê¸° í•¨ìˆ˜ ë‚´ì—ì„œ ë™ê¸° í•¨ìˆ˜ ì‹¤í–‰ (ëª¨ë¸ ë¡œë”©ì€ ë¸”ë¡œí‚¹ ì‘ì—…)\n",
        "#                     await asyncio.to_thread(load_lora_blocking, INITIAL_LORA_PATH)\n",
        "#                 logger.info(f\"ì´ˆê¸° LoRA ë¡œë”© ì„±ê³µ: {INITIAL_LORA_PATH}\")\n",
        "#                 current_lora_path = INITIAL_LORA_PATH\n",
        "#                 if torch.cuda.is_available(): logger.info(f\"ë©”ëª¨ë¦¬ (LoRA ë¡œë”© í›„): í• ë‹¹ë¨ {torch.cuda.memory_allocated(device)/1e9:.2f} GB, ì˜ˆì•½ë¨ {torch.cuda.memory_reserved(device)/1e9:.2f} GB\")\n",
        "#             except Exception as e:\n",
        "#                  logger.error(f\"ì´ˆê¸° LoRA ë¡œë”© ì‹¤íŒ¨: {e}\", exc_info=True)\n",
        "#         elif INITIAL_LORA_PATH:\n",
        "#              logger.warning(f\"ì´ˆê¸° LoRA íŒŒì¼({INITIAL_LORA_PATH}) ì—†ìŒ.\")\n",
        "\n",
        "#         loading_time = time.time() - start_time\n",
        "#         logger.info(f\"ëª¨ë¸ ë° ì´ˆê¸° LoRA ë¡œë”© ì™„ë£Œ. (ì†Œìš” ì‹œê°„: {loading_time:.2f}ì´ˆ)\")\n",
        "\n",
        "#     except Exception as e:\n",
        "#         logger.error(f\"ëª¨ë¸ ë¡œë”© ì¤‘ ì¹˜ëª…ì  ì˜¤ë¥˜ ë°œìƒ: {e}\", exc_info=True)\n",
        "#         pipeline = None # ë¡œë”© ì‹¤íŒ¨ ì‹œ None ìœ ì§€\n",
        "\n",
        "#     yield # ì• í”Œë¦¬ì¼€ì´ì…˜ ì‹¤í–‰ êµ¬ê°„\n",
        "\n",
        "#     # --- ì• í”Œë¦¬ì¼€ì´ì…˜ ì¢…ë£Œ ì‹œ ---\n",
        "#     logger.info(\"ì• í”Œë¦¬ì¼€ì´ì…˜ ì¢…ë£Œ: ëª¨ë¸ ì •ë¦¬...\")\n",
        "#     async with model_lock: # Lock ì‚¬ìš©\n",
        "#         pipeline = None\n",
        "#         current_lora_path = None\n",
        "#     if torch.cuda.is_available():\n",
        "#         torch.cuda.empty_cache()\n",
        "#         logger.info(\"GPU ìºì‹œ ì •ë¦¬ ì™„ë£Œ.\")\n",
        "#     logger.info(\"ì• í”Œë¦¬ì¼€ì´ì…˜ ì¢…ë£Œ ì™„ë£Œ.\")\n",
        "\n",
        "\n",
        "# app = FastAPI(lifespan=lifespan)\n",
        "\n",
        "# # --- Pydantic ëª¨ë¸ ì •ì˜ ---\n",
        "# class InferenceRequest(BaseModel):\n",
        "#     prompt: str\n",
        "#     negative_prompt: str = \"\"\n",
        "#     height: int = 1024\n",
        "#     width: int = 1024\n",
        "#     steps: int = Field(30, gt=0, le=100) # 1 ~ 100 ì‚¬ì´ ê°’\n",
        "#     cfg_scale: float = Field(7.5, gt=0.0, le=20.0) # 0 ì´ˆê³¼ ~ 20 ì´í•˜ ê°’\n",
        "#     seed: int | None = None\n",
        "\n",
        "# class InferenceResponse(BaseModel):\n",
        "#     image_base64: str\n",
        "#     generation_time_ms: int\n",
        "#     model_id: str\n",
        "#     current_lora: str | None\n",
        "\n",
        "# class LoraRequest(BaseModel):\n",
        "#     lora_path: str # Google Drive ë‚´ ì ˆëŒ€ ê²½ë¡œ ë“±\n",
        "\n",
        "# class StatusResponse(BaseModel):\n",
        "#     status: str\n",
        "#     model_id: str | None\n",
        "#     device: str | None\n",
        "#     current_lora_path: str | None\n",
        "#     active_adapters: list | None # í™œì„±í™”ëœ ì–´ëŒ‘í„° ëª©ë¡ (ë””ë²„ê¹…ìš©)\n",
        "\n",
        "# # --- Helper Functions (Blocking tasks) ---\n",
        "# # ë™ê¸° í•¨ìˆ˜ë“¤ì€ asyncio.to_thread ë¡œ í˜¸ì¶œí•´ì•¼ í•¨\n",
        "\n",
        "# def load_lora_blocking(lora_path: str):\n",
        "#     global pipeline, current_lora_path # ì „ì—­ ë³€ìˆ˜ ì‚¬ìš© ì‹œ ëª…ì‹œ\n",
        "#     if not pipeline: raise RuntimeError(\"íŒŒì´í”„ë¼ì¸ì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\")\n",
        "#     if not os.path.exists(lora_path): raise FileNotFoundError(f\"LoRA íŒŒì¼ ì—†ìŒ: {lora_path}\")\n",
        "\n",
        "#     logger.info(f\"LoRA ë¡œë”© (ë™ê¸°): {lora_path}\")\n",
        "#     pipeline.to(pipeline.device) # ì¥ì¹˜ í™•ì¸\n",
        "\n",
        "#     # ê¸°ì¡´ ì–´ëŒ‘í„° ì–¸ë¡œë“œ ì‹œë„\n",
        "#     try:\n",
        "#         active_adapters = getattr(pipeline, \"get_active_adapters\", lambda: [])()\n",
        "#         if ADAPTER_NAME in active_adapters:\n",
        "#             logger.info(f\"ê¸°ì¡´ '{ADAPTER_NAME}' ì–¸ë¡œë“œ ì‹œë„...\")\n",
        "#             if hasattr(pipeline, 'unload_lora_weights'): pipeline.unload_lora_weights(adapter_names=[ADAPTER_NAME])\n",
        "#             elif hasattr(pipeline, 'delete_adapters'): pipeline.delete_adapters(ADAPTER_NAME)\n",
        "#     except Exception as e: logger.warning(f\"ê¸°ì¡´ LoRA ì–¸ë¡œë“œ ì¤‘ ê²½ê³ (ë¬´ì‹œ): {e}\")\n",
        "\n",
        "#     logger.info(f\"ìƒˆ LoRA ë¡œë”©: '{lora_path}' ('{ADAPTER_NAME}')...\")\n",
        "#     pipeline.load_lora_weights(os.path.dirname(lora_path), weight_name=os.path.basename(lora_path), adapter_name=ADAPTER_NAME)\n",
        "#     logger.info(f\"LoRA ë¡œë”© ì„±ê³µ: {lora_path}\")\n",
        "\n",
        "#     if hasattr(pipeline, 'set_adapters'): pipeline.set_adapters([ADAPTER_NAME], adapter_weights=[1.0])\n",
        "#     elif hasattr(pipeline, 'fuse_lora'): pipeline.fuse_lora(adapter_names=[ADAPTER_NAME])\n",
        "\n",
        "#     current_lora_path = lora_path # ìƒíƒœ ì—…ë°ì´íŠ¸\n",
        "#     if torch.cuda.is_available(): torch.cuda.empty_cache()\n",
        "\n",
        "# def unload_lora_blocking():\n",
        "#     global pipeline, current_lora_path\n",
        "#     if not pipeline: raise RuntimeError(\"íŒŒì´í”„ë¼ì¸ì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\")\n",
        "#     if not current_lora_path: return \"í˜„ì¬ ë¡œë“œëœ LoRA ì—†ìŒ.\"\n",
        "\n",
        "#     logger.info(f\"LoRA ì–¸ë¡œë“œ (ë™ê¸°): {current_lora_path}\")\n",
        "#     pipeline.to(pipeline.device)\n",
        "#     unloaded_path = current_lora_path\n",
        "#     try:\n",
        "#         if hasattr(pipeline, 'unload_lora_weights'): pipeline.unload_lora_weights(adapter_names=[ADAPTER_NAME])\n",
        "#         elif hasattr(pipeline, 'delete_adapters'): pipeline.delete_adapters(ADAPTER_NAME)\n",
        "#         else:\n",
        "#              if hasattr(pipeline, 'set_adapters'): pipeline.set_adapters([])\n",
        "#              elif hasattr(pipeline, 'unfuse_lora'): pipeline.unfuse_lora()\n",
        "#         logger.info(f\"ì„±ê³µì ìœ¼ë¡œ LoRA ì–¸ë¡œë“œ/ë¹„í™œì„±í™”: {unloaded_path}\")\n",
        "#         current_lora_path = None\n",
        "#         if torch.cuda.is_available(): torch.cuda.empty_cache()\n",
        "#         return f\"ì„±ê³µì ìœ¼ë¡œ LoRA ì–¸ë¡œë“œ: {unloaded_path}\"\n",
        "#     except Exception as e:\n",
        "#          logger.error(f\"LoRA ì–¸ë¡œë“œ ì¤‘ ì˜¤ë¥˜: {e}\", exc_info=True)\n",
        "#          current_lora_path = None # ì˜¤ë¥˜ ì‹œì—ë„ ìƒíƒœëŠ” ì´ˆê¸°í™”\n",
        "#          raise RuntimeError(f\"LoRA ì–¸ë¡œë“œ ì‹¤íŒ¨: {e}\") from e\n",
        "\n",
        "# def generate_image_blocking(req: InferenceRequest) -> Image.Image:\n",
        "#     global pipeline\n",
        "#     if not pipeline: raise RuntimeError(\"íŒŒì´í”„ë¼ì¸ì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\")\n",
        "\n",
        "#     logger.info(f\"ì´ë¯¸ì§€ ìƒì„± ì‹œì‘ (ë™ê¸°): prompt='{req.prompt[:50]}...'\")\n",
        "#     params = {\n",
        "#         \"negative_prompt\": req.negative_prompt,\n",
        "#         \"num_inference_steps\": req.steps,\n",
        "#         \"guidance_scale\": req.cfg_scale,\n",
        "#         \"height\": req.height,\n",
        "#         \"width\": req.width,\n",
        "#     }\n",
        "#     if req.seed is not None:\n",
        "#         params[\"generator\"] = torch.Generator(device=pipeline.device).manual_seed(req.seed)\n",
        "\n",
        "#     with torch.inference_mode():\n",
        "#         result_image = pipeline(prompt=req.prompt, **params).images[0]\n",
        "#     logger.info(\"ì´ë¯¸ì§€ ìƒì„± ì™„ë£Œ (ë™ê¸°)\")\n",
        "#     return result_image\n",
        "\n",
        "\n",
        "# # --- API ì—”ë“œí¬ì¸íŠ¸ ---\n",
        "# @app.get(\"/\", include_in_schema=False)\n",
        "# async def root_health():\n",
        "#     # ë‹¨ìˆœíˆ 200 OK ë¦¬í„´\n",
        "#     return JSONResponse({\"status\": \"ok\"}, status_code=200)\n",
        "\n",
        "# @app.get(\"/favicon.ico\", include_in_schema=False)\n",
        "# async def favicon():\n",
        "#     return Response(status_code=204)\n",
        "\n",
        "# @app.get(\"/status\", response_model=StatusResponse)\n",
        "# async def get_status():\n",
        "#     \"\"\"í˜„ì¬ ì„œë²„ ìƒíƒœ (ë¡œë“œëœ ëª¨ë¸, LoRA ë“±) ë°˜í™˜\"\"\"\n",
        "#     async with model_lock: # ìƒíƒœ ì½ê¸° ì‹œì—ë„ Lock ì‚¬ìš© (ì¼ê´€ì„±)\n",
        "#         if not pipeline:\n",
        "#             return StatusResponse(status=\"error\", model_id=MODEL_ID, device=None, current_lora_path=None, active_adapters=None)\n",
        "\n",
        "#         active_adapters = []\n",
        "#         try:\n",
        "#             if hasattr(pipeline, 'get_active_adapters'): active_adapters = pipeline.get_active_adapters()\n",
        "#             elif hasattr(pipeline, 'get_list_adapters'): active_adapters = [name for name, enabled in pipeline.get_list_adapters().items() if enabled]\n",
        "#         except Exception as e: logger.warning(f\"í™œì„± ì–´ëŒ‘í„° ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨: {e}\")\n",
        "\n",
        "#         return StatusResponse(\n",
        "#             status=\"ready\" if pipeline else \"initializing_error\",\n",
        "#             model_id=MODEL_ID,\n",
        "#             device=str(pipeline.device) if pipeline else None,\n",
        "#             current_lora_path=current_lora_path,\n",
        "#             active_adapters=active_adapters\n",
        "#         )\n",
        "\n",
        "# @app.post(\"/predictions\", response_model=InferenceResponse)\n",
        "# async def predict(request: InferenceRequest):\n",
        "#     \"\"\"ì´ë¯¸ì§€ ìƒì„± ìš”ì²­ ì²˜ë¦¬\"\"\"\n",
        "#     global pipeline\n",
        "#     if not pipeline:\n",
        "#         raise HTTPException(status_code=503, detail=\"ëª¨ë¸ íŒŒì´í”„ë¼ì¸ì´ ì¤€ë¹„ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\")\n",
        "\n",
        "#     start_time = time.time()\n",
        "#     try:\n",
        "#         # ë¸”ë¡œí‚¹ I/O ë˜ëŠ” CPU/GPU ë°”ìš´ë“œ ì‘ì—…ì„ ë³„ë„ ìŠ¤ë ˆë“œì—ì„œ ì‹¤í–‰\n",
        "#         pil_image = await asyncio.to_thread(generate_image_blocking, request)\n",
        "\n",
        "#         # PIL Image -> Base64\n",
        "#         buffered = io.BytesIO()\n",
        "#         pil_image.save(buffered, format=\"PNG\")\n",
        "#         img_str = base64.b64encode(buffered.getvalue()).decode(\"utf-8\")\n",
        "\n",
        "#         end_time = time.time()\n",
        "#         generation_time_ms = int((end_time - start_time) * 1000)\n",
        "\n",
        "#         # ì‘ë‹µ ìƒì„± ì „ì— í˜„ì¬ LoRA ìƒíƒœ í™•ì¸ (Lock ë‚´ë¶€ì—ì„œ ë³€ê²½ë  ìˆ˜ ìˆìœ¼ë¯€ë¡œ ë‹¤ì‹œ í™•ì¸)\n",
        "#         async with model_lock:\n",
        "#             lora_in_use = current_lora_path\n",
        "\n",
        "#         return InferenceResponse(\n",
        "#             image_base64=img_str,\n",
        "#             generation_time_ms=generation_time_ms,\n",
        "#             model_id=MODEL_ID,\n",
        "#             current_lora=lora_in_use\n",
        "#         )\n",
        "#     except Exception as e:\n",
        "#         logger.error(f\"ì¶”ë¡  ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}\", exc_info=True)\n",
        "#         raise HTTPException(status_code=500, detail=f\"ì¶”ë¡  ì‹¤íŒ¨: {str(e)}\")\n",
        "\n",
        "# @app.post(\"/load-lora\", status_code=200)\n",
        "# async def load_lora(request: LoraRequest):\n",
        "#     \"\"\"ì§€ì •ëœ ê²½ë¡œì˜ LoRA ê°€ì¤‘ì¹˜ë¥¼ ë¡œë“œ\"\"\"\n",
        "#     global pipeline\n",
        "#     if not pipeline:\n",
        "#         raise HTTPException(status_code=503, detail=\"ëª¨ë¸ íŒŒì´í”„ë¼ì¸ì´ ì¤€ë¹„ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\")\n",
        "\n",
        "#     async with model_lock: # Lockì„ ì‚¬ìš©í•˜ì—¬ ë™ì‹œ ì ‘ê·¼ ë°©ì§€\n",
        "#         try:\n",
        "#             logger.info(f\"LoRA ë¡œë“œ ìš”ì²­ ìˆ˜ì‹ : {request.lora_path}\")\n",
        "#             # ë¸”ë¡œí‚¹ ì‘ì—…ì„ ë³„ë„ ìŠ¤ë ˆë“œì—ì„œ ì‹¤í–‰\n",
        "#             message = await asyncio.to_thread(load_lora_blocking, request.lora_path)\n",
        "#             logger.info(f\"LoRA ë¡œë“œ ìš”ì²­ ì²˜ë¦¬ ì™„ë£Œ.\")\n",
        "#             return {\"status\": \"success\", \"message\": message}\n",
        "#         except FileNotFoundError as e:\n",
        "#             logger.error(f\"LoRA íŒŒì¼ ì—†ìŒ ì˜¤ë¥˜: {e}\")\n",
        "#             raise HTTPException(status_code=404, detail=str(e))\n",
        "#         except Exception as e:\n",
        "#             logger.error(f\"LoRA ë¡œë“œ ì¤‘ ì˜¤ë¥˜: {e}\", exc_info=True)\n",
        "#             raise HTTPException(status_code=500, detail=f\"LoRA ë¡œë“œ ì‹¤íŒ¨: {str(e)}\")\n",
        "\n",
        "# @app.post(\"/unload-lora\", status_code=200)\n",
        "# async def unload_lora():\n",
        "#     \"\"\"í˜„ì¬ ë¡œë“œëœ LoRA ê°€ì¤‘ì¹˜ë¥¼ ì–¸ë¡œë“œ\"\"\"\n",
        "#     global pipeline\n",
        "#     if not pipeline:\n",
        "#         raise HTTPException(status_code=503, detail=\"ëª¨ë¸ íŒŒì´í”„ë¼ì¸ì´ ì¤€ë¹„ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\")\n",
        "\n",
        "#     async with model_lock: # Lock ì‚¬ìš©\n",
        "#         try:\n",
        "#             logger.info(f\"LoRA ì–¸ë¡œë“œ ìš”ì²­ ìˆ˜ì‹ .\")\n",
        "#             # ë¸”ë¡œí‚¹ ì‘ì—…ì„ ë³„ë„ ìŠ¤ë ˆë“œì—ì„œ ì‹¤í–‰\n",
        "#             message = await asyncio.to_thread(unload_lora_blocking)\n",
        "#             logger.info(f\"LoRA ì–¸ë¡œë“œ ìš”ì²­ ì²˜ë¦¬ ì™„ë£Œ.\")\n",
        "#             return {\"status\": \"success\", \"message\": message}\n",
        "#         except Exception as e:\n",
        "#             logger.error(f\"LoRA ì–¸ë¡œë“œ ì¤‘ ì˜¤ë¥˜: {e}\", exc_info=True)\n",
        "#             raise HTTPException(status_code=500, detail=f\"LoRA ì–¸ë¡œë“œ ì‹¤íŒ¨: {str(e)}\")\n",
        "\n",
        "# # --- ì„œë²„ ì‹¤í–‰ (Colab ìŠ¤í¬ë¦½íŠ¸ì—ì„œ uvicornìœ¼ë¡œ ì‹¤í–‰) ---\n",
        "# # if __name__ == \"__main__\":\n",
        "# #     import uvicorn\n",
        "# #     # í™˜ê²½ ë³€ìˆ˜ì—ì„œ í¬íŠ¸ ì½ê¸° ì‹œë„ (Colabì—ì„œëŠ” ìŠ¤í¬ë¦½íŠ¸ì—ì„œ ì§ì ‘ ì§€ì •)\n",
        "# #     port = int(os.environ.get(\"PORT\", 9080))\n",
        "# #     uvicorn.run(app, host=\"0.0.0.0\", port=port)\n",
        "# '''\n",
        "# with open(FASTAPI_APP_FILE, 'w', encoding='utf-8') as f: f.write(fastapi_app_content)\n",
        "# logger.info(f\"âœ… FastAPI ì•± ìŠ¤í¬ë¦½íŠ¸ ì €ì¥ ì™„ë£Œ: {FASTAPI_APP_FILE}\")\n",
        "\n",
        "# # ==============================================================================\n",
        "# # 4. FastAPI ì„œë²„ ì‹œì‘ (Uvicorn ì‚¬ìš©)\n",
        "# # ==============================================================================\n",
        "# logger.info(\"\\nğŸš€ FastAPI ì„œë²„ ì‹œì‘ ì¤‘ (Uvicorn)...\")\n",
        "\n",
        "# # í•¸ë“¤ëŸ¬ ë‚´ë¶€ì—ì„œ ì‚¬ìš©í•  í™˜ê²½ ë³€ìˆ˜ ì„¤ì • (FastAPI ì•± ì‹œì‘ ì „ì— ì„¤ì • í•„ìš”)\n",
        "# os.environ['TS_MODEL_ID'] = MODEL_ID\n",
        "# if INITIAL_LORA_WEIGHTS_PATH:\n",
        "#     os.environ['TS_INITIAL_LORA_PATH'] = INITIAL_LORA_WEIGHTS_PATH\n",
        "# else:\n",
        "#     if 'TS_INITIAL_LORA_PATH' in os.environ: del os.environ['TS_INITIAL_LORA_PATH']\n",
        "\n",
        "# # Uvicorn ì‹¤í–‰ ëª…ë ¹ì–´ (ë°±ê·¸ë¼ìš´ë“œ, ë¡œê·¸ íŒŒì¼ ì‚¬ìš©)\n",
        "# # --reload ì˜µì…˜ì€ Colabì—ì„œ íŒŒì¼ ë³€ê²½ ê°ì§€ê°€ ì–´ë ¤ì›Œ ë¹„ì¶”ì²œ\n",
        "# uvicorn_cmd = [\n",
        "#     \"uvicorn\",\n",
        "#     \"main:app\", # FastAPI ì•± ê°ì²´ ìœ„ì¹˜ (main.py íŒŒì¼ì˜ app ê°ì²´)\n",
        "#     \"--host\", \"0.0.0.0\",\n",
        "#     \"--port\", API_PORT,\n",
        "#     \"--workers\", \"1\" # ë©€í‹° ì›Œì»¤ëŠ” ëª¨ë¸ ë¡œë”©/ìƒíƒœ ê³µìœ  ë¬¸ì œë¡œ 1ê°œ ê¶Œì¥\n",
        "# ]\n",
        "# logger.info(f\"ì‹¤í–‰í•  Uvicorn ëª…ë ¹ì–´: {' '.join(uvicorn_cmd)}\")\n",
        "# logger.info(f\"Uvicorn ë¡œê·¸ëŠ” {UVICORN_LOG_FILE} íŒŒì¼ì— ì €ì¥ë©ë‹ˆë‹¤.\")\n",
        "\n",
        "# # ê¸°ì¡´ uvicorn í”„ë¡œì„¸ìŠ¤ ì¢…ë£Œ ì‹œë„ (ì„ íƒì )\n",
        "# subprocess.run(['pkill', '-f', 'uvicorn'], capture_output=True)\n",
        "# time.sleep(3)\n",
        "\n",
        "# # nohup ê³¼ & ë¥¼ ì‚¬ìš©í•˜ì—¬ ë°±ê·¸ë¼ìš´ë“œ ì‹¤í–‰ ë° ë¡œê·¸ ë¦¬ë””ë ‰ì…˜\n",
        "# nohup_cmd = f\"nohup {' '.join(uvicorn_cmd)} > {UVICORN_LOG_FILE} 2>&1 &\"\n",
        "# logger.info(f\"ë°±ê·¸ë¼ìš´ë“œ ì‹¤í–‰ ëª…ë ¹ì–´: {nohup_cmd}\")\n",
        "# # shell=True ì‚¬ìš©ì— ì£¼ì˜ í•„ìš”í•˜ë‚˜, nohup/& ì‚¬ìš© ì‹œ ë¶ˆê°€í”¼\n",
        "# process = subprocess.Popen(nohup_cmd, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
        "# # Popen ìì²´ëŠ” ì¦‰ì‹œ ë°˜í™˜ë¨. ì‹¤ì œ ì„œë²„ ì‹œì‘ì€ ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì§„í–‰.\n",
        "# logger.info(\"   - Uvicorn ì„œë²„ ì‹œì‘ ëª…ë ¹ì–´ ì‹¤í–‰ë¨ (ë°±ê·¸ë¼ìš´ë“œ).\")\n",
        "\n",
        "# # ì„œë²„ ì‹œì‘ ëŒ€ê¸° (ëª¨ë¸ ë¡œë”© ì‹œê°„ ê³ ë ¤)\n",
        "# wait_seconds = 80 # SDXL ëª¨ë¸ ë¡œë”© ì‹œê°„ ê³ ë ¤ (ì¶©ë¶„íˆ ê¸¸ê²Œ)\n",
        "# logger.info(f\"   - FastAPI ì„œë²„ ì´ˆê¸°í™” ë° ëª¨ë¸ ë¡œë”© ëŒ€ê¸° ì¤‘ ({wait_seconds}ì´ˆ)...\")\n",
        "# time.sleep(wait_seconds)\n",
        "\n",
        "# # Uvicorn ë¡œê·¸ íŒŒì¼ ëë¶€ë¶„ í™•ì¸í•˜ì—¬ ì‹œì‘ ì—¬ë¶€ ì¶”ì •\n",
        "# log_check_success = False\n",
        "# try:\n",
        "#     if os.path.exists(UVICORN_LOG_FILE):\n",
        "#         with open(UVICORN_LOG_FILE, 'r') as f:\n",
        "#             lines = f.readlines()\n",
        "#             tail_lines = lines[-20:] # ë§ˆì§€ë§‰ 20ì¤„ í™•ì¸\n",
        "#             print(\"\\n--- Uvicorn ë¡œê·¸ (ë§ˆì§€ë§‰ 20ì¤„) ---\")\n",
        "#             for line in tail_lines:\n",
        "#                 print(line.strip())\n",
        "#                 # Uvicorn ì‹œì‘ ì„±ê³µ ë©”ì‹œì§€ í™•ì¸ (ë²„ì „ë§ˆë‹¤ ë‹¤ë¥¼ ìˆ˜ ìˆìŒ)\n",
        "#                 if f\"Uvicorn running on http://0.0.0.0:{API_PORT}\" in line:\n",
        "#                     logger.info(\"âœ… Uvicorn ë¡œê·¸ì—ì„œ ì„œë²„ ì‹œì‘ ë©”ì‹œì§€ í™•ì¸ë¨.\")\n",
        "#                     log_check_success = True\n",
        "#             if not log_check_success and lines:\n",
        "#                 logger.warning(\"âš ï¸ Uvicorn ë¡œê·¸ì—ì„œ ëª…í™•í•œ ì‹œì‘ ë©”ì‹œì§€ë¥¼ ì°¾ì§€ ëª»í•¨. ìƒíƒœ ì—”ë“œí¬ì¸íŠ¸ë¡œ í™•ì¸ í•„ìš”.\")\n",
        "#             elif not lines:\n",
        "#                  logger.warning(\"âš ï¸ Uvicorn ë¡œê·¸ íŒŒì¼ì´ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤.\")\n",
        "#     else:\n",
        "#         logger.warning(f\"âš ï¸ Uvicorn ë¡œê·¸ íŒŒì¼({UVICORN_LOG_FILE})ì´ ìƒì„±ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\")\n",
        "# except Exception as e:\n",
        "#     logger.error(f\"Uvicorn ë¡œê·¸ í™•ì¸ ì¤‘ ì˜¤ë¥˜: {e}\")\n",
        "\n",
        "# # ìµœì¢… í™•ì¸: ê°„ë‹¨í•œ ìƒíƒœ ìš”ì²­ ë³´ë‚´ë³´ê¸°\n",
        "# if log_check_success: # ë¡œê·¸ì—ì„œ ì‹œì‘ í™•ì¸ ì‹œì—ë§Œ ì‹œë„\n",
        "#     logger.info(\"   - ì„œë²„ ìƒíƒœ í™•ì¸ ì‹œë„ (GET /status)...\")\n",
        "#     try:\n",
        "#         status_url = f\"http://127.0.0.1:{API_PORT}/status\"\n",
        "#         response = requests.get(status_url, timeout=30)\n",
        "#         response.raise_for_status()\n",
        "#         status_data = response.json()\n",
        "#         if status_data.get(\"status\") == \"ready\":\n",
        "#             logger.info(\"âœ… FastAPI ì„œë²„ ìƒíƒœ 'ready' í™•ì¸!\")\n",
        "#         else:\n",
        "#             logger.warning(f\"âš ï¸ FastAPI ì„œë²„ ìƒíƒœê°€ 'ready'ê°€ ì•„ë‹˜: {status_data}\")\n",
        "#     except Exception as e:\n",
        "#         logger.error(f\"âŒ FastAPI ì„œë²„ ìƒíƒœ í™•ì¸ ì‹¤íŒ¨: {e}\")\n",
        "#         logger.error(\"   - ì„œë²„ê°€ ì™„ì „íˆ ì‹œì‘ë˜ì§€ ì•Šì•˜ê±°ë‚˜ ë¬¸ì œê°€ ë°œìƒí–ˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\")\n",
        "#         logger.error(f\"   - Uvicorn ë¡œê·¸ íŒŒì¼({UVICORN_LOG_FILE})ì„ í™•ì¸í•˜ì„¸ìš”.\")\n",
        "#         # ë¬¸ì œê°€ ì‹¬ê°í•˜ë©´ ì—¬ê¸°ì„œ ì¤‘ë‹¨\n",
        "#         raise RuntimeError(\"FastAPI ì„œë²„ ì‹œì‘ ë˜ëŠ” ìƒíƒœ í™•ì¸ ì‹¤íŒ¨\")\n",
        "# else:\n",
        "#      logger.error(\"âŒ Uvicorn ë¡œê·¸ì—ì„œ ì„œë²„ ì‹œì‘ì„ í™•ì¸í•  ìˆ˜ ì—†ì–´ ì¤‘ë‹¨í•©ë‹ˆë‹¤.\")\n",
        "#      raise RuntimeError(\"FastAPI ì„œë²„ ì‹œì‘ í™•ì¸ ì‹¤íŒ¨\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U7ceKaBT7WLI",
        "outputId": "15b2f020-cff2-4962-9be1-414e2e41fbea"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "\n",
            "--- Uvicorn ë¡œê·¸ (ë§ˆì§€ë§‰ 20ì¤„) ---\n",
            "E0000 00:00:1744958119.814446   31475 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1744958119.820939   31475 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-04-18 06:35:19.842455: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "INFO:     Started server process [31475]\n",
            "INFO:     Waiting for application startup.\n",
            "INFO:     ì• í”Œë¦¬ì¼€ì´ì…˜ ì‹œì‘: ëª¨ë¸ ë¡œë”© ì‹œì‘...\n",
            "INFO:     ì‚¬ìš© ê°€ëŠ¥ GPU ê°ì§€ë¨. Device: cuda, Dtype: torch.float16\n",
            "INFO:     ë©”ëª¨ë¦¬ (ë¡œë”© ì „): í• ë‹¹ë¨ 0.00 GB, ì˜ˆì•½ë¨ 0.00 GB\n",
            "\n",
            "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]\n",
            "Loading pipeline components...:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 3/7 [00:00<00:00,  7.06it/s]\n",
            "Loading pipeline components...:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 5/7 [00:00<00:00,  9.83it/s]\n",
            "Loading pipeline components...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:01<00:00,  4.72it/s]\n",
            "Loading pipeline components...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:01<00:00,  5.45it/s]\n",
            "INFO:     ëª¨ë¸ íŒŒì´í”„ë¼ì¸ ë¡œë”© ì™„ë£Œ. Device: cuda:0\n",
            "INFO:     ë©”ëª¨ë¦¬ (íŒŒì´í”„ë¼ì¸ ë¡œë”© í›„): í• ë‹¹ë¨ 7.05 GB, ì˜ˆì•½ë¨ 7.34 GB\n",
            "INFO:     ëª¨ë¸ ë° ì´ˆê¸° LoRA ë¡œë”© ì™„ë£Œ. (ì†Œìš” ì‹œê°„: 4.10ì´ˆ)\n",
            "INFO:     Application startup complete.\n",
            "INFO:     Uvicorn running on http://0.0.0.0:9080 (Press CTRL+C to quit)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "FastAPI + Diffusers + ngrok API ë°°í¬ ìµœì¢… ìŠ¤í¬ë¦½íŠ¸ (Colab ê¸°ë°˜)\n",
        "\n",
        "- FastAPI: API ì„œë²„ êµ¬ì¶•\n",
        "- Diffusers: Stable Diffusion ëª¨ë¸ ì¶”ë¡ \n",
        "- ngrok: ì™¸ë¶€ ì¸í„°ë„· ë…¸ì¶œ (Authtoken í•„ìš”)\n",
        "- ë™ì  LoRA ë¡œë”©/ì–¸ë¡œë”© API í¬í•¨\n",
        "- TorchServe ê´€ë ¨ ì½”ë“œ ëª¨ë‘ ì œê±°\n",
        "- ê¸°ë³¸ í¬íŠ¸ 9080 ì‚¬ìš©\n",
        "\"\"\"\n",
        "\n",
        "# ==============================================================================\n",
        "# 0. í™˜ê²½ ì„¤ì • ë° ë³€ìˆ˜ ì •ì˜\n",
        "# ==============================================================================\n",
        "import os\n",
        "import subprocess\n",
        "import json\n",
        "import time\n",
        "import threading\n",
        "import requests\n",
        "from google.colab import drive, files # LoRA ì‚¬ìš© ì‹œ í•„ìš”í•  ìˆ˜ ìˆìŒ\n",
        "from IPython.display import display, Image as IPyImage, clear_output\n",
        "import base64\n",
        "import io\n",
        "from PIL import Image\n",
        "import logging\n",
        "import shlex # ëª…ë ¹ì–´ ì•ˆì „í•˜ê²Œ ë¶„ë¦¬í•˜ê¸° ìœ„í•´ ì‚¬ìš©\n",
        "\n",
        "# --- ì‚¬ìš©ì ì„¤ì • ---\n",
        "# @markdown ### ëª¨ë¸ ë° LoRA ì„¤ì •\n",
        "# @markdown ì‚¬ìš©í•  Hugging Face Diffusers ëª¨ë¸ ID ë˜ëŠ” ê²½ë¡œ\n",
        "MODEL_ID = \"stabilityai/stable-diffusion-xl-base-1.0\" # @param {type:\"string\"}\n",
        "# @markdown (ì„ íƒ ì‚¬í•­) ì‹œì‘ ì‹œ ë¡œë“œí•  LoRA íŒŒì¼ ê²½ë¡œ (Google Drive ë‚´). ë¹„ì›Œë‘ë©´ LoRA ì—†ì´ ì‹œì‘.\n",
        "INITIAL_LORA_WEIGHTS_PATH = \"\" # @param {type:\"string\"}\n",
        "\n",
        "# @markdown ---\n",
        "# @markdown ### ngrok ì„¤ì •\n",
        "# @markdown [ngrok ëŒ€ì‹œë³´ë“œ](https://dashboard.ngrok.com/get-started/your-authtoken)ì—ì„œ Authtokenì„ ë³µì‚¬í•˜ì—¬ ë¶™ì—¬ë„£ìœ¼ì„¸ìš”.\n",
        "# @markdown **ì£¼ì˜:** Authtokenì€ ë¹„ë°€ ì •ë³´ì´ë¯€ë¡œ ë…¸íŠ¸ë¶ ê³µìœ  ì‹œ ê°ë³„íˆ ì£¼ì˜í•˜ì„¸ìš”.\n",
        "NGROK_AUTH_TOKEN = \"2vtLqb9HoP0xoiiwqovFN2laGrd_6pD67sTGUxgGasqEDTwHb\"  # @param {type:\"string\"}\n",
        "\n",
        "# --- ë‚´ë¶€ ì„¤ì • ---\n",
        "FASTAPI_APP_FILE = \"/content/main.py\" # ìƒì„±ë  FastAPI ì•± íŒŒì¼\n",
        "REQUIREMENTS_PATH = \"/content/requirements_fastapi_ngrok.txt\"\n",
        "UVICORN_LOG_FILE = \"/content/uvicorn.log\" # Uvicorn ë¡œê·¸ íŒŒì¼ ê²½ë¡œ\n",
        "API_PORT = \"9080\" # FastAPI ì„œë²„ê°€ ë¦¬ìŠ¤ë‹í•  í¬íŠ¸\n",
        "MODEL_API_NAME = \"sdxl_diffusers\" # API ê²½ë¡œ ë“±ì— ì°¸ê³ ìš© (í˜„ì¬ ì§ì ‘ ì‚¬ìš© ì•ˆ í•¨)\n",
        "is_google_colab = 'google.colab' in str(get_ipython())\n",
        "public_url = None # ngrok URL ì €ì¥ ë³€ìˆ˜\n",
        "uvicorn_process = None # Uvicorn í”„ë¡œì„¸ìŠ¤ ì¶”ì ìš©\n",
        "\n",
        "# ë¡œê¹… ì„¤ì •\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(__name__)\n"
      ],
      "metadata": {
        "id": "zW2NcNO8tPbV"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ==============================================================================\n",
        "# 1. Google Drive ë§ˆìš´íŠ¸ (LoRA ì‚¬ìš© ì‹œ)\n",
        "# ==============================================================================\n",
        "if INITIAL_LORA_WEIGHTS_PATH and is_google_colab: # LoRA ê²½ë¡œê°€ ìˆê³  Colabì¼ ë•Œë§Œ ë§ˆìš´íŠ¸\n",
        "    logger.info(\"ğŸ“‚ Google Drive ë§ˆìš´íŠ¸ ì¤‘...\")\n",
        "    try:\n",
        "        drive.mount('/content/drive', force_remount=True)\n",
        "        logger.info(\"âœ… Google Drive ë§ˆìš´íŠ¸ ì™„ë£Œ.\")\n",
        "        # ì´ˆê¸° LoRA íŒŒì¼ ì¡´ì¬ í™•ì¸\n",
        "        if not os.path.exists(INITIAL_LORA_WEIGHTS_PATH):\n",
        "             # ì¹˜ëª…ì  ì˜¤ë¥˜ ëŒ€ì‹  ê²½ê³  í‘œì‹œ\n",
        "             logger.error(f\"âš ï¸ ì§€ì •ëœ ì´ˆê¸° LoRA íŒŒì¼ ê²½ë¡œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {INITIAL_LORA_WEIGHTS_PATH}\")\n",
        "             INITIAL_LORA_WEIGHTS_PATH = \"\" # ê²½ë¡œ ë¬´íš¨í™”\n",
        "        else:\n",
        "             logger.info(f\"âœ… ì´ˆê¸° LoRA íŒŒì¼ í™•ì¸: {INITIAL_LORA_WEIGHTS_PATH}\")\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Google Drive ë§ˆìš´íŠ¸ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}\")\n",
        "        INITIAL_LORA_WEIGHTS_PATH = \"\" # ì˜¤ë¥˜ ì‹œ ê²½ë¡œ ë¬´íš¨í™”\n",
        "elif INITIAL_LORA_WEIGHTS_PATH and not is_google_colab:\n",
        "     logger.warning(\"Colab í™˜ê²½ ì•„ë‹˜. Drive ë§ˆìš´íŠ¸ ìƒëµ. ë¡œì»¬ ê²½ë¡œ ìœ íš¨ì„± í™•ì¸ í•„ìš”.\")\n",
        "     if not os.path.exists(INITIAL_LORA_WEIGHTS_PATH):\n",
        "         logger.error(f\"âš ï¸ ì´ˆê¸° LoRA íŒŒì¼ ê²½ë¡œ ì˜¤ë¥˜ (ë¡œì»¬): {INITIAL_LORA_WEIGHTS_PATH}\")\n",
        "         INITIAL_LORA_WEIGHTS_PATH = \"\"\n",
        "else:\n",
        "     logger.info(\"ì´ˆê¸° LoRA ê²½ë¡œê°€ ì„¤ì •ë˜ì§€ ì•Šì•„ Drive ë§ˆìš´íŠ¸ë¥¼ ê±´ë„ˆ<0xEB><0x9C><0x8D>ë‹ˆë‹¤.\")\n",
        "\n",
        "# ==============================================================================\n",
        "# 2. í•„ìš” íŒ¨í‚¤ì§€ ì„¤ì¹˜ (pyngrok ì¶”ê°€)\n",
        "# ==============================================================================\n",
        "logger.info(\"\\nâš™ï¸ í•„ìš”í•œ íŒ¨í‚¤ì§€ ì„¤ì¹˜ ì¤‘ (pyngrok í¬í•¨)...\")\n",
        "\n",
        "# requirements_fastapi_ngrok.txt ìƒì„±\n",
        "requirements_content = \"\"\"\n",
        "fastapi>=0.100.0 # ë²„ì „ ëª…ì‹œ ê¶Œì¥\n",
        "uvicorn[standard]>=0.20.0 # ASGI ì„œë²„\n",
        "python-multipart # FastAPI í¼ ë°ì´í„° ì²˜ë¦¬\n",
        "requests # API í…ŒìŠ¤íŠ¸ìš©\n",
        "pyngrok>=7.0.0 # ngrok ì‚¬ìš© ë¼ì´ë¸ŒëŸ¬ë¦¬\n",
        "\n",
        "torch>=2.0.0\n",
        "diffusers>=0.24.0 # LoRA API ì•ˆì •ì„± ê³ ë ¤\n",
        "transformers>=4.30.0\n",
        "accelerate>=0.20.0\n",
        "safetensors>=0.3.0\n",
        "invisible-watermark>=0.2.0\n",
        "pillow\n",
        "python-dotenv # .env íŒŒì¼ ë¡œë”© (ì„ íƒì )\n",
        "\"\"\"\n",
        "with open(REQUIREMENTS_PATH, 'w') as f: f.write(requirements_content)\n",
        "logger.info(f\"âœ… requirements_fastapi_ngrok.txt ìƒì„± ì™„ë£Œ: {REQUIREMENTS_PATH}\")\n",
        "\n",
        "# pip install\n",
        "logger.info(f\"   - pip install ì‹¤í–‰: {REQUIREMENTS_PATH}\")\n",
        "try:\n",
        "    # ì´ì „ ì‹¤í–‰ì˜ uvicorn ë“± í”„ë¡œì„¸ìŠ¤ ì¢…ë£Œ ì‹œë„ (ì„ íƒì )\n",
        "    subprocess.run(['pkill', '-f', 'uvicorn'], capture_output=True)\n",
        "    pip_process = subprocess.run(['pip', 'install', '-r', REQUIREMENTS_PATH, '-qq'],\n",
        "                                 check=True, capture_output=True, text=True, encoding='utf-8')\n",
        "    logger.info(\"âœ… Python íŒ¨í‚¤ì§€ ì„¤ì¹˜ ì™„ë£Œ.\")\n",
        "except subprocess.CalledProcessError as e:\n",
        "    logger.error(f\"ğŸ’¥ pip install ì‹¤íŒ¨! (ì¢…ë£Œ ì½”ë“œ: {e.returncode})\")\n",
        "    print(\"--- pip stdout ---\\n\", e.stdout)\n",
        "    print(\"--- pip stderr ---\\n\", e.stderr)\n",
        "    # pip ì„¤ì¹˜ ì‹¤íŒ¨ëŠ” ì¹˜ëª…ì ì´ë¯€ë¡œ ìŠ¤í¬ë¦½íŠ¸ ì¤‘ë‹¨\n",
        "    raise RuntimeError(f\"íŒ¨í‚¤ì§€ ì„¤ì¹˜ ì‹¤íŒ¨: {e}\")\n",
        "\n",
        "# ngrok Authtoken í™•ì¸\n",
        "if not NGROK_AUTH_TOKEN:\n",
        "    logger.warning(\"âš ï¸ ngrok Authtokenì´ ì…ë ¥ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤! ngrok í„°ë„ ìƒì„±ì— ì‹¤íŒ¨í•˜ê±°ë‚˜ ì œí•œì´ ìˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\")\n",
        "    # Authtoken ì—†ì´ ì§„í–‰ì€ ê°€ëŠ¥í•˜ë‚˜, ê²½ê³  í‘œì‹œ\n",
        "else:\n",
        "     logger.info(\"âœ… ngrok Authtoken í™•ì¸ë¨ (ì…ë ¥ê°’ ê¸°ì¤€).\")\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "# 3. FastAPI ì•± ìŠ¤í¬ë¦½íŠ¸ ìƒì„± (main.py)\n",
        "# ==============================================================================\n",
        "logger.info(f\"\\nğŸ“ FastAPI ì•± ìŠ¤í¬ë¦½íŠ¸ ìƒì„± ì¤‘ ({FASTAPI_APP_FILE})...\")\n",
        "\n",
        "# FastAPI ì•± ì½”ë“œ (f''' ëŒ€ì‹  ''' ì‚¬ìš©, APP_ í™˜ê²½ë³€ìˆ˜ ì‚¬ìš©)\n",
        "fastapi_app_content = '''\n",
        "import logging\n",
        "import os\n",
        "import base64\n",
        "import io\n",
        "import time\n",
        "import torch\n",
        "from diffusers import DiffusionPipeline, DPMSolverMultistepScheduler\n",
        "from fastapi import FastAPI, HTTPException, Request\n",
        "from pydantic import BaseModel, Field\n",
        "from PIL import Image\n",
        "import asyncio\n",
        "from contextlib import asynccontextmanager\n",
        "import threading # ì „ì—­ ë³€ìˆ˜ ë³´í˜¸ìš© Lock (asyncio.Lockë„ ê°€ëŠ¥)\n",
        "\n",
        "# --- ê¸°ë³¸ ì„¤ì • ---\n",
        "# í™˜ê²½ ë³€ìˆ˜ ë˜ëŠ” ê¸°ë³¸ê°’ ì‚¬ìš©\n",
        "APP_MODEL_ID = os.environ.get(\"APP_MODEL_ID\", \"stabilityai/stable-diffusion-xl-base-1.0\")\n",
        "APP_INITIAL_LORA_PATH = os.environ.get(\"APP_INITIAL_LORA_PATH\", None)\n",
        "ADAPTER_NAME = \"default\" # ì‚¬ìš©í•  LoRA ì–´ëŒ‘í„° ì´ë¦„\n",
        "\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - [%(levelname)s] - %(name)s - %(message)s')\n",
        "logger = logging.getLogger(\"uvicorn.error\") # Uvicorn ì—ëŸ¬ ë¡œê±°ì— ì¶œë ¥\n",
        "\n",
        "# --- ëª¨ë¸ ë° ìƒíƒœ ê´€ë¦¬ ---\n",
        "pipeline = None\n",
        "current_lora_path = None\n",
        "model_lock = threading.Lock() # ë™ê¸° í•¨ìˆ˜ì—ì„œ ì „ì—­ ë³€ìˆ˜ ë³´í˜¸ìš©\n",
        "\n",
        "# --- FastAPI ì•± ìƒëª…ì£¼ê¸° ê´€ë¦¬ (ëª¨ë¸ ë¡œë”©/ì–¸ë¡œë”©) ---\n",
        "@asynccontextmanager\n",
        "async def lifespan(app: FastAPI):\n",
        "    global pipeline, current_lora_path, model_lock\n",
        "    logger.info(\"Lifespan: ì• í”Œë¦¬ì¼€ì´ì…˜ ì‹œì‘ - ëª¨ë¸ ë¡œë”©...\")\n",
        "    start_time = time.time()\n",
        "    try:\n",
        "        # ëª¨ë¸ ë¡œë”© ë¡œì§ (ë™ê¸°) - lifespanì€ ë™ê¸° í•¨ìˆ˜ ì‹¤í–‰ ê°€ëŠ¥\n",
        "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "        torch_dtype = torch.float16 if device == \"cuda\" else torch.float32\n",
        "        logger.info(f\"Lifespan: Device={device}, Dtype={torch_dtype}\")\n",
        "\n",
        "        if torch.cuda.is_available(): logger.info(f\"Lifespan: ë©”ëª¨ë¦¬ (ë¡œë”© ì „) Allocated={torch.cuda.memory_allocated(device)/1e9:.2f}GB, Reserved={torch.cuda.memory_reserved(device)/1e9:.2f}GB\")\n",
        "\n",
        "        temp_pipeline = DiffusionPipeline.from_pretrained(\n",
        "            APP_MODEL_ID,\n",
        "            torch_dtype=torch_dtype,\n",
        "            variant=\"fp16\" if torch_dtype == torch.float16 else None,\n",
        "            use_safetensors=True\n",
        "        )\n",
        "        temp_pipeline.scheduler = DPMSolverMultistepScheduler.from_config(temp_pipeline.scheduler.config, use_karras_sigmas=True)\n",
        "        temp_pipeline.to(device)\n",
        "        logger.info(f\"Lifespan: ëª¨ë¸ íŒŒì´í”„ë¼ì¸ ë¡œë”© ì™„ë£Œ. Device={temp_pipeline.device}\")\n",
        "        if torch.cuda.is_available(): logger.info(f\"Lifespan: ë©”ëª¨ë¦¬ (íŒŒì´í”„ë¼ì¸ ë¡œë”© í›„) Allocated={torch.cuda.memory_allocated(device)/1e9:.2f}GB, Reserved={torch.cuda.memory_reserved(device)/1e9:.2f}GB\")\n",
        "\n",
        "        # ì „ì—­ ë³€ìˆ˜ í• ë‹¹ (Lock ë¶ˆí•„ìš”, ì‹œì‘ ì‹œ í•œ ë²ˆë§Œ ì‹¤í–‰ë¨)\n",
        "        pipeline = temp_pipeline\n",
        "\n",
        "        # ì´ˆê¸° LoRA ë¡œë“œ (ë™ê¸°)\n",
        "        if APP_INITIAL_LORA_PATH and os.path.exists(APP_INITIAL_LORA_PATH):\n",
        "            logger.info(f\"Lifespan: ì´ˆê¸° LoRA ë¡œë”© ì‹œë„: {APP_INITIAL_LORA_PATH}\")\n",
        "            try:\n",
        "                # lifespan ë‚´ì—ì„œëŠ” Lock ì—†ì´ ì§ì ‘ í˜¸ì¶œ ê°€ëŠ¥ (ì‹œì‘ ì‹œ ë™ê¸° ì‹¤í–‰)\n",
        "                load_lora_blocking(APP_INITIAL_LORA_PATH)\n",
        "                logger.info(f\"Lifespan: ì´ˆê¸° LoRA ë¡œë”© ì„±ê³µ: {APP_INITIAL_LORA_PATH}\")\n",
        "                # current_lora_path ëŠ” load_lora_blocking ë‚´ë¶€ì—ì„œ ì—…ë°ì´íŠ¸ë¨\n",
        "                if torch.cuda.is_available(): logger.info(f\"Lifespan: ë©”ëª¨ë¦¬ (LoRA ë¡œë”© í›„) Allocated={torch.cuda.memory_allocated(device)/1e9:.2f}GB, Reserved={torch.cuda.memory_reserved(device)/1e9:.2f}GB\")\n",
        "            except Exception as e:\n",
        "                 logger.error(f\"Lifespan: ì´ˆê¸° LoRA ë¡œë”© ì‹¤íŒ¨: {e}\", exc_info=True)\n",
        "                 # ì‹¤íŒ¨í•´ë„ ì„œë²„ëŠ” ì‹œì‘ë  ìˆ˜ ìˆìŒ\n",
        "        elif APP_INITIAL_LORA_PATH:\n",
        "             logger.warning(f\"Lifespan: ì´ˆê¸° LoRA íŒŒì¼({APP_INITIAL_LORA_PATH}) ì—†ìŒ.\")\n",
        "\n",
        "        loading_time = time.time() - start_time\n",
        "        logger.info(f\"Lifespan: ëª¨ë¸ ë° ì´ˆê¸° LoRA ë¡œë”© ì™„ë£Œ. (ì†Œìš” ì‹œê°„: {loading_time:.2f}ì´ˆ)\")\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Lifespan: ëª¨ë¸ ë¡œë”© ì¤‘ ì¹˜ëª…ì  ì˜¤ë¥˜ ë°œìƒ: {e}\", exc_info=True)\n",
        "        pipeline = None # ë¡œë”© ì‹¤íŒ¨ ì‹œ None ìœ ì§€\n",
        "\n",
        "    yield # ì• í”Œë¦¬ì¼€ì´ì…˜ ì‹¤í–‰ êµ¬ê°„\n",
        "\n",
        "    # --- ì• í”Œë¦¬ì¼€ì´ì…˜ ì¢…ë£Œ ì‹œ ---\n",
        "    logger.info(\"Lifespan: ì• í”Œë¦¬ì¼€ì´ì…˜ ì¢…ë£Œ - ëª¨ë¸ ì •ë¦¬...\")\n",
        "    with model_lock: # Lock ì‚¬ìš©í•˜ì—¬ ì•ˆì „í•˜ê²Œ ì •ë¦¬\n",
        "        pipeline = None\n",
        "        current_lora_path = None\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "        logger.info(\"Lifespan: GPU ìºì‹œ ì •ë¦¬ ì™„ë£Œ.\")\n",
        "    logger.info(\"Lifespan: ì• í”Œë¦¬ì¼€ì´ì…˜ ì¢…ë£Œ ì™„ë£Œ.\")\n",
        "\n",
        "\n",
        "app = FastAPI(lifespan=lifespan)\n",
        "\n",
        "# --- Pydantic ëª¨ë¸ ì •ì˜ ---\n",
        "class InferenceRequest(BaseModel):\n",
        "    prompt: str\n",
        "    negative_prompt: str = \"\"\n",
        "    height: int = Field(1024, gt=0)\n",
        "    width: int = Field(1024, gt=0)\n",
        "    steps: int = Field(30, gt=0, le=100)\n",
        "    cfg_scale: float = Field(7.5, gt=0.0, le=20.0)\n",
        "    seed: int | None = None\n",
        "\n",
        "class InferenceResponse(BaseModel):\n",
        "    image_base64: str\n",
        "    generation_time_ms: int\n",
        "    model_id: str\n",
        "    current_lora: str | None\n",
        "\n",
        "class LoraRequest(BaseModel):\n",
        "    lora_path: str\n",
        "\n",
        "class StatusResponse(BaseModel):\n",
        "    status: str\n",
        "    model_id: str | None\n",
        "    device: str | None\n",
        "    current_lora_path: str | None\n",
        "    active_adapters: list | None\n",
        "\n",
        "\n",
        "# --- Helper Functions (Blocking tasks) ---\n",
        "# ì´ í•¨ìˆ˜ë“¤ì€ ì „ì—­ pipelineê³¼ current_lora_pathë¥¼ ì§ì ‘ ìˆ˜ì •í•˜ë¯€ë¡œ Lock í•„ìš”\n",
        "\n",
        "def load_lora_blocking(lora_path: str) -> str:\n",
        "    global pipeline, current_lora_path # ì „ì—­ ë³€ìˆ˜ ì‚¬ìš© ëª…ì‹œ\n",
        "    if not pipeline: raise RuntimeError(\"íŒŒì´í”„ë¼ì¸ì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\")\n",
        "    if not os.path.exists(lora_path): raise FileNotFoundError(f\"LoRA íŒŒì¼ ì—†ìŒ: {lora_path}\")\n",
        "\n",
        "    logger.info(f\"LoRA ë¡œë”© (ë™ê¸°): {lora_path}\")\n",
        "    pipeline.to(pipeline.device) # ì¥ì¹˜ í™•ì¸\n",
        "\n",
        "    # ê¸°ì¡´ ì–´ëŒ‘í„° ì–¸ë¡œë“œ ì‹œë„\n",
        "    try:\n",
        "        active_adapters = []\n",
        "        # Diffusers ë²„ì „ì— ë”°ë¥¸ API í˜¸í™˜ì„± ê³ ë ¤\n",
        "        if hasattr(pipeline, 'get_active_adapters'):\n",
        "             active_adapters = pipeline.get_active_adapters()\n",
        "        elif hasattr(pipeline, 'get_list_adapters'):\n",
        "             adapter_info = pipeline.get_list_adapters()\n",
        "             active_adapters = [name for name, enabled in adapter_info.items() if enabled]\n",
        "\n",
        "        if ADAPTER_NAME in active_adapters:\n",
        "            logger.info(f\"ê¸°ì¡´ '{ADAPTER_NAME}' ì–¸ë¡œë“œ/ì‚­ì œ ì‹œë„...\")\n",
        "            # ìµœì‹  ë²„ì „ ìš°ì„  (delete_adapters ê°€ unload í¬í•¨ ê°€ëŠ¥ì„±)\n",
        "            if hasattr(pipeline, 'delete_adapters'):\n",
        "                 pipeline.delete_adapters(ADAPTER_NAME)\n",
        "                 logger.info(\"delete_adapters í˜¸ì¶œ ì™„ë£Œ.\")\n",
        "            elif hasattr(pipeline, 'unload_lora_weights'):\n",
        "                 pipeline.unload_lora_weights(adapter_names=[ADAPTER_NAME])\n",
        "                 logger.info(\"unload_lora_weights í˜¸ì¶œ ì™„ë£Œ.\")\n",
        "            else:\n",
        "                 logger.warning(\"LoRA ì–¸ë¡œë“œ/ì‚­ì œ ë©”ì„œë“œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n",
        "    except Exception as e:\n",
        "        logger.warning(f\"ê¸°ì¡´ LoRA ì–¸ë¡œë“œ ì¤‘ ê²½ê³ (ë¬´ì‹œ): {e}\")\n",
        "\n",
        "    logger.info(f\"ìƒˆ LoRA ë¡œë”©: '{lora_path}' ('{ADAPTER_NAME}')...\")\n",
        "    pipeline.load_lora_weights(\n",
        "        os.path.dirname(lora_path),\n",
        "        weight_name=os.path.basename(lora_path),\n",
        "        adapter_name=ADAPTER_NAME\n",
        "    )\n",
        "    logger.info(f\"LoRA ê°€ì¤‘ì¹˜ ë¡œë”© ì„±ê³µ: {lora_path}\")\n",
        "\n",
        "    # í™œì„±í™” (set_adapters ì‚¬ìš© ê¶Œì¥)\n",
        "    if hasattr(pipeline, 'set_adapters'):\n",
        "        pipeline.set_adapters([ADAPTER_NAME], adapter_weights=[1.0])\n",
        "        logger.info(f\"'{ADAPTER_NAME}' í™œì„±í™” (set_adapters)\")\n",
        "    elif hasattr(pipeline, 'fuse_lora'): # êµ¬ë²„ì „ í˜¸í™˜ì„±\n",
        "         logger.warning(\"set_adapters ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ. fuse_lora() ì‹œë„.\")\n",
        "         pipeline.fuse_lora(adapter_names=[ADAPTER_NAME])\n",
        "\n",
        "    current_lora_path = lora_path # ìƒíƒœ ì—…ë°ì´íŠ¸\n",
        "    if torch.cuda.is_available(): torch.cuda.empty_cache()\n",
        "    return f\"ì„±ê³µì ìœ¼ë¡œ LoRA ë¡œë“œ: {lora_path}\"\n",
        "\n",
        "def unload_lora_blocking() -> str:\n",
        "    global pipeline, current_lora_path\n",
        "    if not pipeline: raise RuntimeError(\"íŒŒì´í”„ë¼ì¸ì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\")\n",
        "    if not current_lora_path: return \"í˜„ì¬ ë¡œë“œëœ LoRA ì—†ìŒ.\"\n",
        "\n",
        "    logger.info(f\"LoRA ì–¸ë¡œë“œ (ë™ê¸°): {current_lora_path}\")\n",
        "    pipeline.to(pipeline.device)\n",
        "    unloaded_path = current_lora_path\n",
        "    try:\n",
        "        logger.info(f\"'{ADAPTER_NAME}' ì–¸ë¡œë“œ/ì‚­ì œ ì‹œë„...\")\n",
        "        if hasattr(pipeline, 'delete_adapters'):\n",
        "             pipeline.delete_adapters(ADAPTER_NAME)\n",
        "             logger.info(\"delete_adapters í˜¸ì¶œ ì™„ë£Œ.\")\n",
        "        elif hasattr(pipeline, 'unload_lora_weights'):\n",
        "             pipeline.unload_lora_weights(adapter_names=[ADAPTER_NAME])\n",
        "             logger.info(\"unload_lora_weights í˜¸ì¶œ ì™„ë£Œ.\")\n",
        "        else: # Fallback ë¹„í™œì„±í™”\n",
        "             if hasattr(pipeline, 'set_adapters'): pipeline.set_adapters([]); logger.info(\"set_adapters([]) í˜¸ì¶œ\")\n",
        "             elif hasattr(pipeline, 'unfuse_lora'): pipeline.unfuse_lora(); logger.info(\"unfuse_lora() í˜¸ì¶œ\")\n",
        "             else: logger.warning(\"LoRA ë¹„í™œì„±í™” ë©”ì„œë“œ ì°¾ì„ ìˆ˜ ì—†ìŒ\")\n",
        "\n",
        "        logger.info(f\"ì„±ê³µì ìœ¼ë¡œ LoRA ì–¸ë¡œë“œ/ë¹„í™œì„±í™”: {unloaded_path}\")\n",
        "        current_lora_path = None # ìƒíƒœ ì—…ë°ì´íŠ¸\n",
        "        if torch.cuda.is_available(): torch.cuda.empty_cache()\n",
        "        return f\"ì„±ê³µì ìœ¼ë¡œ LoRA ì–¸ë¡œë“œ: {unloaded_path}\"\n",
        "    except Exception as e:\n",
        "         logger.error(f\"LoRA ì–¸ë¡œë“œ ì¤‘ ì˜¤ë¥˜: {e}\", exc_info=True)\n",
        "         current_lora_path = None # ì˜¤ë¥˜ ì‹œì—ë„ ìƒíƒœ ì´ˆê¸°í™”\n",
        "         raise RuntimeError(f\"LoRA ì–¸ë¡œë“œ ì‹¤íŒ¨: {e}\") from e\n",
        "\n",
        "def generate_image_blocking(req: InferenceRequest) -> Image.Image:\n",
        "    global pipeline # pipelineì€ ì½ê¸°ë§Œ í•˜ë¯€ë¡œ Lock ë¶ˆí•„ìš”\n",
        "    if not pipeline: raise RuntimeError(\"íŒŒì´í”„ë¼ì¸ì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\")\n",
        "\n",
        "    logger.info(f\"ì´ë¯¸ì§€ ìƒì„± ì‹œì‘ (ë™ê¸°): prompt='{req.prompt[:50]}...'\")\n",
        "    start_gen_time = time.time()\n",
        "    # ìš”ì²­ íŒŒë¼ë¯¸í„° ì¤€ë¹„\n",
        "    params = {\n",
        "        \"negative_prompt\": req.negative_prompt,\n",
        "        \"num_inference_steps\": req.steps,\n",
        "        \"guidance_scale\": req.cfg_scale,\n",
        "        \"height\": req.height,\n",
        "        \"width\": req.width,\n",
        "    }\n",
        "    if req.seed is not None:\n",
        "        # GeneratorëŠ” ë§¤ë²ˆ ìƒì„±í•˜ê±°ë‚˜, ì‹œë“œë³„ë¡œ ìºì‹±í•  ìˆ˜ ìˆìŒ\n",
        "        params[\"generator\"] = torch.Generator(device=pipeline.device).manual_seed(req.seed)\n",
        "\n",
        "    # ì¶”ë¡  ì‹¤í–‰\n",
        "    with torch.inference_mode():\n",
        "        result_image = pipeline(prompt=req.prompt, **params).images[0]\n",
        "\n",
        "    logger.info(f\"ì´ë¯¸ì§€ ìƒì„± ì™„ë£Œ (ë™ê¸°, {(time.time() - start_gen_time)*1000:.0f}ms)\")\n",
        "    return result_image\n",
        "\n",
        "\n",
        "# --- API ì—”ë“œí¬ì¸íŠ¸ ---\n",
        "\n",
        "@app.get(\"/status\", response_model=StatusResponse)\n",
        "async def get_status():\n",
        "    \"\"\"í˜„ì¬ ì„œë²„ ìƒíƒœ (ë¡œë“œëœ ëª¨ë¸, LoRA ë“±) ë°˜í™˜\"\"\"\n",
        "    with model_lock: # ìƒíƒœ ì½ê¸° ì‹œì—ë„ Lock ì‚¬ìš© (current_lora_path ì¼ê´€ì„±)\n",
        "        if not pipeline:\n",
        "            status = \"error\"\n",
        "            device = None\n",
        "            lora = current_lora_path # pipeline ì—†ì–´ë„ lora ê²½ë¡œ ë³€ìˆ˜ í™•ì¸ ê°€ëŠ¥\n",
        "            adapters = None\n",
        "        else:\n",
        "            status = \"ready\"\n",
        "            device = str(pipeline.device)\n",
        "            lora = current_lora_path\n",
        "            adapters = []\n",
        "            try:\n",
        "                if hasattr(pipeline, 'get_active_adapters'): adapters = pipeline.get_active_adapters()\n",
        "                elif hasattr(pipeline, 'get_list_adapters'): adapters = [name for name, enabled in pipeline.get_list_adapters().items() if enabled]\n",
        "            except Exception as e: logger.warning(f\"í™œì„± ì–´ëŒ‘í„° ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨: {e}\")\n",
        "\n",
        "        return StatusResponse(\n",
        "            status=status,\n",
        "            model_id=APP_MODEL_ID,\n",
        "            device=device,\n",
        "            current_lora_path=lora,\n",
        "            active_adapters=adapters\n",
        "        )\n",
        "\n",
        "@app.post(\"/predictions\", response_model=InferenceResponse)\n",
        "async def predict(request: InferenceRequest):\n",
        "    \"\"\"ì´ë¯¸ì§€ ìƒì„± ìš”ì²­ ì²˜ë¦¬\"\"\"\n",
        "    if not pipeline:\n",
        "        raise HTTPException(status_code=503, detail=\"ëª¨ë¸ íŒŒì´í”„ë¼ì¸ ì¤€ë¹„ë˜ì§€ ì•ŠìŒ.\")\n",
        "\n",
        "    start_time = time.time()\n",
        "    try:\n",
        "        # ë¸”ë¡œí‚¹ ì‘ì—…ì„ ë³„ë„ ìŠ¤ë ˆë“œì—ì„œ ì‹¤í–‰\n",
        "        pil_image = await asyncio.to_thread(generate_image_blocking, request)\n",
        "\n",
        "        # PIL Image -> Base64 (ì´ ì‘ì—…ë„ ì˜¤ë˜ ê±¸ë¦¬ë©´ to_thread ê³ ë ¤)\n",
        "        buffered = io.BytesIO()\n",
        "        pil_image.save(buffered, format=\"PNG\")\n",
        "        img_str = base64.b64encode(buffered.getvalue()).decode(\"utf-8\")\n",
        "\n",
        "        end_time = time.time()\n",
        "        generation_time_ms = int((end_time - start_time) * 1000)\n",
        "\n",
        "        # ì‘ë‹µ ìƒì„± ì „ í˜„ì¬ LoRA ìƒíƒœ í™•ì¸ (Lock ë‚´ë¶€ì—ì„œ ë³€ê²½ë  ìˆ˜ ìˆìœ¼ë¯€ë¡œ ë‹¤ì‹œ í™•ì¸)\n",
        "        with model_lock:\n",
        "            lora_in_use = current_lora_path\n",
        "\n",
        "        return InferenceResponse(\n",
        "            image_base64=img_str,\n",
        "            generation_time_ms=generation_time_ms,\n",
        "            model_id=APP_MODEL_ID,\n",
        "            current_lora=lora_in_use\n",
        "        )\n",
        "    # íŠ¹ì • ì‚¬ìš©ì ì…ë ¥ ì˜¤ë¥˜ ì²˜ë¦¬ (ì˜ˆ: Pydantic ê²€ì¦ ì‹¤íŒ¨ëŠ” FastAPIê°€ ì²˜ë¦¬)\n",
        "    # except ValueError as ve:\n",
        "    #     raise HTTPException(status_code=400, detail=str(ve))\n",
        "    except Exception as e:\n",
        "        logger.error(f\"ì¶”ë¡  ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}\", exc_info=True)\n",
        "        raise HTTPException(status_code=500, detail=f\"ì¶”ë¡  ì‹¤íŒ¨: {str(e)}\")\n",
        "\n",
        "@app.post(\"/load-lora\", status_code=200)\n",
        "async def load_lora(request: LoraRequest):\n",
        "    \"\"\"ì§€ì •ëœ ê²½ë¡œì˜ LoRA ê°€ì¤‘ì¹˜ë¥¼ ë¡œë“œ\"\"\"\n",
        "    if not pipeline:\n",
        "        raise HTTPException(status_code=503, detail=\"ëª¨ë¸ íŒŒì´í”„ë¼ì¸ ì¤€ë¹„ë˜ì§€ ì•ŠìŒ.\")\n",
        "\n",
        "    # Lockì„ ì‚¬ìš©í•˜ì—¬ ë™ì‹œ ì ‘ê·¼ ë°©ì§€\n",
        "    # asyncio.Lock ì€ async í•¨ìˆ˜ ë‚´ì—ì„œ ì‚¬ìš©í•´ì•¼ í•¨\n",
        "    # ì—¬ê¸°ì„œëŠ” model_lock (threading.Lock)ì„ ì‚¬ìš©í•˜ê³  ë™ê¸° í•¨ìˆ˜ë¥¼ ìŠ¤ë ˆë“œì—ì„œ ì‹¤í–‰\n",
        "    try:\n",
        "        logger.info(f\"LoRA ë¡œë“œ ìš”ì²­ ìˆ˜ì‹ : {request.lora_path}\")\n",
        "        # ë¸”ë¡œí‚¹ í•¨ìˆ˜ load_lora_blocking ì€ ë‚´ë¶€ì—ì„œ model_lock ì‚¬ìš©\n",
        "        message = await asyncio.to_thread(load_lora_blocking, request.lora_path)\n",
        "        logger.info(f\"LoRA ë¡œë“œ ìš”ì²­ ì²˜ë¦¬ ì™„ë£Œ.\")\n",
        "        return {\"status\": \"success\", \"message\": message}\n",
        "    except FileNotFoundError as e:\n",
        "        logger.error(f\"LoRA íŒŒì¼ ì—†ìŒ ì˜¤ë¥˜: {e}\")\n",
        "        raise HTTPException(status_code=404, detail=str(e))\n",
        "    except Exception as e:\n",
        "        logger.error(f\"LoRA ë¡œë“œ ì¤‘ ì˜¤ë¥˜: {e}\", exc_info=True)\n",
        "        raise HTTPException(status_code=500, detail=f\"LoRA ë¡œë“œ ì‹¤íŒ¨: {str(e)}\")\n",
        "\n",
        "@app.post(\"/unload-lora\", status_code=200)\n",
        "async def unload_lora():\n",
        "    \"\"\"í˜„ì¬ ë¡œë“œëœ LoRA ê°€ì¤‘ì¹˜ë¥¼ ì–¸ë¡œë“œ\"\"\"\n",
        "    if not pipeline:\n",
        "        raise HTTPException(status_code=503, detail=\"ëª¨ë¸ íŒŒì´í”„ë¼ì¸ ì¤€ë¹„ë˜ì§€ ì•ŠìŒ.\")\n",
        "\n",
        "    try:\n",
        "        logger.info(f\"LoRA ì–¸ë¡œë“œ ìš”ì²­ ìˆ˜ì‹ .\")\n",
        "        # ë¸”ë¡œí‚¹ í•¨ìˆ˜ unload_lora_blocking ì€ ë‚´ë¶€ì—ì„œ model_lock ì‚¬ìš©\n",
        "        message = await asyncio.to_thread(unload_lora_blocking)\n",
        "        logger.info(f\"LoRA ì–¸ë¡œë“œ ìš”ì²­ ì²˜ë¦¬ ì™„ë£Œ.\")\n",
        "        return {\"status\": \"success\", \"message\": message}\n",
        "    except Exception as e:\n",
        "        logger.error(f\"LoRA ì–¸ë¡œë“œ ì¤‘ ì˜¤ë¥˜: {e}\", exc_info=True)\n",
        "        raise HTTPException(status_code=500, detail=f\"LoRA ì–¸ë¡œë“œ ì‹¤íŒ¨: {str(e)}\")\n",
        "\n",
        "# uvicornìœ¼ë¡œ ì‹¤í–‰ ì‹œ ì´ íŒŒì¼ì´ ì§ì ‘ ì‹¤í–‰ë˜ì§€ëŠ” ì•ŠìŒ\n",
        "# if __name__ == \"__main__\":\n",
        "#     print(\"ì´ íŒŒì¼ì€ uvicornì„ í†µí•´ ì‹¤í–‰ë˜ì–´ì•¼ í•©ë‹ˆë‹¤: uvicorn main:app --host 0.0.0.0 --port 9080\")\n",
        "\n",
        "'''\n",
        "with open(FASTAPI_APP_FILE, 'w', encoding='utf-8') as f: f.write(fastapi_app_content)\n",
        "logger.info(f\"âœ… FastAPI ì•± ìŠ¤í¬ë¦½íŠ¸ ì €ì¥ ì™„ë£Œ: {FASTAPI_APP_FILE}\")\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "# 4. FastAPI ì„œë²„ ì‹œì‘ (Uvicorn ì‚¬ìš©)\n",
        "# ==============================================================================\n",
        "logger.info(\"\\nğŸš€ FastAPI ì„œë²„ ì‹œì‘ ì¤‘ (Uvicorn)...\")\n",
        "\n",
        "# FastAPI ì•± ë‚´ë¶€ì—ì„œ ì‚¬ìš©í•  í™˜ê²½ ë³€ìˆ˜ ì„¤ì •\n",
        "os.environ['APP_MODEL_ID'] = MODEL_ID\n",
        "if INITIAL_LORA_WEIGHTS_PATH: os.environ['APP_INITIAL_LORA_PATH'] = INITIAL_LORA_WEIGHTS_PATH\n",
        "else:\n",
        "    if 'APP_INITIAL_LORA_PATH' in os.environ: del os.environ['APP_INITIAL_LORA_PATH']\n",
        "\n",
        "# Uvicorn ì‹¤í–‰ ëª…ë ¹ì–´\n",
        "uvicorn_cmd_parts = [\"uvicorn\", \"main:app\", \"--host\", \"0.0.0.0\", \"--port\", API_PORT, \"--workers\", \"1\"]\n",
        "logger.info(f\"ì‹¤í–‰í•  Uvicorn ëª…ë ¹ì–´: {' '.join(uvicorn_cmd_parts)}\")\n",
        "logger.info(f\"Uvicorn ë¡œê·¸ëŠ” '{UVICORN_LOG_FILE}' íŒŒì¼ì— ì €ì¥ë©ë‹ˆë‹¤.\")\n",
        "\n",
        "# ê¸°ì¡´ uvicorn í”„ë¡œì„¸ìŠ¤ ì¢…ë£Œ\n",
        "subprocess.run(['pkill', '-f', 'uvicorn'], capture_output=True); time.sleep(3)\n",
        "\n",
        "# nohupìœ¼ë¡œ ë°±ê·¸ë¼ìš´ë“œ ì‹¤í–‰ ë° ë¡œê·¸ ë¦¬ë””ë ‰ì…˜\n",
        "# shell=True ëŒ€ì‹  shlex.split ì‚¬ìš© ì‹œë„ (ë” ì•ˆì „) -> nohup/& ì²˜ë¦¬ ë¶ˆê°€\n",
        "# nohup_cmd = f\"nohup {' '.join(uvicorn_cmd_parts)} > {UVICORN_LOG_FILE} 2>&1 &\" # ê°„ë‹¨í•˜ê²Œ shell=True ì‚¬ìš©\n",
        "cmd_str = f\"nohup {' '.join(uvicorn_cmd_parts)} > {UVICORN_LOG_FILE} 2>&1 & disown\" # disown ì¶”ê°€ ì‹œë„\n",
        "logger.info(f\"ë°±ê·¸ë¼ìš´ë“œ ì‹¤í–‰ ëª…ë ¹ì–´: {cmd_str}\")\n",
        "# Popen ëŒ€ì‹  run ì‚¬ìš©í•˜ê³  & ë¡œ ë°±ê·¸ë¼ìš´ë“œ ì‹¤í–‰ (ë” ê°„ë‹¨í•  ìˆ˜ ìˆìŒ)\n",
        "# subprocess.run(cmd_str, shell=True)\n",
        "uvicorn_process = subprocess.Popen(cmd_str, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
        "# Popen ì€ ëª…ë ¹ì–´ ì‹¤í–‰ ìì²´ì˜ ì„±ê³µ ì—¬ë¶€ë§Œ íŒë‹¨, ë°±ê·¸ë¼ìš´ë“œ í”„ë¡œì„¸ìŠ¤ ìƒíƒœëŠ” ë³„ë„ í™•ì¸ í•„ìš”\n",
        "logger.info(\"   - Uvicorn ì„œë²„ ì‹œì‘ ëª…ë ¹ì–´ ì‹¤í–‰ë¨ (ë°±ê·¸ë¼ìš´ë“œ).\")\n",
        "\n",
        "# ì„œë²„ ì‹œì‘ ë° ëª¨ë¸ ë¡œë”© ëŒ€ê¸° (ì‹œê°„ ì¶©ë¶„íˆ)\n",
        "wait_seconds = 150 # SDXL + ì´ˆê¸° LoRA ë¡œë”© ì‹œê°„ ê³ ë ¤ (ë„‰ë„‰í•˜ê²Œ)\n",
        "logger.info(f\"   - FastAPI ì„œë²„ ì´ˆê¸°í™” ë° ëª¨ë¸ ë¡œë”© ëŒ€ê¸° ì¤‘ ({wait_seconds}ì´ˆ)...\")\n",
        "# ëŒ€ê¸° ì‹œê°„ ë™ì•ˆ ë¡œê·¸ íŒŒì¼ ë³€í™”ë¥¼ ê°ì§€í•˜ê±°ë‚˜, ì£¼ê¸°ì ìœ¼ë¡œ ìƒíƒœ ì²´í¬\n",
        "time.sleep(wait_seconds) # ì¼ë‹¨ ê³ ì • ì‹œê°„ ëŒ€ê¸°\n",
        "\n",
        "# Uvicorn ë¡œê·¸ ë° ìƒíƒœ í™•ì¸\n",
        "server_ready = False\n",
        "logger.info(f\"   - {wait_seconds}ì´ˆ ê²½ê³¼. ë¡œê·¸ ë° ìƒíƒœ í™•ì¸ ì‹œì‘...\")\n",
        "try:\n",
        "    if os.path.exists(UVICORN_LOG_FILE):\n",
        "        with open(UVICORN_LOG_FILE, 'r', encoding='utf-8', errors='replace') as f: lines = f.readlines(); tail_lines = lines[-20:]\n",
        "        print(\"\\n--- Uvicorn ë¡œê·¸ (ë§ˆì§€ë§‰ 20ì¤„) ---\"); [print(line.strip()) for line in tail_lines]\n",
        "        # ì‹¤ì œ ì‹œì‘ ì™„ë£Œ ë©”ì‹œì§€ í™•ì¸ (uvicorn ë²„ì „ì— ë”°ë¼ ë‹¤ë¥¼ ìˆ˜ ìˆìŒ)\n",
        "        if any(f\"Application startup complete.\" in line for line in lines) or \\\n",
        "           any(f\"Uvicorn running on http://0.0.0.0:{API_PORT}\" in line for line in lines):\n",
        "            logger.info(\"âœ… Uvicorn ë¡œê·¸ì—ì„œ ì„œë²„ ì‹œì‘/ì¤€ë¹„ ë©”ì‹œì§€ í™•ì¸ë¨.\")\n",
        "            server_ready = True\n",
        "        else: logger.warning(\"âš ï¸ Uvicorn ë¡œê·¸ì—ì„œ ëª…í™•í•œ ì‹œì‘/ì¤€ë¹„ ë©”ì‹œì§€ë¥¼ ì°¾ì§€ ëª»í•¨.\")\n",
        "    else: logger.warning(f\"âš ï¸ Uvicorn ë¡œê·¸ íŒŒì¼({UVICORN_LOG_FILE}) ì—†ìŒ.\")\n",
        "\n",
        "    if server_ready: # ë¡œê·¸ì—ì„œ ì‹œì‘í–ˆìœ¼ë©´ ìƒíƒœ ì—”ë“œí¬ì¸íŠ¸ë¡œ ìµœì¢… í™•ì¸\n",
        "        logger.info(\"   - ì„œë²„ ìƒíƒœ í™•ì¸ ì‹œë„ (GET /status)...\")\n",
        "        status_url = f\"http://127.0.0.1:{API_PORT}/status\"\n",
        "        response = requests.get(status_url, timeout=30)\n",
        "        response.raise_for_status(); status_data = response.json()\n",
        "        if status_data.get(\"status\") == \"ready\": logger.info(\"âœ… FastAPI ì„œë²„ ìƒíƒœ 'ready' ìµœì¢… í™•ì¸!\")\n",
        "        else: logger.warning(f\"âš ï¸ FastAPI ì„œë²„ ìƒíƒœê°€ 'ready'ê°€ ì•„ë‹˜: {status_data}\"); server_ready = False\n",
        "    else: logger.error(\"âŒ Uvicorn ë¡œê·¸ì—ì„œ ì„œë²„ ì‹œì‘ì„ í™•ì¸í•  ìˆ˜ ì—†ìŒ.\")\n",
        "\n",
        "except Exception as e:\n",
        "    logger.error(f\"âŒ FastAPI ì„œë²„ ìƒíƒœ í™•ì¸ ì‹¤íŒ¨: {e}\")\n",
        "    logger.error(f\"   - Uvicorn ë¡œê·¸ íŒŒì¼({UVICORN_LOG_FILE})ì„ ì§ì ‘ í™•ì¸í•˜ì„¸ìš”.\")\n",
        "    server_ready = False\n",
        "\n",
        "if not server_ready:\n",
        "    # ë¡œê·¸ íŒŒì¼ ë‚´ìš© ì „ì²´ ì¶œë ¥ (ë””ë²„ê¹… ë„ì›€)\n",
        "    if os.path.exists(UVICORN_LOG_FILE):\n",
        "        logger.error(\"--- ì „ì²´ Uvicorn ë¡œê·¸ ---\")\n",
        "        try:\n",
        "            with open(UVICORN_LOG_FILE, 'r', encoding='utf-8', errors='replace') as f: print(f.read())\n",
        "        except Exception as read_e: print(f\"ë¡œê·¸ íŒŒì¼ ì½ê¸° ì˜¤ë¥˜: {read_e}\")\n",
        "        logger.error(\"-----------------------\")\n",
        "    raise RuntimeError(\"FastAPI ì„œë²„ ì‹œì‘ ë˜ëŠ” ìƒíƒœ í™•ì¸ ì‹¤íŒ¨\")\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "# 5. ngrok í„°ë„ ì‹œì‘\n",
        "# ==============================================================================\n",
        "# FastAPI ì„œë²„ê°€ ì¤€ë¹„ëœ ê²½ìš°ì—ë§Œ ì‹¤í–‰\n",
        "if server_ready and is_google_colab:\n",
        "    logger.info(\"\\nâ˜ï¸ ngrok í„°ë„ ì‹œì‘ ì¤‘...\")\n",
        "    from pyngrok import ngrok, conf\n",
        "\n",
        "    if not NGROK_AUTH_TOKEN:\n",
        "        logger.error(\"âŒ ngrok Authtokenì´ ì—†ìŠµë‹ˆë‹¤! https://dashboard.ngrok.com/get-started/your-authtoken ì—ì„œ í™•ì¸ í›„ ì…ë ¥í•˜ì„¸ìš”.\")\n",
        "        raise ValueError(\"ngrok Authtoken í•„ìš”\")\n",
        "    else:\n",
        "        try:\n",
        "            # ê¸°ì¡´ ngrok í„°ë„/í”„ë¡œì„¸ìŠ¤ ì •ë¦¬\n",
        "            try:\n",
        "                for tunnel in ngrok.get_tunnels(): ngrok.disconnect(tunnel.public_url); logger.info(f\"   - ê¸°ì¡´ ngrok í„°ë„ ì¢…ë£Œ: {tunnel.public_url}\")\n",
        "                ngrok.kill()\n",
        "                time.sleep(2)\n",
        "            except Exception as ng_kill_e: logger.warning(f\"ê¸°ì¡´ ngrok ì¢…ë£Œ ì¤‘ ì˜¤ë¥˜(ë¬´ì‹œ): {ng_kill_e}\")\n",
        "\n",
        "            # Authtoken ì„¤ì •\n",
        "            ngrok.set_auth_token(NGROK_AUTH_TOKEN)\n",
        "            logger.info(\"   - ngrok Authtoken ì„¤ì • ì™„ë£Œ.\")\n",
        "\n",
        "            # í„°ë„ ìƒì„±\n",
        "            logger.info(f\"   - í¬íŠ¸ {API_PORT}ì— ëŒ€í•œ ngrok í„°ë„ ìƒì„± ì‹œë„...\")\n",
        "            # Colab í™˜ê²½ì— ë§ëŠ” ì„¤ì • ì¶”ê°€ (ì„ íƒì )\n",
        "            conf.get_default().region = 'ap' # ì•„ì‹œì•„ íƒœí‰ì–‘ ì§€ì—­ ì„œë²„ ì‚¬ìš© ì‹œë„ (ap, eu, au, sa, jp, in)\n",
        "            # conf.get_default().keep_log_files = True # ë¡œê·¸ íŒŒì¼ ìœ ì§€ (ë””ë²„ê¹…ìš©)\n",
        "\n",
        "            public_url = ngrok.connect(API_PORT, name=f\"fastapi-colab-{time.strftime('%Y%m%d%H%M%S')}\") # ê³ ìœ  ì´ë¦„ ë¶€ì—¬ ì‹œë„\n",
        "            logger.info(f\"âœ… ngrok í„°ë„ URL: {public_url}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"âŒ ngrok í„°ë„ ìƒì„± ì‹¤íŒ¨: {e}\", exc_info=True) # ìƒì„¸ ì˜¤ë¥˜ ì¶œë ¥\n",
        "            public_url = None # ì‹¤íŒ¨ ì‹œ URL ì´ˆê¸°í™”\n",
        "elif not is_google_colab:\n",
        "    logger.info(f\"\\nâ˜ï¸ ë¡œì»¬ ì ‘ì† URL: http://127.0.0.1:{API_PORT}\")\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "# 6. API í…ŒìŠ¤íŠ¸\n",
        "# ==============================================================================\n",
        "# í…ŒìŠ¤íŠ¸ëŠ” URL ì´ ì„±ê³µì ìœ¼ë¡œ ìƒì„±ë˜ì—ˆì„ ë•Œë§Œ ì˜ë¯¸ ìˆìŒ\n",
        "if public_url or (not is_google_colab and server_ready):\n",
        "    logger.info(\"\\nğŸ§ª API ì¶”ë¡  ë° ê´€ë¦¬ í…ŒìŠ¤íŠ¸ ì‹œì‘...\")\n",
        "    target_base_url = public_url if public_url else f\"http://127.0.0.1:{API_PORT}\"\n",
        "    status_url = f\"{target_base_url}/status\"\n",
        "    predictions_url = f\"{target_base_url}/predictions\"\n",
        "    load_lora_url = f\"{target_base_url}/load-lora\"\n",
        "    unload_lora_url = f\"{target_base_url}/unload-lora\"\n",
        "    logger.info(f\"   - í…ŒìŠ¤íŠ¸ ëŒ€ìƒ URL: {target_base_url}\")\n",
        "\n",
        "    # --- í…ŒìŠ¤íŠ¸ 1: ìƒíƒœ ì¡°íšŒ ---\n",
        "    logger.info(\"\\n   --- í…ŒìŠ¤íŠ¸ 1: ìƒíƒœ ì¡°íšŒ ---\")\n",
        "    try:\n",
        "        response = requests.get(status_url, timeout=30)\n",
        "        response.raise_for_status(); logger.info(f\"   - ìƒíƒœ ì¡°íšŒ ì„±ê³µ: {response.json()}\")\n",
        "    except Exception as e: logger.error(f\"   - ìƒíƒœ ì¡°íšŒ ì‹¤íŒ¨: {e}\")\n",
        "\n",
        "    # --- í…ŒìŠ¤íŠ¸ 2: ì¶”ë¡  ---\n",
        "    logger.info(\"\\n   --- í…ŒìŠ¤íŠ¸ 2: ê¸°ë³¸ ì¶”ë¡  ---\")\n",
        "    test_payload = {\n",
        "        \"prompt\": \"photo of a cute corgi wearing sunglasses, cinematic lighting, masterpiece, high detail\",\n",
        "        \"negative_prompt\": \"ugly, deformed, blurry, low quality, text, words, letters, signature\",\n",
        "        \"steps\": 28, \"cfg_scale\": 7.0, \"width\": 1024, \"height\": 1024, \"seed\": 42\n",
        "    }\n",
        "    logger.info(f\"   - ìš”ì²­: {json.dumps(test_payload)}\")\n",
        "    try:\n",
        "        start_infer_time = time.time()\n",
        "        response = requests.post(predictions_url, json=test_payload, timeout=400) # ì‹œê°„ ì¶©ë¶„íˆ\n",
        "        response.raise_for_status(); result = response.json()\n",
        "        end_infer_time = time.time()\n",
        "        if result and \"image_base64\" in result:\n",
        "            logger.info(f\"   - ì¶”ë¡  ì„±ê³µ! (ì†Œìš” ì‹œê°„: {end_infer_time - start_infer_time:.2f}ì´ˆ)\")\n",
        "            img_data = base64.b64decode(result[\"image_base64\"])\n",
        "            img = Image.open(io.BytesIO(img_data)); display(img)\n",
        "        else: logger.error(f\"   - ì¶”ë¡  ì‘ë‹µ í˜•ì‹ ì˜¤ë¥˜: {result}\")\n",
        "    except Exception as e: logger.error(f\"   - ì¶”ë¡  ì‹¤íŒ¨: {e}\")\n",
        "\n",
        "    # --- í…ŒìŠ¤íŠ¸ 3 & 4 & 5 (LoRA ê´€ë ¨) ---\n",
        "    # (ì´ì „ ì½”ë“œì™€ ë™ì¼, NEW_LORA_PATH ì„¤ì • í•„ìš”)\n",
        "    NEW_LORA_PATH = \"\" # <--- í…ŒìŠ¤íŠ¸í•  ë‹¤ë¥¸ LoRA ê²½ë¡œ ì„¤ì •\n",
        "    if INITIAL_LORA_WEIGHTS_PATH or NEW_LORA_PATH:\n",
        "        logger.info(f\"\\n   --- LoRA ë³€ê²½ í…ŒìŠ¤íŠ¸ (NEW_LORA_PATH='{NEW_LORA_PATH}') ---\")\n",
        "        # 1. ì–¸ë¡œë“œ í…ŒìŠ¤íŠ¸ (ì´ˆê¸° LoRAê°€ ìˆì—ˆë‹¤ë©´)\n",
        "        if INITIAL_LORA_WEIGHTS_PATH:\n",
        "            logger.info(\"   - LoRA ì–¸ë¡œë“œ ì‹œë„...\")\n",
        "            try:\n",
        "                resp_unload = requests.post(unload_lora_url, timeout=60)\n",
        "                resp_unload.raise_for_status(); logger.info(f\"     LoRA ì–¸ë¡œë“œ ì‘ë‹µ: {resp_unload.json()}\")\n",
        "            except Exception as e: logger.error(f\"     LoRA ì–¸ë¡œë“œ ì‹¤íŒ¨: {e}\")\n",
        "            time.sleep(2) # ì ìš© ì‹œê°„\n",
        "\n",
        "        # 2. ìƒˆ LoRA ë¡œë“œ í…ŒìŠ¤íŠ¸ (ê²½ë¡œê°€ ì§€ì •ë˜ì—ˆë‹¤ë©´)\n",
        "        if NEW_LORA_PATH and os.path.exists(NEW_LORA_PATH):\n",
        "             logger.info(f\"   - ìƒˆ LoRA ë¡œë“œ ì‹œë„: {NEW_LORA_PATH}\")\n",
        "             try:\n",
        "                 resp_load = requests.post(load_lora_url, json={\"lora_path\": NEW_LORA_PATH}, timeout=120)\n",
        "                 resp_load.raise_for_status(); logger.info(f\"     ìƒˆ LoRA ë¡œë“œ ì‘ë‹µ: {resp_load.json()}\")\n",
        "             except Exception as e: logger.error(f\"     ìƒˆ LoRA ë¡œë“œ ì‹¤íŒ¨: {e}\")\n",
        "             time.sleep(2) # ì ìš© ì‹œê°„\n",
        "\n",
        "             # 3. ìƒˆ LoRA ì ìš© í›„ ì¶”ë¡  í…ŒìŠ¤íŠ¸\n",
        "             logger.info(\"   - ìƒˆ LoRA ì ìš© í›„ ì¶”ë¡  ì‹œë„...\")\n",
        "             try:\n",
        "                 start_infer_time = time.time()\n",
        "                 response = requests.post(predictions_url, json=test_payload, timeout=400) # ë™ì¼ í˜ì´ë¡œë“œ ì‚¬ìš©\n",
        "                 response.raise_for_status(); result = response.json()\n",
        "                 end_infer_time = time.time()\n",
        "                 if result and \"image_base64\" in result:\n",
        "                     logger.info(f\"   - ìƒˆ LoRA ì¶”ë¡  ì„±ê³µ! (ì†Œìš” ì‹œê°„: {end_infer_time - start_infer_time:.2f}ì´ˆ)\")\n",
        "                     img_data = base64.b64decode(result[\"image_base64\"])\n",
        "                     img = Image.open(io.BytesIO(img_data)); display(img)\n",
        "                 else: logger.error(f\"   - ìƒˆ LoRA ì¶”ë¡  ì‘ë‹µ í˜•ì‹ ì˜¤ë¥˜: {result}\")\n",
        "             except Exception as e: logger.error(f\"   - ìƒˆ LoRA ì ìš© í›„ ì¶”ë¡  ì‹¤íŒ¨: {e}\")\n",
        "        elif NEW_LORA_PATH:\n",
        "             logger.warning(f\"   - ìƒˆ LoRA ê²½ë¡œ({NEW_LORA_PATH})ê°€ ìœ íš¨í•˜ì§€ ì•Šì•„ ë¡œë“œ í…ŒìŠ¤íŠ¸ ê±´ë„ˆ<0xEB><0x9C><0x8D>.\")\n",
        "\n",
        "else:\n",
        "    logger.warning(\"âš ï¸ API í…ŒìŠ¤íŠ¸ë¥¼ ê±´ë„ˆ<0xEB><0x9C><0x8D>ë‹ˆë‹¤ (URL ìƒì„± ì‹¤íŒ¨ ë˜ëŠ” ì„œë²„ ë¯¸ì¤€ë¹„).\")\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "# 7. ì„œë²„ ì •ë³´ ë° ì¢…ë£Œ ì•ˆë‚´\n",
        "# ==============================================================================\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"ğŸ‰ ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰ ì™„ë£Œ ğŸ‰\")\n",
        "print(\"=\"*60)\n",
        "print(\"=\"*60)\n",
        "# server_ready ë³€ìˆ˜ëŠ” ì½”ë“œ ë¸”ë¡ 4 ëì—ì„œ FastAPI ì„œë²„ ìƒíƒœì— ë”°ë¼ True ë˜ëŠ” Falseë¡œ ì„¤ì •ë¨\n",
        "# server_ok ëŒ€ì‹  server_ready ë³€ìˆ˜ë¥¼ ì§ì ‘ ì‚¬ìš©í•©ë‹ˆë‹¤.\n",
        "\n",
        "# FastAPI ì„œë²„ê°€ ì„±ê³µì ìœ¼ë¡œ ì¤€ë¹„ë˜ì—ˆëŠ”ì§€ í™•ì¸\n",
        "if 'server_ready' in locals() and server_ready: # server_ready ë³€ìˆ˜ê°€ ì¡´ì¬í•˜ê³  True ì¸ì§€ í™•ì¸\n",
        "    if public_url: print(f\"âœ… ì™¸ë¶€ ì ‘ì† URL (ngrok): {public_url}\")\n",
        "    else: print(f\"âš ï¸ ngrok URL ìƒì„± ì‹¤íŒ¨ ë˜ëŠ” Colab í™˜ê²½ ì•„ë‹˜.\")\n",
        "    print(f\"\\nğŸ”— API ì—”ë“œí¬ì¸íŠ¸:\")\n",
        "    print(f\"  - ì¶”ë¡  (POST) : {target_base_url}/predictions\")\n",
        "    print(f\"  - ìƒíƒœ (GET)  : {target_base_url}/status\")\n",
        "    print(f\"  - LoRA ë¡œë“œ(POST): {target_base_url}/load-lora\")\n",
        "    print(f\"  - LoRAì–¸ë¡œë“œ(POST): {target_base_url}/unload-lora\")\n",
        "    print(f\"\\nğŸ“„ Uvicorn ë¡œê·¸ íŒŒì¼: {UVICORN_LOG_FILE}\")\n",
        "    print(\"\\nğŸš€ FastAPI ì„œë²„ì™€ ngrok í„°ë„(ì‹¤í–‰ëœ ê²½ìš°)ì´ ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì‹¤í–‰ ì¤‘ì…ë‹ˆë‹¤.\")\n",
        "    print(\"   - APIë¥¼ ê³„ì† ì‚¬ìš©í•˜ë ¤ë©´ ì´ Colab ë…¸íŠ¸ë¶ ì„¸ì…˜ì„ í™œì„± ìƒíƒœë¡œ ìœ ì§€í•˜ì„¸ìš”.\")\n",
        "    print(\"   - ì„œë²„/í„°ë„ì„ ì¤‘ì§€í•˜ë ¤ë©´ ì•„ë˜ 8ë²ˆ ì…€ì˜ ì£¼ì„ì„ í•´ì œí•˜ê³  ì‹¤í–‰í•˜ì„¸ìš”.\")\n",
        "else:\n",
        "    print(\"\\nâŒ ì„œë²„ ì‹œì‘ ë˜ëŠ” ëª¨ë¸ ì¤€ë¹„ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.\")\n",
        "    print(\"   - ìœ„ì˜ ë¡œê·¸ë¥¼ ê²€í† í•˜ì—¬ ì›ì¸ì„ í™•ì¸í•˜ì„¸ìš”.\")\n",
        "    print(\"   - íŠ¹íˆ Uvicorn ë¡œê·¸ íŒŒì¼ì„ í™•ì¸í•˜ëŠ” ê²ƒì´ ë„ì›€ì´ ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\")\n",
        "\n",
        "# ==============================================================================\n",
        "# 8. ì„œë²„ ë° í„°ë„ ì¢…ë£Œ (ì£¼ì„ í•´ì œ í›„ ì‹¤í–‰)\n",
        "# ==============================================================================\n",
        "# logger.info(\"\\nâ¹ï¸ FastAPI ì„œë²„ ë° ngrok í„°ë„ ì¢…ë£Œ ì¤‘...\")\n",
        "# try:\n",
        "#     from pyngrok import ngrok\n",
        "#     logger.info(\"   - ngrok í„°ë„ ì¢…ë£Œ ì‹œë„...\")\n",
        "#     ngrok.kill()\n",
        "#     logger.info(\"   - ngrok í”„ë¡œì„¸ìŠ¤ ì¢…ë£Œ ì™„ë£Œ.\")\n",
        "# except ImportError: logger.warning(\"   - pyngrok ë¯¸ì„¤ì¹˜ë¨.\")\n",
        "# except Exception as e: logger.error(f\"   - ngrok ì¢…ë£Œ ì¤‘ ì˜¤ë¥˜: {e}\")\n",
        "#\n",
        "# logger.info(\"   - Uvicorn í”„ë¡œì„¸ìŠ¤ ì¢…ë£Œ ì‹œë„ (pkill)...\")\n",
        "# # pkillì´ í•­ìƒ ì„±ê³µí•˜ëŠ” ê²ƒì€ ì•„ë‹˜\n",
        "# result_pkill = subprocess.run(['pkill', '-f', 'uvicorn main:app'], capture_output=True)\n",
        "# logger.info(f\"   - pkill ê²°ê³¼: {result_pkill.returncode}\")\n",
        "# time.sleep(3)\n",
        "# logger.info(\"âœ… ì„œë²„ ì¢…ë£Œ ì™„ë£Œ ì‹œë„.\")\n",
        "# # Uvicorn í”„ë¡œì„¸ìŠ¤ê°€ ì—¬ì „íˆ ì‚´ì•„ìˆëŠ”ì§€ í™•ì¸ (ì„ íƒì )\n",
        "# result_pgrep = subprocess.run(['pgrep', '-f', 'uvicorn main:app'], capture_output=True)\n",
        "# if result_pgrep.stdout: logger.warning(\"   - Uvicorn í”„ë¡œì„¸ìŠ¤ê°€ ì—¬ì „íˆ ì‹¤í–‰ ì¤‘ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fJlUUH5CsPYd",
        "outputId": "49d0f0bc-accb-4568-ded3-292278eb7912"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:pyngrok.process.ngrok:t=2025-04-18T07:37:24+0000 lvl=warn msg=\"Stopping forwarder\" name=fastapi-colab-20250418071242 acceptErr=\"failed to accept connection: Listener closed\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Uvicorn ë¡œê·¸ (ë§ˆì§€ë§‰ 20ì¤„) ---\n",
            "E0000 00:00:1744961699.488144   47255 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1744961699.494652   47255 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-04-18 07:34:59.516337: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "INFO:     Started server process [47255]\n",
            "INFO:     Waiting for application startup.\n",
            "INFO:     Lifespan: ì• í”Œë¦¬ì¼€ì´ì…˜ ì‹œì‘ - ëª¨ë¸ ë¡œë”©...\n",
            "INFO:     Lifespan: Device=cuda, Dtype=torch.float16\n",
            "INFO:     Lifespan: ë©”ëª¨ë¦¬ (ë¡œë”© ì „) Allocated=0.00GB, Reserved=0.00GB\n",
            "\n",
            "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]\n",
            "Loading pipeline components...:  29%|â–ˆâ–ˆâ–Š       | 2/7 [00:00<00:00, 14.82it/s]\n",
            "Loading pipeline components...:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 5/7 [00:00<00:00, 18.40it/s]\n",
            "Loading pipeline components...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:01<00:00,  4.17it/s]\n",
            "Loading pipeline components...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:01<00:00,  5.26it/s]\n",
            "INFO:     Lifespan: ëª¨ë¸ íŒŒì´í”„ë¼ì¸ ë¡œë”© ì™„ë£Œ. Device=cuda:0\n",
            "INFO:     Lifespan: ë©”ëª¨ë¦¬ (íŒŒì´í”„ë¼ì¸ ë¡œë”© í›„) Allocated=7.06GB, Reserved=7.35GB\n",
            "INFO:     Lifespan: ëª¨ë¸ ë° ì´ˆê¸° LoRA ë¡œë”© ì™„ë£Œ. (ì†Œìš” ì‹œê°„: 4.18ì´ˆ)\n",
            "INFO:     Application startup complete.\n",
            "INFO:     Uvicorn running on http://0.0.0.0:9080 (Press CTRL+C to quit)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:__main__:   - ìƒíƒœ ì¡°íšŒ ì‹¤íŒ¨: No connection adapters were found for 'NgrokTunnel: \"https://172f-35-247-173-221.ngrok-free.app\" -> \"http://localhost:9080\"/status'\n",
            "ERROR:__main__:   - ì¶”ë¡  ì‹¤íŒ¨: No connection adapters were found for 'NgrokTunnel: \"https://172f-35-247-173-221.ngrok-free.app\" -> \"http://localhost:9080\"/predictions'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "ğŸ‰ ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰ ì™„ë£Œ ğŸ‰\n",
            "============================================================\n",
            "============================================================\n",
            "âœ… ì™¸ë¶€ ì ‘ì† URL (ngrok): NgrokTunnel: \"https://172f-35-247-173-221.ngrok-free.app\" -> \"http://localhost:9080\"\n",
            "\n",
            "ğŸ”— API ì—”ë“œí¬ì¸íŠ¸:\n",
            "  - ì¶”ë¡  (POST) : NgrokTunnel: \"https://172f-35-247-173-221.ngrok-free.app\" -> \"http://localhost:9080\"/predictions\n",
            "  - ìƒíƒœ (GET)  : NgrokTunnel: \"https://172f-35-247-173-221.ngrok-free.app\" -> \"http://localhost:9080\"/status\n",
            "  - LoRA ë¡œë“œ(POST): NgrokTunnel: \"https://172f-35-247-173-221.ngrok-free.app\" -> \"http://localhost:9080\"/load-lora\n",
            "  - LoRAì–¸ë¡œë“œ(POST): NgrokTunnel: \"https://172f-35-247-173-221.ngrok-free.app\" -> \"http://localhost:9080\"/unload-lora\n",
            "\n",
            "ğŸ“„ Uvicorn ë¡œê·¸ íŒŒì¼: /content/uvicorn.log\n",
            "\n",
            "ğŸš€ FastAPI ì„œë²„ì™€ ngrok í„°ë„(ì‹¤í–‰ëœ ê²½ìš°)ì´ ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì‹¤í–‰ ì¤‘ì…ë‹ˆë‹¤.\n",
            "   - APIë¥¼ ê³„ì† ì‚¬ìš©í•˜ë ¤ë©´ ì´ Colab ë…¸íŠ¸ë¶ ì„¸ì…˜ì„ í™œì„± ìƒíƒœë¡œ ìœ ì§€í•˜ì„¸ìš”.\n",
            "   - ì„œë²„/í„°ë„ì„ ì¤‘ì§€í•˜ë ¤ë©´ ì•„ë˜ 8ë²ˆ ì…€ì˜ ì£¼ì„ì„ í•´ì œí•˜ê³  ì‹¤í–‰í•˜ì„¸ìš”.\n"
          ]
        }
      ]
    }
  ]
}