{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# # -*- coding: utf-8 -*-\n",
        "# \"\"\"\n",
        "# FastAPI + Diffusers + Cloudflared API 배포 (Colab 기반)\n",
        "\n",
        "# - FastAPI: API 서버 구축\n",
        "# - Diffusers: Stable Diffusion 모델 추론\n",
        "# - Cloudflared: 외부 인터넷 노출\n",
        "# - 동적 LoRA 로딩/언로딩 API 포함\n",
        "# \"\"\"\n",
        "\n",
        "# # ==============================================================================\n",
        "# # 0. 환경 설정 및 변수 정의\n",
        "# # ==============================================================================\n",
        "# import os\n",
        "# import subprocess\n",
        "# import json\n",
        "# import time\n",
        "# import threading\n",
        "# import requests\n",
        "# from google.colab import drive, files\n",
        "# from IPython.display import display, Image as IPyImage, clear_output\n",
        "# import logging\n",
        "\n",
        "# # --- 사용자 설정 ---\n",
        "# # Hugging Face Hub 또는 로컬 경로에서 불러올 Diffusers 모델 ID\n",
        "# MODEL_ID = \"stabilityai/stable-diffusion-xl-base-1.0\"\n",
        "\n",
        "# # (선택 사항) 초기 로드할 LoRA 가중치 파일 경로 (Google Drive 내)\n",
        "# # 비워두면 초기 LoRA 없이 시작\n",
        "# INITIAL_LORA_WEIGHTS_PATH = \"\" # 예: \"/content/drive/MyDrive/Loras/YourLora/output/YourLora-000010.safetensors\"\n",
        "\n",
        "# # API 서버 포트 설정\n",
        "# API_PORT = \"9080\"\n",
        "\n",
        "# # --- 내부 설정 ---\n",
        "# FASTAPI_APP_FILE = \"/content/main.py\"\n",
        "# REQUIREMENTS_PATH = \"/content/requirements_fastapi.txt\"\n",
        "# UVICORN_LOG_FILE = \"/content/uvicorn.log\"\n",
        "# is_google_colab = 'google.colab' in str(get_ipython())\n",
        "# cloudflared_process = None\n",
        "# cloudflared_url = None\n",
        "# uvicorn_process = None # Uvicorn 프로세스 추적용\n",
        "\n",
        "# # 로깅 설정\n",
        "# logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
        "# logger = logging.getLogger(__name__)\n",
        "\n",
        "# # ==============================================================================\n",
        "# # 1. Google Drive 마운트 (LoRA 사용 시)\n",
        "# # ==============================================================================\n",
        "# if is_google_colab:\n",
        "#     logger.info(\"📂 Google Drive 마운트 중...\")\n",
        "#     try:\n",
        "#         drive.mount('/content/drive', force_remount=True)\n",
        "#         logger.info(\"✅ Google Drive 마운트 완료.\")\n",
        "#         if INITIAL_LORA_WEIGHTS_PATH and not os.path.exists(INITIAL_LORA_WEIGHTS_PATH):\n",
        "#              logger.error(f\"초기 LoRA 파일 경로 오류: {INITIAL_LORA_WEIGHTS_PATH}\")\n",
        "#         elif INITIAL_LORA_WEIGHTS_PATH:\n",
        "#             logger.info(f\"✅ 초기 LoRA 파일 확인: {INITIAL_LORA_WEIGHTS_PATH}\")\n",
        "#     except Exception as e:\n",
        "#         logger.error(f\"Google Drive 마운트 오류: {e}\")\n",
        "# else:\n",
        "#     logger.warning(\"Colab 환경 아님. Drive 마운트 생략.\")\n",
        "\n",
        "# # ==============================================================================\n",
        "# # 2. 필요 패키지 설치 및 Cloudflared\n",
        "# # ==============================================================================\n",
        "# logger.info(\"\\n⚙️ 필요한 패키지 설치 및 Cloudflared 준비 중...\")\n",
        "\n",
        "# # requirements_fastapi.txt 생성\n",
        "# requirements_content = \"\"\"\n",
        "# fastapi\n",
        "# uvicorn[standard] # ASGI 서버 (standard는 추가 기능 포함)\n",
        "# python-multipart # FastAPI에서 폼 데이터 처리 등에 필요할 수 있음\n",
        "# requests # 테스트용\n",
        "\n",
        "# torch>=2.0.0\n",
        "# # torchvision # 핸들러 직접 사용 안 함\n",
        "# # torchaudio # 핸들러 직접 사용 안 함\n",
        "# diffusers>=0.24.0\n",
        "# transformers>=4.30.0\n",
        "# accelerate>=0.20.0\n",
        "# safetensors>=0.3.0\n",
        "# invisible-watermark>=0.2.0\n",
        "# pillow\n",
        "# python-dotenv # 환경변수 관리 (선택적)\n",
        "# \"\"\"\n",
        "# with open(REQUIREMENTS_PATH, 'w') as f: f.write(requirements_content)\n",
        "# logger.info(f\"✅ requirements_fastapi.txt 생성 완료: {REQUIREMENTS_PATH}\")\n",
        "\n",
        "# # pip install\n",
        "# logger.info(f\"   - pip install 실행: {REQUIREMENTS_PATH}\")\n",
        "# try:\n",
        "#     # 이전 실행의 uvicorn 등 프로세스 종료 시도 (선택적)\n",
        "#     # subprocess.run(['pkill', '-f', 'uvicorn'], capture_output=True)\n",
        "#     pip_process = subprocess.run(['pip', 'install', '-r', REQUIREMENTS_PATH, '-qq'],\n",
        "#                                  check=True, capture_output=True, text=True, encoding='utf-8')\n",
        "#     logger.info(\"✅ Python 패키지 설치 완료.\")\n",
        "# except subprocess.CalledProcessError as e:\n",
        "#     logger.error(f\"💥 pip install 실패! (종료 코드: {e.returncode})\")\n",
        "#     print(\"--- pip stdout ---\\n\", e.stdout)\n",
        "#     print(\"--- pip stderr ---\\n\", e.stderr)\n",
        "#     raise e\n",
        "\n",
        "# # Cloudflared 설치\n",
        "# if is_google_colab:\n",
        "#     logger.info(\"   - Cloudflared 다운로드 및 설치 중...\")\n",
        "#     try:\n",
        "#         if os.path.exists(\"cloudflared-linux-amd64.deb\"): os.remove(\"cloudflared-linux-amd64.deb\")\n",
        "#         subprocess.run(['wget', '-q', 'https://github.com/cloudflare/cloudflared/releases/latest/download/cloudflared-linux-amd64.deb'], check=True)\n",
        "#         time.sleep(1)\n",
        "#         subprocess.run(['dpkg', '-i', 'cloudflared-linux-amd64.deb'], check=True, capture_output=True)\n",
        "#         logger.info(\"✅ Cloudflared 설치 완료.\")\n",
        "#     except Exception as e: logger.error(f\"Cloudflared 설치 오류: {e}\")\n",
        "# else: logger.warning(\"Colab 환경 아님. Cloudflared 설치 생략.\")\n",
        "\n",
        "# # ==============================================================================\n",
        "# # 3. FastAPI 앱 스크립트 생성 (main.py)\n",
        "# # ==============================================================================\n",
        "# logger.info(f\"\\n📝 FastAPI 앱 스크립트 생성 중 ({FASTAPI_APP_FILE})...\")\n",
        "\n",
        "# fastapi_app_content = '''\n",
        "# import logging\n",
        "# import os\n",
        "# import base64\n",
        "# import io\n",
        "# import time\n",
        "# import torch\n",
        "# from diffusers import DiffusionPipeline, DPMSolverMultistepScheduler\n",
        "# from fastapi import FastAPI, HTTPException, Request\n",
        "# from pydantic import BaseModel, Field\n",
        "# from PIL import Image\n",
        "# import asyncio # 비동기 Lock 및 스레드 실행용\n",
        "# from contextlib import asynccontextmanager\n",
        "# from fastapi import FastAPI\n",
        "# from fastapi.responses import JSONResponse, Response\n",
        "\n",
        "# # --- 기본 설정 ---\n",
        "# # 환경 변수 또는 기본값 사용 (스크립트 시작 시 설정된 값 활용)\n",
        "# MODEL_ID = os.environ.get(\"TS_MODEL_ID\", \"stabilityai/stable-diffusion-xl-base-1.0\")\n",
        "# INITIAL_LORA_PATH = os.environ.get(\"TS_INITIAL_LORA_PATH\", None)\n",
        "# ADAPTER_NAME = \"default\" # 사용할 LoRA 어댑터 이름\n",
        "\n",
        "# # 로깅 설정\n",
        "# logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
        "# logger = logging.getLogger(\"uvicorn\") # Uvicorn 로거 사용 권장\n",
        "\n",
        "# # --- 모델 및 상태 관리 ---\n",
        "# # 전역 변수 대신 context manager 또는 클래스 사용 권장되나, 여기서는 간단하게 전역 사용\n",
        "# pipeline = None\n",
        "# current_lora_path = None\n",
        "# model_lock = asyncio.Lock() # 파이프라인 접근 제어용 Lock\n",
        "\n",
        "# # --- FastAPI 앱 생명주기 관리 (모델 로딩/언로딩) ---\n",
        "# @asynccontextmanager\n",
        "# async def lifespan(app: FastAPI):\n",
        "#     global pipeline, current_lora_path, model_lock\n",
        "#     logger.info(\"애플리케이션 시작: 모델 로딩 시작...\")\n",
        "#     start_time = time.time()\n",
        "#     try:\n",
        "#         if torch.cuda.is_available():\n",
        "#             device = \"cuda\"\n",
        "#             torch_dtype = torch.float16\n",
        "#             logger.info(f\"사용 가능 GPU 감지됨. Device: {device}, Dtype: {torch_dtype}\")\n",
        "#         else:\n",
        "#             device = \"cpu\"\n",
        "#             torch_dtype = torch.float32 # CPU에서는 float32 사용\n",
        "#             logger.info(f\"GPU 사용 불가. Device: {device}, Dtype: {torch_dtype}\")\n",
        "\n",
        "#         if torch.cuda.is_available(): logger.info(f\"메모리 (로딩 전): 할당됨 {torch.cuda.memory_allocated(device)/1e9:.2f} GB, 예약됨 {torch.cuda.memory_reserved(device)/1e9:.2f} GB\")\n",
        "\n",
        "#         temp_pipeline = DiffusionPipeline.from_pretrained(\n",
        "#             MODEL_ID,\n",
        "#             torch_dtype=torch_dtype,\n",
        "#             variant=\"fp16\" if torch_dtype == torch.float16 else None, # GPU 사용 시 FP16 변형 사용\n",
        "#             use_safetensors=True\n",
        "#         )\n",
        "#         temp_pipeline.scheduler = DPMSolverMultistepScheduler.from_config(temp_pipeline.scheduler.config, use_karras_sigmas=True)\n",
        "#         temp_pipeline.to(device)\n",
        "#         logger.info(f\"모델 파이프라인 로딩 완료. Device: {temp_pipeline.device}\")\n",
        "\n",
        "#         if torch.cuda.is_available(): logger.info(f\"메모리 (파이프라인 로딩 후): 할당됨 {torch.cuda.memory_allocated(device)/1e9:.2f} GB, 예약됨 {torch.cuda.memory_reserved(device)/1e9:.2f} GB\")\n",
        "\n",
        "#         pipeline = temp_pipeline # 전역 변수에 할당\n",
        "\n",
        "#         # 초기 LoRA 로드\n",
        "#         if INITIAL_LORA_PATH and os.path.exists(INITIAL_LORA_PATH):\n",
        "#             logger.info(f\"초기 LoRA 로딩 시도: {INITIAL_LORA_PATH}\")\n",
        "#             try:\n",
        "#                 async with model_lock: # Lock 사용\n",
        "#                     # 비동기 함수 내에서 동기 함수 실행 (모델 로딩은 블로킹 작업)\n",
        "#                     await asyncio.to_thread(load_lora_blocking, INITIAL_LORA_PATH)\n",
        "#                 logger.info(f\"초기 LoRA 로딩 성공: {INITIAL_LORA_PATH}\")\n",
        "#                 current_lora_path = INITIAL_LORA_PATH\n",
        "#                 if torch.cuda.is_available(): logger.info(f\"메모리 (LoRA 로딩 후): 할당됨 {torch.cuda.memory_allocated(device)/1e9:.2f} GB, 예약됨 {torch.cuda.memory_reserved(device)/1e9:.2f} GB\")\n",
        "#             except Exception as e:\n",
        "#                  logger.error(f\"초기 LoRA 로딩 실패: {e}\", exc_info=True)\n",
        "#         elif INITIAL_LORA_PATH:\n",
        "#              logger.warning(f\"초기 LoRA 파일({INITIAL_LORA_PATH}) 없음.\")\n",
        "\n",
        "#         loading_time = time.time() - start_time\n",
        "#         logger.info(f\"모델 및 초기 LoRA 로딩 완료. (소요 시간: {loading_time:.2f}초)\")\n",
        "\n",
        "#     except Exception as e:\n",
        "#         logger.error(f\"모델 로딩 중 치명적 오류 발생: {e}\", exc_info=True)\n",
        "#         pipeline = None # 로딩 실패 시 None 유지\n",
        "\n",
        "#     yield # 애플리케이션 실행 구간\n",
        "\n",
        "#     # --- 애플리케이션 종료 시 ---\n",
        "#     logger.info(\"애플리케이션 종료: 모델 정리...\")\n",
        "#     async with model_lock: # Lock 사용\n",
        "#         pipeline = None\n",
        "#         current_lora_path = None\n",
        "#     if torch.cuda.is_available():\n",
        "#         torch.cuda.empty_cache()\n",
        "#         logger.info(\"GPU 캐시 정리 완료.\")\n",
        "#     logger.info(\"애플리케이션 종료 완료.\")\n",
        "\n",
        "\n",
        "# app = FastAPI(lifespan=lifespan)\n",
        "\n",
        "# # --- Pydantic 모델 정의 ---\n",
        "# class InferenceRequest(BaseModel):\n",
        "#     prompt: str\n",
        "#     negative_prompt: str = \"\"\n",
        "#     height: int = 1024\n",
        "#     width: int = 1024\n",
        "#     steps: int = Field(30, gt=0, le=100) # 1 ~ 100 사이 값\n",
        "#     cfg_scale: float = Field(7.5, gt=0.0, le=20.0) # 0 초과 ~ 20 이하 값\n",
        "#     seed: int | None = None\n",
        "\n",
        "# class InferenceResponse(BaseModel):\n",
        "#     image_base64: str\n",
        "#     generation_time_ms: int\n",
        "#     model_id: str\n",
        "#     current_lora: str | None\n",
        "\n",
        "# class LoraRequest(BaseModel):\n",
        "#     lora_path: str # Google Drive 내 절대 경로 등\n",
        "\n",
        "# class StatusResponse(BaseModel):\n",
        "#     status: str\n",
        "#     model_id: str | None\n",
        "#     device: str | None\n",
        "#     current_lora_path: str | None\n",
        "#     active_adapters: list | None # 활성화된 어댑터 목록 (디버깅용)\n",
        "\n",
        "# # --- Helper Functions (Blocking tasks) ---\n",
        "# # 동기 함수들은 asyncio.to_thread 로 호출해야 함\n",
        "\n",
        "# def load_lora_blocking(lora_path: str):\n",
        "#     global pipeline, current_lora_path # 전역 변수 사용 시 명시\n",
        "#     if not pipeline: raise RuntimeError(\"파이프라인이 초기화되지 않았습니다.\")\n",
        "#     if not os.path.exists(lora_path): raise FileNotFoundError(f\"LoRA 파일 없음: {lora_path}\")\n",
        "\n",
        "#     logger.info(f\"LoRA 로딩 (동기): {lora_path}\")\n",
        "#     pipeline.to(pipeline.device) # 장치 확인\n",
        "\n",
        "#     # 기존 어댑터 언로드 시도\n",
        "#     try:\n",
        "#         active_adapters = getattr(pipeline, \"get_active_adapters\", lambda: [])()\n",
        "#         if ADAPTER_NAME in active_adapters:\n",
        "#             logger.info(f\"기존 '{ADAPTER_NAME}' 언로드 시도...\")\n",
        "#             if hasattr(pipeline, 'unload_lora_weights'): pipeline.unload_lora_weights(adapter_names=[ADAPTER_NAME])\n",
        "#             elif hasattr(pipeline, 'delete_adapters'): pipeline.delete_adapters(ADAPTER_NAME)\n",
        "#     except Exception as e: logger.warning(f\"기존 LoRA 언로드 중 경고(무시): {e}\")\n",
        "\n",
        "#     logger.info(f\"새 LoRA 로딩: '{lora_path}' ('{ADAPTER_NAME}')...\")\n",
        "#     pipeline.load_lora_weights(os.path.dirname(lora_path), weight_name=os.path.basename(lora_path), adapter_name=ADAPTER_NAME)\n",
        "#     logger.info(f\"LoRA 로딩 성공: {lora_path}\")\n",
        "\n",
        "#     if hasattr(pipeline, 'set_adapters'): pipeline.set_adapters([ADAPTER_NAME], adapter_weights=[1.0])\n",
        "#     elif hasattr(pipeline, 'fuse_lora'): pipeline.fuse_lora(adapter_names=[ADAPTER_NAME])\n",
        "\n",
        "#     current_lora_path = lora_path # 상태 업데이트\n",
        "#     if torch.cuda.is_available(): torch.cuda.empty_cache()\n",
        "\n",
        "# def unload_lora_blocking():\n",
        "#     global pipeline, current_lora_path\n",
        "#     if not pipeline: raise RuntimeError(\"파이프라인이 초기화되지 않았습니다.\")\n",
        "#     if not current_lora_path: return \"현재 로드된 LoRA 없음.\"\n",
        "\n",
        "#     logger.info(f\"LoRA 언로드 (동기): {current_lora_path}\")\n",
        "#     pipeline.to(pipeline.device)\n",
        "#     unloaded_path = current_lora_path\n",
        "#     try:\n",
        "#         if hasattr(pipeline, 'unload_lora_weights'): pipeline.unload_lora_weights(adapter_names=[ADAPTER_NAME])\n",
        "#         elif hasattr(pipeline, 'delete_adapters'): pipeline.delete_adapters(ADAPTER_NAME)\n",
        "#         else:\n",
        "#              if hasattr(pipeline, 'set_adapters'): pipeline.set_adapters([])\n",
        "#              elif hasattr(pipeline, 'unfuse_lora'): pipeline.unfuse_lora()\n",
        "#         logger.info(f\"성공적으로 LoRA 언로드/비활성화: {unloaded_path}\")\n",
        "#         current_lora_path = None\n",
        "#         if torch.cuda.is_available(): torch.cuda.empty_cache()\n",
        "#         return f\"성공적으로 LoRA 언로드: {unloaded_path}\"\n",
        "#     except Exception as e:\n",
        "#          logger.error(f\"LoRA 언로드 중 오류: {e}\", exc_info=True)\n",
        "#          current_lora_path = None # 오류 시에도 상태는 초기화\n",
        "#          raise RuntimeError(f\"LoRA 언로드 실패: {e}\") from e\n",
        "\n",
        "# def generate_image_blocking(req: InferenceRequest) -> Image.Image:\n",
        "#     global pipeline\n",
        "#     if not pipeline: raise RuntimeError(\"파이프라인이 초기화되지 않았습니다.\")\n",
        "\n",
        "#     logger.info(f\"이미지 생성 시작 (동기): prompt='{req.prompt[:50]}...'\")\n",
        "#     params = {\n",
        "#         \"negative_prompt\": req.negative_prompt,\n",
        "#         \"num_inference_steps\": req.steps,\n",
        "#         \"guidance_scale\": req.cfg_scale,\n",
        "#         \"height\": req.height,\n",
        "#         \"width\": req.width,\n",
        "#     }\n",
        "#     if req.seed is not None:\n",
        "#         params[\"generator\"] = torch.Generator(device=pipeline.device).manual_seed(req.seed)\n",
        "\n",
        "#     with torch.inference_mode():\n",
        "#         result_image = pipeline(prompt=req.prompt, **params).images[0]\n",
        "#     logger.info(\"이미지 생성 완료 (동기)\")\n",
        "#     return result_image\n",
        "\n",
        "\n",
        "# # --- API 엔드포인트 ---\n",
        "# @app.get(\"/\", include_in_schema=False)\n",
        "# async def root_health():\n",
        "#     # 단순히 200 OK 리턴\n",
        "#     return JSONResponse({\"status\": \"ok\"}, status_code=200)\n",
        "\n",
        "# @app.get(\"/favicon.ico\", include_in_schema=False)\n",
        "# async def favicon():\n",
        "#     return Response(status_code=204)\n",
        "\n",
        "# @app.get(\"/status\", response_model=StatusResponse)\n",
        "# async def get_status():\n",
        "#     \"\"\"현재 서버 상태 (로드된 모델, LoRA 등) 반환\"\"\"\n",
        "#     async with model_lock: # 상태 읽기 시에도 Lock 사용 (일관성)\n",
        "#         if not pipeline:\n",
        "#             return StatusResponse(status=\"error\", model_id=MODEL_ID, device=None, current_lora_path=None, active_adapters=None)\n",
        "\n",
        "#         active_adapters = []\n",
        "#         try:\n",
        "#             if hasattr(pipeline, 'get_active_adapters'): active_adapters = pipeline.get_active_adapters()\n",
        "#             elif hasattr(pipeline, 'get_list_adapters'): active_adapters = [name for name, enabled in pipeline.get_list_adapters().items() if enabled]\n",
        "#         except Exception as e: logger.warning(f\"활성 어댑터 가져오기 실패: {e}\")\n",
        "\n",
        "#         return StatusResponse(\n",
        "#             status=\"ready\" if pipeline else \"initializing_error\",\n",
        "#             model_id=MODEL_ID,\n",
        "#             device=str(pipeline.device) if pipeline else None,\n",
        "#             current_lora_path=current_lora_path,\n",
        "#             active_adapters=active_adapters\n",
        "#         )\n",
        "\n",
        "# @app.post(\"/predictions\", response_model=InferenceResponse)\n",
        "# async def predict(request: InferenceRequest):\n",
        "#     \"\"\"이미지 생성 요청 처리\"\"\"\n",
        "#     global pipeline\n",
        "#     if not pipeline:\n",
        "#         raise HTTPException(status_code=503, detail=\"모델 파이프라인이 준비되지 않았습니다.\")\n",
        "\n",
        "#     start_time = time.time()\n",
        "#     try:\n",
        "#         # 블로킹 I/O 또는 CPU/GPU 바운드 작업을 별도 스레드에서 실행\n",
        "#         pil_image = await asyncio.to_thread(generate_image_blocking, request)\n",
        "\n",
        "#         # PIL Image -> Base64\n",
        "#         buffered = io.BytesIO()\n",
        "#         pil_image.save(buffered, format=\"PNG\")\n",
        "#         img_str = base64.b64encode(buffered.getvalue()).decode(\"utf-8\")\n",
        "\n",
        "#         end_time = time.time()\n",
        "#         generation_time_ms = int((end_time - start_time) * 1000)\n",
        "\n",
        "#         # 응답 생성 전에 현재 LoRA 상태 확인 (Lock 내부에서 변경될 수 있으므로 다시 확인)\n",
        "#         async with model_lock:\n",
        "#             lora_in_use = current_lora_path\n",
        "\n",
        "#         return InferenceResponse(\n",
        "#             image_base64=img_str,\n",
        "#             generation_time_ms=generation_time_ms,\n",
        "#             model_id=MODEL_ID,\n",
        "#             current_lora=lora_in_use\n",
        "#         )\n",
        "#     except Exception as e:\n",
        "#         logger.error(f\"추론 중 오류 발생: {e}\", exc_info=True)\n",
        "#         raise HTTPException(status_code=500, detail=f\"추론 실패: {str(e)}\")\n",
        "\n",
        "# @app.post(\"/load-lora\", status_code=200)\n",
        "# async def load_lora(request: LoraRequest):\n",
        "#     \"\"\"지정된 경로의 LoRA 가중치를 로드\"\"\"\n",
        "#     global pipeline\n",
        "#     if not pipeline:\n",
        "#         raise HTTPException(status_code=503, detail=\"모델 파이프라인이 준비되지 않았습니다.\")\n",
        "\n",
        "#     async with model_lock: # Lock을 사용하여 동시 접근 방지\n",
        "#         try:\n",
        "#             logger.info(f\"LoRA 로드 요청 수신: {request.lora_path}\")\n",
        "#             # 블로킹 작업을 별도 스레드에서 실행\n",
        "#             message = await asyncio.to_thread(load_lora_blocking, request.lora_path)\n",
        "#             logger.info(f\"LoRA 로드 요청 처리 완료.\")\n",
        "#             return {\"status\": \"success\", \"message\": message}\n",
        "#         except FileNotFoundError as e:\n",
        "#             logger.error(f\"LoRA 파일 없음 오류: {e}\")\n",
        "#             raise HTTPException(status_code=404, detail=str(e))\n",
        "#         except Exception as e:\n",
        "#             logger.error(f\"LoRA 로드 중 오류: {e}\", exc_info=True)\n",
        "#             raise HTTPException(status_code=500, detail=f\"LoRA 로드 실패: {str(e)}\")\n",
        "\n",
        "# @app.post(\"/unload-lora\", status_code=200)\n",
        "# async def unload_lora():\n",
        "#     \"\"\"현재 로드된 LoRA 가중치를 언로드\"\"\"\n",
        "#     global pipeline\n",
        "#     if not pipeline:\n",
        "#         raise HTTPException(status_code=503, detail=\"모델 파이프라인이 준비되지 않았습니다.\")\n",
        "\n",
        "#     async with model_lock: # Lock 사용\n",
        "#         try:\n",
        "#             logger.info(f\"LoRA 언로드 요청 수신.\")\n",
        "#             # 블로킹 작업을 별도 스레드에서 실행\n",
        "#             message = await asyncio.to_thread(unload_lora_blocking)\n",
        "#             logger.info(f\"LoRA 언로드 요청 처리 완료.\")\n",
        "#             return {\"status\": \"success\", \"message\": message}\n",
        "#         except Exception as e:\n",
        "#             logger.error(f\"LoRA 언로드 중 오류: {e}\", exc_info=True)\n",
        "#             raise HTTPException(status_code=500, detail=f\"LoRA 언로드 실패: {str(e)}\")\n",
        "\n",
        "# # --- 서버 실행 (Colab 스크립트에서 uvicorn으로 실행) ---\n",
        "# # if __name__ == \"__main__\":\n",
        "# #     import uvicorn\n",
        "# #     # 환경 변수에서 포트 읽기 시도 (Colab에서는 스크립트에서 직접 지정)\n",
        "# #     port = int(os.environ.get(\"PORT\", 9080))\n",
        "# #     uvicorn.run(app, host=\"0.0.0.0\", port=port)\n",
        "# '''\n",
        "# with open(FASTAPI_APP_FILE, 'w', encoding='utf-8') as f: f.write(fastapi_app_content)\n",
        "# logger.info(f\"✅ FastAPI 앱 스크립트 저장 완료: {FASTAPI_APP_FILE}\")\n",
        "\n",
        "# # ==============================================================================\n",
        "# # 4. FastAPI 서버 시작 (Uvicorn 사용)\n",
        "# # ==============================================================================\n",
        "# logger.info(\"\\n🚀 FastAPI 서버 시작 중 (Uvicorn)...\")\n",
        "\n",
        "# # 핸들러 내부에서 사용할 환경 변수 설정 (FastAPI 앱 시작 전에 설정 필요)\n",
        "# os.environ['TS_MODEL_ID'] = MODEL_ID\n",
        "# if INITIAL_LORA_WEIGHTS_PATH:\n",
        "#     os.environ['TS_INITIAL_LORA_PATH'] = INITIAL_LORA_WEIGHTS_PATH\n",
        "# else:\n",
        "#     if 'TS_INITIAL_LORA_PATH' in os.environ: del os.environ['TS_INITIAL_LORA_PATH']\n",
        "\n",
        "# # Uvicorn 실행 명령어 (백그라운드, 로그 파일 사용)\n",
        "# # --reload 옵션은 Colab에서 파일 변경 감지가 어려워 비추천\n",
        "# uvicorn_cmd = [\n",
        "#     \"uvicorn\",\n",
        "#     \"main:app\", # FastAPI 앱 객체 위치 (main.py 파일의 app 객체)\n",
        "#     \"--host\", \"0.0.0.0\",\n",
        "#     \"--port\", API_PORT,\n",
        "#     \"--workers\", \"1\" # 멀티 워커는 모델 로딩/상태 공유 문제로 1개 권장\n",
        "# ]\n",
        "# logger.info(f\"실행할 Uvicorn 명령어: {' '.join(uvicorn_cmd)}\")\n",
        "# logger.info(f\"Uvicorn 로그는 {UVICORN_LOG_FILE} 파일에 저장됩니다.\")\n",
        "\n",
        "# # 기존 uvicorn 프로세스 종료 시도 (선택적)\n",
        "# subprocess.run(['pkill', '-f', 'uvicorn'], capture_output=True)\n",
        "# time.sleep(3)\n",
        "\n",
        "# # nohup 과 & 를 사용하여 백그라운드 실행 및 로그 리디렉션\n",
        "# nohup_cmd = f\"nohup {' '.join(uvicorn_cmd)} > {UVICORN_LOG_FILE} 2>&1 &\"\n",
        "# logger.info(f\"백그라운드 실행 명령어: {nohup_cmd}\")\n",
        "# # shell=True 사용에 주의 필요하나, nohup/& 사용 시 불가피\n",
        "# process = subprocess.Popen(nohup_cmd, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
        "# # Popen 자체는 즉시 반환됨. 실제 서버 시작은 백그라운드에서 진행.\n",
        "# logger.info(\"   - Uvicorn 서버 시작 명령어 실행됨 (백그라운드).\")\n",
        "\n",
        "# # 서버 시작 대기 (모델 로딩 시간 고려)\n",
        "# wait_seconds = 80 # SDXL 모델 로딩 시간 고려 (충분히 길게)\n",
        "# logger.info(f\"   - FastAPI 서버 초기화 및 모델 로딩 대기 중 ({wait_seconds}초)...\")\n",
        "# time.sleep(wait_seconds)\n",
        "\n",
        "# # Uvicorn 로그 파일 끝부분 확인하여 시작 여부 추정\n",
        "# log_check_success = False\n",
        "# try:\n",
        "#     if os.path.exists(UVICORN_LOG_FILE):\n",
        "#         with open(UVICORN_LOG_FILE, 'r') as f:\n",
        "#             lines = f.readlines()\n",
        "#             tail_lines = lines[-20:] # 마지막 20줄 확인\n",
        "#             print(\"\\n--- Uvicorn 로그 (마지막 20줄) ---\")\n",
        "#             for line in tail_lines:\n",
        "#                 print(line.strip())\n",
        "#                 # Uvicorn 시작 성공 메시지 확인 (버전마다 다를 수 있음)\n",
        "#                 if f\"Uvicorn running on http://0.0.0.0:{API_PORT}\" in line:\n",
        "#                     logger.info(\"✅ Uvicorn 로그에서 서버 시작 메시지 확인됨.\")\n",
        "#                     log_check_success = True\n",
        "#             if not log_check_success and lines:\n",
        "#                 logger.warning(\"⚠️ Uvicorn 로그에서 명확한 시작 메시지를 찾지 못함. 상태 엔드포인트로 확인 필요.\")\n",
        "#             elif not lines:\n",
        "#                  logger.warning(\"⚠️ Uvicorn 로그 파일이 비어 있습니다.\")\n",
        "#     else:\n",
        "#         logger.warning(f\"⚠️ Uvicorn 로그 파일({UVICORN_LOG_FILE})이 생성되지 않았습니다.\")\n",
        "# except Exception as e:\n",
        "#     logger.error(f\"Uvicorn 로그 확인 중 오류: {e}\")\n",
        "\n",
        "# # 최종 확인: 간단한 상태 요청 보내보기\n",
        "# if log_check_success: # 로그에서 시작 확인 시에만 시도\n",
        "#     logger.info(\"   - 서버 상태 확인 시도 (GET /status)...\")\n",
        "#     try:\n",
        "#         status_url = f\"http://127.0.0.1:{API_PORT}/status\"\n",
        "#         response = requests.get(status_url, timeout=30)\n",
        "#         response.raise_for_status()\n",
        "#         status_data = response.json()\n",
        "#         if status_data.get(\"status\") == \"ready\":\n",
        "#             logger.info(\"✅ FastAPI 서버 상태 'ready' 확인!\")\n",
        "#         else:\n",
        "#             logger.warning(f\"⚠️ FastAPI 서버 상태가 'ready'가 아님: {status_data}\")\n",
        "#     except Exception as e:\n",
        "#         logger.error(f\"❌ FastAPI 서버 상태 확인 실패: {e}\")\n",
        "#         logger.error(\"   - 서버가 완전히 시작되지 않았거나 문제가 발생했을 수 있습니다.\")\n",
        "#         logger.error(f\"   - Uvicorn 로그 파일({UVICORN_LOG_FILE})을 확인하세요.\")\n",
        "#         # 문제가 심각하면 여기서 중단\n",
        "#         raise RuntimeError(\"FastAPI 서버 시작 또는 상태 확인 실패\")\n",
        "# else:\n",
        "#      logger.error(\"❌ Uvicorn 로그에서 서버 시작을 확인할 수 없어 중단합니다.\")\n",
        "#      raise RuntimeError(\"FastAPI 서버 시작 확인 실패\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U7ceKaBT7WLI",
        "outputId": "15b2f020-cff2-4962-9be1-414e2e41fbea"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "\n",
            "--- Uvicorn 로그 (마지막 20줄) ---\n",
            "E0000 00:00:1744958119.814446   31475 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1744958119.820939   31475 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-04-18 06:35:19.842455: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "INFO:     Started server process [31475]\n",
            "INFO:     Waiting for application startup.\n",
            "INFO:     애플리케이션 시작: 모델 로딩 시작...\n",
            "INFO:     사용 가능 GPU 감지됨. Device: cuda, Dtype: torch.float16\n",
            "INFO:     메모리 (로딩 전): 할당됨 0.00 GB, 예약됨 0.00 GB\n",
            "\n",
            "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]\n",
            "Loading pipeline components...:  43%|████▎     | 3/7 [00:00<00:00,  7.06it/s]\n",
            "Loading pipeline components...:  71%|███████▏  | 5/7 [00:00<00:00,  9.83it/s]\n",
            "Loading pipeline components...: 100%|██████████| 7/7 [00:01<00:00,  4.72it/s]\n",
            "Loading pipeline components...: 100%|██████████| 7/7 [00:01<00:00,  5.45it/s]\n",
            "INFO:     모델 파이프라인 로딩 완료. Device: cuda:0\n",
            "INFO:     메모리 (파이프라인 로딩 후): 할당됨 7.05 GB, 예약됨 7.34 GB\n",
            "INFO:     모델 및 초기 LoRA 로딩 완료. (소요 시간: 4.10초)\n",
            "INFO:     Application startup complete.\n",
            "INFO:     Uvicorn running on http://0.0.0.0:9080 (Press CTRL+C to quit)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "FastAPI + Diffusers + ngrok API 배포 최종 스크립트 (Colab 기반)\n",
        "\n",
        "- FastAPI: API 서버 구축\n",
        "- Diffusers: Stable Diffusion 모델 추론\n",
        "- ngrok: 외부 인터넷 노출 (Authtoken 필요)\n",
        "- 동적 LoRA 로딩/언로딩 API 포함\n",
        "- TorchServe 관련 코드 모두 제거\n",
        "- 기본 포트 9080 사용\n",
        "\"\"\"\n",
        "\n",
        "# ==============================================================================\n",
        "# 0. 환경 설정 및 변수 정의\n",
        "# ==============================================================================\n",
        "import os\n",
        "import subprocess\n",
        "import json\n",
        "import time\n",
        "import threading\n",
        "import requests\n",
        "from google.colab import drive, files # LoRA 사용 시 필요할 수 있음\n",
        "from IPython.display import display, Image as IPyImage, clear_output\n",
        "import base64\n",
        "import io\n",
        "from PIL import Image\n",
        "import logging\n",
        "import shlex # 명령어 안전하게 분리하기 위해 사용\n",
        "\n",
        "# --- 사용자 설정 ---\n",
        "# @markdown ### 모델 및 LoRA 설정\n",
        "# @markdown 사용할 Hugging Face Diffusers 모델 ID 또는 경로\n",
        "MODEL_ID = \"stabilityai/stable-diffusion-xl-base-1.0\" # @param {type:\"string\"}\n",
        "# @markdown (선택 사항) 시작 시 로드할 LoRA 파일 경로 (Google Drive 내). 비워두면 LoRA 없이 시작.\n",
        "INITIAL_LORA_WEIGHTS_PATH = \"\" # @param {type:\"string\"}\n",
        "\n",
        "# @markdown ---\n",
        "# @markdown ### ngrok 설정\n",
        "# @markdown [ngrok 대시보드](https://dashboard.ngrok.com/get-started/your-authtoken)에서 Authtoken을 복사하여 붙여넣으세요.\n",
        "# @markdown **주의:** Authtoken은 비밀 정보이므로 노트북 공유 시 각별히 주의하세요.\n",
        "NGROK_AUTH_TOKEN = \"2vtLqb9HoP0xoiiwqovFN2laGrd_6pD67sTGUxgGasqEDTwHb\"  # @param {type:\"string\"}\n",
        "\n",
        "# --- 내부 설정 ---\n",
        "FASTAPI_APP_FILE = \"/content/main.py\" # 생성될 FastAPI 앱 파일\n",
        "REQUIREMENTS_PATH = \"/content/requirements_fastapi_ngrok.txt\"\n",
        "UVICORN_LOG_FILE = \"/content/uvicorn.log\" # Uvicorn 로그 파일 경로\n",
        "API_PORT = \"9080\" # FastAPI 서버가 리스닝할 포트\n",
        "MODEL_API_NAME = \"sdxl_diffusers\" # API 경로 등에 참고용 (현재 직접 사용 안 함)\n",
        "is_google_colab = 'google.colab' in str(get_ipython())\n",
        "public_url = None # ngrok URL 저장 변수\n",
        "uvicorn_process = None # Uvicorn 프로세스 추적용\n",
        "\n",
        "# 로깅 설정\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(__name__)\n"
      ],
      "metadata": {
        "id": "zW2NcNO8tPbV"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ==============================================================================\n",
        "# 1. Google Drive 마운트 (LoRA 사용 시)\n",
        "# ==============================================================================\n",
        "if INITIAL_LORA_WEIGHTS_PATH and is_google_colab: # LoRA 경로가 있고 Colab일 때만 마운트\n",
        "    logger.info(\"📂 Google Drive 마운트 중...\")\n",
        "    try:\n",
        "        drive.mount('/content/drive', force_remount=True)\n",
        "        logger.info(\"✅ Google Drive 마운트 완료.\")\n",
        "        # 초기 LoRA 파일 존재 확인\n",
        "        if not os.path.exists(INITIAL_LORA_WEIGHTS_PATH):\n",
        "             # 치명적 오류 대신 경고 표시\n",
        "             logger.error(f\"⚠️ 지정된 초기 LoRA 파일 경로를 찾을 수 없습니다: {INITIAL_LORA_WEIGHTS_PATH}\")\n",
        "             INITIAL_LORA_WEIGHTS_PATH = \"\" # 경로 무효화\n",
        "        else:\n",
        "             logger.info(f\"✅ 초기 LoRA 파일 확인: {INITIAL_LORA_WEIGHTS_PATH}\")\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Google Drive 마운트 중 오류 발생: {e}\")\n",
        "        INITIAL_LORA_WEIGHTS_PATH = \"\" # 오류 시 경로 무효화\n",
        "elif INITIAL_LORA_WEIGHTS_PATH and not is_google_colab:\n",
        "     logger.warning(\"Colab 환경 아님. Drive 마운트 생략. 로컬 경로 유효성 확인 필요.\")\n",
        "     if not os.path.exists(INITIAL_LORA_WEIGHTS_PATH):\n",
        "         logger.error(f\"⚠️ 초기 LoRA 파일 경로 오류 (로컬): {INITIAL_LORA_WEIGHTS_PATH}\")\n",
        "         INITIAL_LORA_WEIGHTS_PATH = \"\"\n",
        "else:\n",
        "     logger.info(\"초기 LoRA 경로가 설정되지 않아 Drive 마운트를 건너<0xEB><0x9C><0x8D>니다.\")\n",
        "\n",
        "# ==============================================================================\n",
        "# 2. 필요 패키지 설치 (pyngrok 추가)\n",
        "# ==============================================================================\n",
        "logger.info(\"\\n⚙️ 필요한 패키지 설치 중 (pyngrok 포함)...\")\n",
        "\n",
        "# requirements_fastapi_ngrok.txt 생성\n",
        "requirements_content = \"\"\"\n",
        "fastapi>=0.100.0 # 버전 명시 권장\n",
        "uvicorn[standard]>=0.20.0 # ASGI 서버\n",
        "python-multipart # FastAPI 폼 데이터 처리\n",
        "requests # API 테스트용\n",
        "pyngrok>=7.0.0 # ngrok 사용 라이브러리\n",
        "\n",
        "torch>=2.0.0\n",
        "diffusers>=0.24.0 # LoRA API 안정성 고려\n",
        "transformers>=4.30.0\n",
        "accelerate>=0.20.0\n",
        "safetensors>=0.3.0\n",
        "invisible-watermark>=0.2.0\n",
        "pillow\n",
        "python-dotenv # .env 파일 로딩 (선택적)\n",
        "\"\"\"\n",
        "with open(REQUIREMENTS_PATH, 'w') as f: f.write(requirements_content)\n",
        "logger.info(f\"✅ requirements_fastapi_ngrok.txt 생성 완료: {REQUIREMENTS_PATH}\")\n",
        "\n",
        "# pip install\n",
        "logger.info(f\"   - pip install 실행: {REQUIREMENTS_PATH}\")\n",
        "try:\n",
        "    # 이전 실행의 uvicorn 등 프로세스 종료 시도 (선택적)\n",
        "    subprocess.run(['pkill', '-f', 'uvicorn'], capture_output=True)\n",
        "    pip_process = subprocess.run(['pip', 'install', '-r', REQUIREMENTS_PATH, '-qq'],\n",
        "                                 check=True, capture_output=True, text=True, encoding='utf-8')\n",
        "    logger.info(\"✅ Python 패키지 설치 완료.\")\n",
        "except subprocess.CalledProcessError as e:\n",
        "    logger.error(f\"💥 pip install 실패! (종료 코드: {e.returncode})\")\n",
        "    print(\"--- pip stdout ---\\n\", e.stdout)\n",
        "    print(\"--- pip stderr ---\\n\", e.stderr)\n",
        "    # pip 설치 실패는 치명적이므로 스크립트 중단\n",
        "    raise RuntimeError(f\"패키지 설치 실패: {e}\")\n",
        "\n",
        "# ngrok Authtoken 확인\n",
        "if not NGROK_AUTH_TOKEN:\n",
        "    logger.warning(\"⚠️ ngrok Authtoken이 입력되지 않았습니다! ngrok 터널 생성에 실패하거나 제한이 있을 수 있습니다.\")\n",
        "    # Authtoken 없이 진행은 가능하나, 경고 표시\n",
        "else:\n",
        "     logger.info(\"✅ ngrok Authtoken 확인됨 (입력값 기준).\")\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "# 3. FastAPI 앱 스크립트 생성 (main.py)\n",
        "# ==============================================================================\n",
        "logger.info(f\"\\n📝 FastAPI 앱 스크립트 생성 중 ({FASTAPI_APP_FILE})...\")\n",
        "\n",
        "# FastAPI 앱 코드 (f''' 대신 ''' 사용, APP_ 환경변수 사용)\n",
        "fastapi_app_content = '''\n",
        "import logging\n",
        "import os\n",
        "import base64\n",
        "import io\n",
        "import time\n",
        "import torch\n",
        "from diffusers import DiffusionPipeline, DPMSolverMultistepScheduler\n",
        "from fastapi import FastAPI, HTTPException, Request\n",
        "from pydantic import BaseModel, Field\n",
        "from PIL import Image\n",
        "import asyncio\n",
        "from contextlib import asynccontextmanager\n",
        "import threading # 전역 변수 보호용 Lock (asyncio.Lock도 가능)\n",
        "\n",
        "# --- 기본 설정 ---\n",
        "# 환경 변수 또는 기본값 사용\n",
        "APP_MODEL_ID = os.environ.get(\"APP_MODEL_ID\", \"stabilityai/stable-diffusion-xl-base-1.0\")\n",
        "APP_INITIAL_LORA_PATH = os.environ.get(\"APP_INITIAL_LORA_PATH\", None)\n",
        "ADAPTER_NAME = \"default\" # 사용할 LoRA 어댑터 이름\n",
        "\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - [%(levelname)s] - %(name)s - %(message)s')\n",
        "logger = logging.getLogger(\"uvicorn.error\") # Uvicorn 에러 로거에 출력\n",
        "\n",
        "# --- 모델 및 상태 관리 ---\n",
        "pipeline = None\n",
        "current_lora_path = None\n",
        "model_lock = threading.Lock() # 동기 함수에서 전역 변수 보호용\n",
        "\n",
        "# --- FastAPI 앱 생명주기 관리 (모델 로딩/언로딩) ---\n",
        "@asynccontextmanager\n",
        "async def lifespan(app: FastAPI):\n",
        "    global pipeline, current_lora_path, model_lock\n",
        "    logger.info(\"Lifespan: 애플리케이션 시작 - 모델 로딩...\")\n",
        "    start_time = time.time()\n",
        "    try:\n",
        "        # 모델 로딩 로직 (동기) - lifespan은 동기 함수 실행 가능\n",
        "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "        torch_dtype = torch.float16 if device == \"cuda\" else torch.float32\n",
        "        logger.info(f\"Lifespan: Device={device}, Dtype={torch_dtype}\")\n",
        "\n",
        "        if torch.cuda.is_available(): logger.info(f\"Lifespan: 메모리 (로딩 전) Allocated={torch.cuda.memory_allocated(device)/1e9:.2f}GB, Reserved={torch.cuda.memory_reserved(device)/1e9:.2f}GB\")\n",
        "\n",
        "        temp_pipeline = DiffusionPipeline.from_pretrained(\n",
        "            APP_MODEL_ID,\n",
        "            torch_dtype=torch_dtype,\n",
        "            variant=\"fp16\" if torch_dtype == torch.float16 else None,\n",
        "            use_safetensors=True\n",
        "        )\n",
        "        temp_pipeline.scheduler = DPMSolverMultistepScheduler.from_config(temp_pipeline.scheduler.config, use_karras_sigmas=True)\n",
        "        temp_pipeline.to(device)\n",
        "        logger.info(f\"Lifespan: 모델 파이프라인 로딩 완료. Device={temp_pipeline.device}\")\n",
        "        if torch.cuda.is_available(): logger.info(f\"Lifespan: 메모리 (파이프라인 로딩 후) Allocated={torch.cuda.memory_allocated(device)/1e9:.2f}GB, Reserved={torch.cuda.memory_reserved(device)/1e9:.2f}GB\")\n",
        "\n",
        "        # 전역 변수 할당 (Lock 불필요, 시작 시 한 번만 실행됨)\n",
        "        pipeline = temp_pipeline\n",
        "\n",
        "        # 초기 LoRA 로드 (동기)\n",
        "        if APP_INITIAL_LORA_PATH and os.path.exists(APP_INITIAL_LORA_PATH):\n",
        "            logger.info(f\"Lifespan: 초기 LoRA 로딩 시도: {APP_INITIAL_LORA_PATH}\")\n",
        "            try:\n",
        "                # lifespan 내에서는 Lock 없이 직접 호출 가능 (시작 시 동기 실행)\n",
        "                load_lora_blocking(APP_INITIAL_LORA_PATH)\n",
        "                logger.info(f\"Lifespan: 초기 LoRA 로딩 성공: {APP_INITIAL_LORA_PATH}\")\n",
        "                # current_lora_path 는 load_lora_blocking 내부에서 업데이트됨\n",
        "                if torch.cuda.is_available(): logger.info(f\"Lifespan: 메모리 (LoRA 로딩 후) Allocated={torch.cuda.memory_allocated(device)/1e9:.2f}GB, Reserved={torch.cuda.memory_reserved(device)/1e9:.2f}GB\")\n",
        "            except Exception as e:\n",
        "                 logger.error(f\"Lifespan: 초기 LoRA 로딩 실패: {e}\", exc_info=True)\n",
        "                 # 실패해도 서버는 시작될 수 있음\n",
        "        elif APP_INITIAL_LORA_PATH:\n",
        "             logger.warning(f\"Lifespan: 초기 LoRA 파일({APP_INITIAL_LORA_PATH}) 없음.\")\n",
        "\n",
        "        loading_time = time.time() - start_time\n",
        "        logger.info(f\"Lifespan: 모델 및 초기 LoRA 로딩 완료. (소요 시간: {loading_time:.2f}초)\")\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Lifespan: 모델 로딩 중 치명적 오류 발생: {e}\", exc_info=True)\n",
        "        pipeline = None # 로딩 실패 시 None 유지\n",
        "\n",
        "    yield # 애플리케이션 실행 구간\n",
        "\n",
        "    # --- 애플리케이션 종료 시 ---\n",
        "    logger.info(\"Lifespan: 애플리케이션 종료 - 모델 정리...\")\n",
        "    with model_lock: # Lock 사용하여 안전하게 정리\n",
        "        pipeline = None\n",
        "        current_lora_path = None\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "        logger.info(\"Lifespan: GPU 캐시 정리 완료.\")\n",
        "    logger.info(\"Lifespan: 애플리케이션 종료 완료.\")\n",
        "\n",
        "\n",
        "app = FastAPI(lifespan=lifespan)\n",
        "\n",
        "# --- Pydantic 모델 정의 ---\n",
        "class InferenceRequest(BaseModel):\n",
        "    prompt: str\n",
        "    negative_prompt: str = \"\"\n",
        "    height: int = Field(1024, gt=0)\n",
        "    width: int = Field(1024, gt=0)\n",
        "    steps: int = Field(30, gt=0, le=100)\n",
        "    cfg_scale: float = Field(7.5, gt=0.0, le=20.0)\n",
        "    seed: int | None = None\n",
        "\n",
        "class InferenceResponse(BaseModel):\n",
        "    image_base64: str\n",
        "    generation_time_ms: int\n",
        "    model_id: str\n",
        "    current_lora: str | None\n",
        "\n",
        "class LoraRequest(BaseModel):\n",
        "    lora_path: str\n",
        "\n",
        "class StatusResponse(BaseModel):\n",
        "    status: str\n",
        "    model_id: str | None\n",
        "    device: str | None\n",
        "    current_lora_path: str | None\n",
        "    active_adapters: list | None\n",
        "\n",
        "\n",
        "# --- Helper Functions (Blocking tasks) ---\n",
        "# 이 함수들은 전역 pipeline과 current_lora_path를 직접 수정하므로 Lock 필요\n",
        "\n",
        "def load_lora_blocking(lora_path: str) -> str:\n",
        "    global pipeline, current_lora_path # 전역 변수 사용 명시\n",
        "    if not pipeline: raise RuntimeError(\"파이프라인이 초기화되지 않았습니다.\")\n",
        "    if not os.path.exists(lora_path): raise FileNotFoundError(f\"LoRA 파일 없음: {lora_path}\")\n",
        "\n",
        "    logger.info(f\"LoRA 로딩 (동기): {lora_path}\")\n",
        "    pipeline.to(pipeline.device) # 장치 확인\n",
        "\n",
        "    # 기존 어댑터 언로드 시도\n",
        "    try:\n",
        "        active_adapters = []\n",
        "        # Diffusers 버전에 따른 API 호환성 고려\n",
        "        if hasattr(pipeline, 'get_active_adapters'):\n",
        "             active_adapters = pipeline.get_active_adapters()\n",
        "        elif hasattr(pipeline, 'get_list_adapters'):\n",
        "             adapter_info = pipeline.get_list_adapters()\n",
        "             active_adapters = [name for name, enabled in adapter_info.items() if enabled]\n",
        "\n",
        "        if ADAPTER_NAME in active_adapters:\n",
        "            logger.info(f\"기존 '{ADAPTER_NAME}' 언로드/삭제 시도...\")\n",
        "            # 최신 버전 우선 (delete_adapters 가 unload 포함 가능성)\n",
        "            if hasattr(pipeline, 'delete_adapters'):\n",
        "                 pipeline.delete_adapters(ADAPTER_NAME)\n",
        "                 logger.info(\"delete_adapters 호출 완료.\")\n",
        "            elif hasattr(pipeline, 'unload_lora_weights'):\n",
        "                 pipeline.unload_lora_weights(adapter_names=[ADAPTER_NAME])\n",
        "                 logger.info(\"unload_lora_weights 호출 완료.\")\n",
        "            else:\n",
        "                 logger.warning(\"LoRA 언로드/삭제 메서드를 찾을 수 없습니다.\")\n",
        "    except Exception as e:\n",
        "        logger.warning(f\"기존 LoRA 언로드 중 경고(무시): {e}\")\n",
        "\n",
        "    logger.info(f\"새 LoRA 로딩: '{lora_path}' ('{ADAPTER_NAME}')...\")\n",
        "    pipeline.load_lora_weights(\n",
        "        os.path.dirname(lora_path),\n",
        "        weight_name=os.path.basename(lora_path),\n",
        "        adapter_name=ADAPTER_NAME\n",
        "    )\n",
        "    logger.info(f\"LoRA 가중치 로딩 성공: {lora_path}\")\n",
        "\n",
        "    # 활성화 (set_adapters 사용 권장)\n",
        "    if hasattr(pipeline, 'set_adapters'):\n",
        "        pipeline.set_adapters([ADAPTER_NAME], adapter_weights=[1.0])\n",
        "        logger.info(f\"'{ADAPTER_NAME}' 활성화 (set_adapters)\")\n",
        "    elif hasattr(pipeline, 'fuse_lora'): # 구버전 호환성\n",
        "         logger.warning(\"set_adapters 를 찾을 수 없음. fuse_lora() 시도.\")\n",
        "         pipeline.fuse_lora(adapter_names=[ADAPTER_NAME])\n",
        "\n",
        "    current_lora_path = lora_path # 상태 업데이트\n",
        "    if torch.cuda.is_available(): torch.cuda.empty_cache()\n",
        "    return f\"성공적으로 LoRA 로드: {lora_path}\"\n",
        "\n",
        "def unload_lora_blocking() -> str:\n",
        "    global pipeline, current_lora_path\n",
        "    if not pipeline: raise RuntimeError(\"파이프라인이 초기화되지 않았습니다.\")\n",
        "    if not current_lora_path: return \"현재 로드된 LoRA 없음.\"\n",
        "\n",
        "    logger.info(f\"LoRA 언로드 (동기): {current_lora_path}\")\n",
        "    pipeline.to(pipeline.device)\n",
        "    unloaded_path = current_lora_path\n",
        "    try:\n",
        "        logger.info(f\"'{ADAPTER_NAME}' 언로드/삭제 시도...\")\n",
        "        if hasattr(pipeline, 'delete_adapters'):\n",
        "             pipeline.delete_adapters(ADAPTER_NAME)\n",
        "             logger.info(\"delete_adapters 호출 완료.\")\n",
        "        elif hasattr(pipeline, 'unload_lora_weights'):\n",
        "             pipeline.unload_lora_weights(adapter_names=[ADAPTER_NAME])\n",
        "             logger.info(\"unload_lora_weights 호출 완료.\")\n",
        "        else: # Fallback 비활성화\n",
        "             if hasattr(pipeline, 'set_adapters'): pipeline.set_adapters([]); logger.info(\"set_adapters([]) 호출\")\n",
        "             elif hasattr(pipeline, 'unfuse_lora'): pipeline.unfuse_lora(); logger.info(\"unfuse_lora() 호출\")\n",
        "             else: logger.warning(\"LoRA 비활성화 메서드 찾을 수 없음\")\n",
        "\n",
        "        logger.info(f\"성공적으로 LoRA 언로드/비활성화: {unloaded_path}\")\n",
        "        current_lora_path = None # 상태 업데이트\n",
        "        if torch.cuda.is_available(): torch.cuda.empty_cache()\n",
        "        return f\"성공적으로 LoRA 언로드: {unloaded_path}\"\n",
        "    except Exception as e:\n",
        "         logger.error(f\"LoRA 언로드 중 오류: {e}\", exc_info=True)\n",
        "         current_lora_path = None # 오류 시에도 상태 초기화\n",
        "         raise RuntimeError(f\"LoRA 언로드 실패: {e}\") from e\n",
        "\n",
        "def generate_image_blocking(req: InferenceRequest) -> Image.Image:\n",
        "    global pipeline # pipeline은 읽기만 하므로 Lock 불필요\n",
        "    if not pipeline: raise RuntimeError(\"파이프라인이 초기화되지 않았습니다.\")\n",
        "\n",
        "    logger.info(f\"이미지 생성 시작 (동기): prompt='{req.prompt[:50]}...'\")\n",
        "    start_gen_time = time.time()\n",
        "    # 요청 파라미터 준비\n",
        "    params = {\n",
        "        \"negative_prompt\": req.negative_prompt,\n",
        "        \"num_inference_steps\": req.steps,\n",
        "        \"guidance_scale\": req.cfg_scale,\n",
        "        \"height\": req.height,\n",
        "        \"width\": req.width,\n",
        "    }\n",
        "    if req.seed is not None:\n",
        "        # Generator는 매번 생성하거나, 시드별로 캐싱할 수 있음\n",
        "        params[\"generator\"] = torch.Generator(device=pipeline.device).manual_seed(req.seed)\n",
        "\n",
        "    # 추론 실행\n",
        "    with torch.inference_mode():\n",
        "        result_image = pipeline(prompt=req.prompt, **params).images[0]\n",
        "\n",
        "    logger.info(f\"이미지 생성 완료 (동기, {(time.time() - start_gen_time)*1000:.0f}ms)\")\n",
        "    return result_image\n",
        "\n",
        "\n",
        "# --- API 엔드포인트 ---\n",
        "\n",
        "@app.get(\"/status\", response_model=StatusResponse)\n",
        "async def get_status():\n",
        "    \"\"\"현재 서버 상태 (로드된 모델, LoRA 등) 반환\"\"\"\n",
        "    with model_lock: # 상태 읽기 시에도 Lock 사용 (current_lora_path 일관성)\n",
        "        if not pipeline:\n",
        "            status = \"error\"\n",
        "            device = None\n",
        "            lora = current_lora_path # pipeline 없어도 lora 경로 변수 확인 가능\n",
        "            adapters = None\n",
        "        else:\n",
        "            status = \"ready\"\n",
        "            device = str(pipeline.device)\n",
        "            lora = current_lora_path\n",
        "            adapters = []\n",
        "            try:\n",
        "                if hasattr(pipeline, 'get_active_adapters'): adapters = pipeline.get_active_adapters()\n",
        "                elif hasattr(pipeline, 'get_list_adapters'): adapters = [name for name, enabled in pipeline.get_list_adapters().items() if enabled]\n",
        "            except Exception as e: logger.warning(f\"활성 어댑터 가져오기 실패: {e}\")\n",
        "\n",
        "        return StatusResponse(\n",
        "            status=status,\n",
        "            model_id=APP_MODEL_ID,\n",
        "            device=device,\n",
        "            current_lora_path=lora,\n",
        "            active_adapters=adapters\n",
        "        )\n",
        "\n",
        "@app.post(\"/predictions\", response_model=InferenceResponse)\n",
        "async def predict(request: InferenceRequest):\n",
        "    \"\"\"이미지 생성 요청 처리\"\"\"\n",
        "    if not pipeline:\n",
        "        raise HTTPException(status_code=503, detail=\"모델 파이프라인 준비되지 않음.\")\n",
        "\n",
        "    start_time = time.time()\n",
        "    try:\n",
        "        # 블로킹 작업을 별도 스레드에서 실행\n",
        "        pil_image = await asyncio.to_thread(generate_image_blocking, request)\n",
        "\n",
        "        # PIL Image -> Base64 (이 작업도 오래 걸리면 to_thread 고려)\n",
        "        buffered = io.BytesIO()\n",
        "        pil_image.save(buffered, format=\"PNG\")\n",
        "        img_str = base64.b64encode(buffered.getvalue()).decode(\"utf-8\")\n",
        "\n",
        "        end_time = time.time()\n",
        "        generation_time_ms = int((end_time - start_time) * 1000)\n",
        "\n",
        "        # 응답 생성 전 현재 LoRA 상태 확인 (Lock 내부에서 변경될 수 있으므로 다시 확인)\n",
        "        with model_lock:\n",
        "            lora_in_use = current_lora_path\n",
        "\n",
        "        return InferenceResponse(\n",
        "            image_base64=img_str,\n",
        "            generation_time_ms=generation_time_ms,\n",
        "            model_id=APP_MODEL_ID,\n",
        "            current_lora=lora_in_use\n",
        "        )\n",
        "    # 특정 사용자 입력 오류 처리 (예: Pydantic 검증 실패는 FastAPI가 처리)\n",
        "    # except ValueError as ve:\n",
        "    #     raise HTTPException(status_code=400, detail=str(ve))\n",
        "    except Exception as e:\n",
        "        logger.error(f\"추론 중 오류 발생: {e}\", exc_info=True)\n",
        "        raise HTTPException(status_code=500, detail=f\"추론 실패: {str(e)}\")\n",
        "\n",
        "@app.post(\"/load-lora\", status_code=200)\n",
        "async def load_lora(request: LoraRequest):\n",
        "    \"\"\"지정된 경로의 LoRA 가중치를 로드\"\"\"\n",
        "    if not pipeline:\n",
        "        raise HTTPException(status_code=503, detail=\"모델 파이프라인 준비되지 않음.\")\n",
        "\n",
        "    # Lock을 사용하여 동시 접근 방지\n",
        "    # asyncio.Lock 은 async 함수 내에서 사용해야 함\n",
        "    # 여기서는 model_lock (threading.Lock)을 사용하고 동기 함수를 스레드에서 실행\n",
        "    try:\n",
        "        logger.info(f\"LoRA 로드 요청 수신: {request.lora_path}\")\n",
        "        # 블로킹 함수 load_lora_blocking 은 내부에서 model_lock 사용\n",
        "        message = await asyncio.to_thread(load_lora_blocking, request.lora_path)\n",
        "        logger.info(f\"LoRA 로드 요청 처리 완료.\")\n",
        "        return {\"status\": \"success\", \"message\": message}\n",
        "    except FileNotFoundError as e:\n",
        "        logger.error(f\"LoRA 파일 없음 오류: {e}\")\n",
        "        raise HTTPException(status_code=404, detail=str(e))\n",
        "    except Exception as e:\n",
        "        logger.error(f\"LoRA 로드 중 오류: {e}\", exc_info=True)\n",
        "        raise HTTPException(status_code=500, detail=f\"LoRA 로드 실패: {str(e)}\")\n",
        "\n",
        "@app.post(\"/unload-lora\", status_code=200)\n",
        "async def unload_lora():\n",
        "    \"\"\"현재 로드된 LoRA 가중치를 언로드\"\"\"\n",
        "    if not pipeline:\n",
        "        raise HTTPException(status_code=503, detail=\"모델 파이프라인 준비되지 않음.\")\n",
        "\n",
        "    try:\n",
        "        logger.info(f\"LoRA 언로드 요청 수신.\")\n",
        "        # 블로킹 함수 unload_lora_blocking 은 내부에서 model_lock 사용\n",
        "        message = await asyncio.to_thread(unload_lora_blocking)\n",
        "        logger.info(f\"LoRA 언로드 요청 처리 완료.\")\n",
        "        return {\"status\": \"success\", \"message\": message}\n",
        "    except Exception as e:\n",
        "        logger.error(f\"LoRA 언로드 중 오류: {e}\", exc_info=True)\n",
        "        raise HTTPException(status_code=500, detail=f\"LoRA 언로드 실패: {str(e)}\")\n",
        "\n",
        "# uvicorn으로 실행 시 이 파일이 직접 실행되지는 않음\n",
        "# if __name__ == \"__main__\":\n",
        "#     print(\"이 파일은 uvicorn을 통해 실행되어야 합니다: uvicorn main:app --host 0.0.0.0 --port 9080\")\n",
        "\n",
        "'''\n",
        "with open(FASTAPI_APP_FILE, 'w', encoding='utf-8') as f: f.write(fastapi_app_content)\n",
        "logger.info(f\"✅ FastAPI 앱 스크립트 저장 완료: {FASTAPI_APP_FILE}\")\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "# 4. FastAPI 서버 시작 (Uvicorn 사용)\n",
        "# ==============================================================================\n",
        "logger.info(\"\\n🚀 FastAPI 서버 시작 중 (Uvicorn)...\")\n",
        "\n",
        "# FastAPI 앱 내부에서 사용할 환경 변수 설정\n",
        "os.environ['APP_MODEL_ID'] = MODEL_ID\n",
        "if INITIAL_LORA_WEIGHTS_PATH: os.environ['APP_INITIAL_LORA_PATH'] = INITIAL_LORA_WEIGHTS_PATH\n",
        "else:\n",
        "    if 'APP_INITIAL_LORA_PATH' in os.environ: del os.environ['APP_INITIAL_LORA_PATH']\n",
        "\n",
        "# Uvicorn 실행 명령어\n",
        "uvicorn_cmd_parts = [\"uvicorn\", \"main:app\", \"--host\", \"0.0.0.0\", \"--port\", API_PORT, \"--workers\", \"1\"]\n",
        "logger.info(f\"실행할 Uvicorn 명령어: {' '.join(uvicorn_cmd_parts)}\")\n",
        "logger.info(f\"Uvicorn 로그는 '{UVICORN_LOG_FILE}' 파일에 저장됩니다.\")\n",
        "\n",
        "# 기존 uvicorn 프로세스 종료\n",
        "subprocess.run(['pkill', '-f', 'uvicorn'], capture_output=True); time.sleep(3)\n",
        "\n",
        "# nohup으로 백그라운드 실행 및 로그 리디렉션\n",
        "# shell=True 대신 shlex.split 사용 시도 (더 안전) -> nohup/& 처리 불가\n",
        "# nohup_cmd = f\"nohup {' '.join(uvicorn_cmd_parts)} > {UVICORN_LOG_FILE} 2>&1 &\" # 간단하게 shell=True 사용\n",
        "cmd_str = f\"nohup {' '.join(uvicorn_cmd_parts)} > {UVICORN_LOG_FILE} 2>&1 & disown\" # disown 추가 시도\n",
        "logger.info(f\"백그라운드 실행 명령어: {cmd_str}\")\n",
        "# Popen 대신 run 사용하고 & 로 백그라운드 실행 (더 간단할 수 있음)\n",
        "# subprocess.run(cmd_str, shell=True)\n",
        "uvicorn_process = subprocess.Popen(cmd_str, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
        "# Popen 은 명령어 실행 자체의 성공 여부만 판단, 백그라운드 프로세스 상태는 별도 확인 필요\n",
        "logger.info(\"   - Uvicorn 서버 시작 명령어 실행됨 (백그라운드).\")\n",
        "\n",
        "# 서버 시작 및 모델 로딩 대기 (시간 충분히)\n",
        "wait_seconds = 150 # SDXL + 초기 LoRA 로딩 시간 고려 (넉넉하게)\n",
        "logger.info(f\"   - FastAPI 서버 초기화 및 모델 로딩 대기 중 ({wait_seconds}초)...\")\n",
        "# 대기 시간 동안 로그 파일 변화를 감지하거나, 주기적으로 상태 체크\n",
        "time.sleep(wait_seconds) # 일단 고정 시간 대기\n",
        "\n",
        "# Uvicorn 로그 및 상태 확인\n",
        "server_ready = False\n",
        "logger.info(f\"   - {wait_seconds}초 경과. 로그 및 상태 확인 시작...\")\n",
        "try:\n",
        "    if os.path.exists(UVICORN_LOG_FILE):\n",
        "        with open(UVICORN_LOG_FILE, 'r', encoding='utf-8', errors='replace') as f: lines = f.readlines(); tail_lines = lines[-20:]\n",
        "        print(\"\\n--- Uvicorn 로그 (마지막 20줄) ---\"); [print(line.strip()) for line in tail_lines]\n",
        "        # 실제 시작 완료 메시지 확인 (uvicorn 버전에 따라 다를 수 있음)\n",
        "        if any(f\"Application startup complete.\" in line for line in lines) or \\\n",
        "           any(f\"Uvicorn running on http://0.0.0.0:{API_PORT}\" in line for line in lines):\n",
        "            logger.info(\"✅ Uvicorn 로그에서 서버 시작/준비 메시지 확인됨.\")\n",
        "            server_ready = True\n",
        "        else: logger.warning(\"⚠️ Uvicorn 로그에서 명확한 시작/준비 메시지를 찾지 못함.\")\n",
        "    else: logger.warning(f\"⚠️ Uvicorn 로그 파일({UVICORN_LOG_FILE}) 없음.\")\n",
        "\n",
        "    if server_ready: # 로그에서 시작했으면 상태 엔드포인트로 최종 확인\n",
        "        logger.info(\"   - 서버 상태 확인 시도 (GET /status)...\")\n",
        "        status_url = f\"http://127.0.0.1:{API_PORT}/status\"\n",
        "        response = requests.get(status_url, timeout=30)\n",
        "        response.raise_for_status(); status_data = response.json()\n",
        "        if status_data.get(\"status\") == \"ready\": logger.info(\"✅ FastAPI 서버 상태 'ready' 최종 확인!\")\n",
        "        else: logger.warning(f\"⚠️ FastAPI 서버 상태가 'ready'가 아님: {status_data}\"); server_ready = False\n",
        "    else: logger.error(\"❌ Uvicorn 로그에서 서버 시작을 확인할 수 없음.\")\n",
        "\n",
        "except Exception as e:\n",
        "    logger.error(f\"❌ FastAPI 서버 상태 확인 실패: {e}\")\n",
        "    logger.error(f\"   - Uvicorn 로그 파일({UVICORN_LOG_FILE})을 직접 확인하세요.\")\n",
        "    server_ready = False\n",
        "\n",
        "if not server_ready:\n",
        "    # 로그 파일 내용 전체 출력 (디버깅 도움)\n",
        "    if os.path.exists(UVICORN_LOG_FILE):\n",
        "        logger.error(\"--- 전체 Uvicorn 로그 ---\")\n",
        "        try:\n",
        "            with open(UVICORN_LOG_FILE, 'r', encoding='utf-8', errors='replace') as f: print(f.read())\n",
        "        except Exception as read_e: print(f\"로그 파일 읽기 오류: {read_e}\")\n",
        "        logger.error(\"-----------------------\")\n",
        "    raise RuntimeError(\"FastAPI 서버 시작 또는 상태 확인 실패\")\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "# 5. ngrok 터널 시작\n",
        "# ==============================================================================\n",
        "# FastAPI 서버가 준비된 경우에만 실행\n",
        "if server_ready and is_google_colab:\n",
        "    logger.info(\"\\n☁️ ngrok 터널 시작 중...\")\n",
        "    from pyngrok import ngrok, conf\n",
        "\n",
        "    if not NGROK_AUTH_TOKEN:\n",
        "        logger.error(\"❌ ngrok Authtoken이 없습니다! https://dashboard.ngrok.com/get-started/your-authtoken 에서 확인 후 입력하세요.\")\n",
        "        raise ValueError(\"ngrok Authtoken 필요\")\n",
        "    else:\n",
        "        try:\n",
        "            # 기존 ngrok 터널/프로세스 정리\n",
        "            try:\n",
        "                for tunnel in ngrok.get_tunnels(): ngrok.disconnect(tunnel.public_url); logger.info(f\"   - 기존 ngrok 터널 종료: {tunnel.public_url}\")\n",
        "                ngrok.kill()\n",
        "                time.sleep(2)\n",
        "            except Exception as ng_kill_e: logger.warning(f\"기존 ngrok 종료 중 오류(무시): {ng_kill_e}\")\n",
        "\n",
        "            # Authtoken 설정\n",
        "            ngrok.set_auth_token(NGROK_AUTH_TOKEN)\n",
        "            logger.info(\"   - ngrok Authtoken 설정 완료.\")\n",
        "\n",
        "            # 터널 생성\n",
        "            logger.info(f\"   - 포트 {API_PORT}에 대한 ngrok 터널 생성 시도...\")\n",
        "            # Colab 환경에 맞는 설정 추가 (선택적)\n",
        "            conf.get_default().region = 'ap' # 아시아 태평양 지역 서버 사용 시도 (ap, eu, au, sa, jp, in)\n",
        "            # conf.get_default().keep_log_files = True # 로그 파일 유지 (디버깅용)\n",
        "\n",
        "            public_url = ngrok.connect(API_PORT, name=f\"fastapi-colab-{time.strftime('%Y%m%d%H%M%S')}\") # 고유 이름 부여 시도\n",
        "            logger.info(f\"✅ ngrok 터널 URL: {public_url}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"❌ ngrok 터널 생성 실패: {e}\", exc_info=True) # 상세 오류 출력\n",
        "            public_url = None # 실패 시 URL 초기화\n",
        "elif not is_google_colab:\n",
        "    logger.info(f\"\\n☁️ 로컬 접속 URL: http://127.0.0.1:{API_PORT}\")\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "# 6. API 테스트\n",
        "# ==============================================================================\n",
        "# 테스트는 URL 이 성공적으로 생성되었을 때만 의미 있음\n",
        "if public_url or (not is_google_colab and server_ready):\n",
        "    logger.info(\"\\n🧪 API 추론 및 관리 테스트 시작...\")\n",
        "    target_base_url = public_url if public_url else f\"http://127.0.0.1:{API_PORT}\"\n",
        "    status_url = f\"{target_base_url}/status\"\n",
        "    predictions_url = f\"{target_base_url}/predictions\"\n",
        "    load_lora_url = f\"{target_base_url}/load-lora\"\n",
        "    unload_lora_url = f\"{target_base_url}/unload-lora\"\n",
        "    logger.info(f\"   - 테스트 대상 URL: {target_base_url}\")\n",
        "\n",
        "    # --- 테스트 1: 상태 조회 ---\n",
        "    logger.info(\"\\n   --- 테스트 1: 상태 조회 ---\")\n",
        "    try:\n",
        "        response = requests.get(status_url, timeout=30)\n",
        "        response.raise_for_status(); logger.info(f\"   - 상태 조회 성공: {response.json()}\")\n",
        "    except Exception as e: logger.error(f\"   - 상태 조회 실패: {e}\")\n",
        "\n",
        "    # --- 테스트 2: 추론 ---\n",
        "    logger.info(\"\\n   --- 테스트 2: 기본 추론 ---\")\n",
        "    test_payload = {\n",
        "        \"prompt\": \"photo of a cute corgi wearing sunglasses, cinematic lighting, masterpiece, high detail\",\n",
        "        \"negative_prompt\": \"ugly, deformed, blurry, low quality, text, words, letters, signature\",\n",
        "        \"steps\": 28, \"cfg_scale\": 7.0, \"width\": 1024, \"height\": 1024, \"seed\": 42\n",
        "    }\n",
        "    logger.info(f\"   - 요청: {json.dumps(test_payload)}\")\n",
        "    try:\n",
        "        start_infer_time = time.time()\n",
        "        response = requests.post(predictions_url, json=test_payload, timeout=400) # 시간 충분히\n",
        "        response.raise_for_status(); result = response.json()\n",
        "        end_infer_time = time.time()\n",
        "        if result and \"image_base64\" in result:\n",
        "            logger.info(f\"   - 추론 성공! (소요 시간: {end_infer_time - start_infer_time:.2f}초)\")\n",
        "            img_data = base64.b64decode(result[\"image_base64\"])\n",
        "            img = Image.open(io.BytesIO(img_data)); display(img)\n",
        "        else: logger.error(f\"   - 추론 응답 형식 오류: {result}\")\n",
        "    except Exception as e: logger.error(f\"   - 추론 실패: {e}\")\n",
        "\n",
        "    # --- 테스트 3 & 4 & 5 (LoRA 관련) ---\n",
        "    # (이전 코드와 동일, NEW_LORA_PATH 설정 필요)\n",
        "    NEW_LORA_PATH = \"\" # <--- 테스트할 다른 LoRA 경로 설정\n",
        "    if INITIAL_LORA_WEIGHTS_PATH or NEW_LORA_PATH:\n",
        "        logger.info(f\"\\n   --- LoRA 변경 테스트 (NEW_LORA_PATH='{NEW_LORA_PATH}') ---\")\n",
        "        # 1. 언로드 테스트 (초기 LoRA가 있었다면)\n",
        "        if INITIAL_LORA_WEIGHTS_PATH:\n",
        "            logger.info(\"   - LoRA 언로드 시도...\")\n",
        "            try:\n",
        "                resp_unload = requests.post(unload_lora_url, timeout=60)\n",
        "                resp_unload.raise_for_status(); logger.info(f\"     LoRA 언로드 응답: {resp_unload.json()}\")\n",
        "            except Exception as e: logger.error(f\"     LoRA 언로드 실패: {e}\")\n",
        "            time.sleep(2) # 적용 시간\n",
        "\n",
        "        # 2. 새 LoRA 로드 테스트 (경로가 지정되었다면)\n",
        "        if NEW_LORA_PATH and os.path.exists(NEW_LORA_PATH):\n",
        "             logger.info(f\"   - 새 LoRA 로드 시도: {NEW_LORA_PATH}\")\n",
        "             try:\n",
        "                 resp_load = requests.post(load_lora_url, json={\"lora_path\": NEW_LORA_PATH}, timeout=120)\n",
        "                 resp_load.raise_for_status(); logger.info(f\"     새 LoRA 로드 응답: {resp_load.json()}\")\n",
        "             except Exception as e: logger.error(f\"     새 LoRA 로드 실패: {e}\")\n",
        "             time.sleep(2) # 적용 시간\n",
        "\n",
        "             # 3. 새 LoRA 적용 후 추론 테스트\n",
        "             logger.info(\"   - 새 LoRA 적용 후 추론 시도...\")\n",
        "             try:\n",
        "                 start_infer_time = time.time()\n",
        "                 response = requests.post(predictions_url, json=test_payload, timeout=400) # 동일 페이로드 사용\n",
        "                 response.raise_for_status(); result = response.json()\n",
        "                 end_infer_time = time.time()\n",
        "                 if result and \"image_base64\" in result:\n",
        "                     logger.info(f\"   - 새 LoRA 추론 성공! (소요 시간: {end_infer_time - start_infer_time:.2f}초)\")\n",
        "                     img_data = base64.b64decode(result[\"image_base64\"])\n",
        "                     img = Image.open(io.BytesIO(img_data)); display(img)\n",
        "                 else: logger.error(f\"   - 새 LoRA 추론 응답 형식 오류: {result}\")\n",
        "             except Exception as e: logger.error(f\"   - 새 LoRA 적용 후 추론 실패: {e}\")\n",
        "        elif NEW_LORA_PATH:\n",
        "             logger.warning(f\"   - 새 LoRA 경로({NEW_LORA_PATH})가 유효하지 않아 로드 테스트 건너<0xEB><0x9C><0x8D>.\")\n",
        "\n",
        "else:\n",
        "    logger.warning(\"⚠️ API 테스트를 건너<0xEB><0x9C><0x8D>니다 (URL 생성 실패 또는 서버 미준비).\")\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "# 7. 서버 정보 및 종료 안내\n",
        "# ==============================================================================\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"🎉 스크립트 실행 완료 🎉\")\n",
        "print(\"=\"*60)\n",
        "print(\"=\"*60)\n",
        "# server_ready 변수는 코드 블록 4 끝에서 FastAPI 서버 상태에 따라 True 또는 False로 설정됨\n",
        "# server_ok 대신 server_ready 변수를 직접 사용합니다.\n",
        "\n",
        "# FastAPI 서버가 성공적으로 준비되었는지 확인\n",
        "if 'server_ready' in locals() and server_ready: # server_ready 변수가 존재하고 True 인지 확인\n",
        "    if public_url: print(f\"✅ 외부 접속 URL (ngrok): {public_url}\")\n",
        "    else: print(f\"⚠️ ngrok URL 생성 실패 또는 Colab 환경 아님.\")\n",
        "    print(f\"\\n🔗 API 엔드포인트:\")\n",
        "    print(f\"  - 추론 (POST) : {target_base_url}/predictions\")\n",
        "    print(f\"  - 상태 (GET)  : {target_base_url}/status\")\n",
        "    print(f\"  - LoRA 로드(POST): {target_base_url}/load-lora\")\n",
        "    print(f\"  - LoRA언로드(POST): {target_base_url}/unload-lora\")\n",
        "    print(f\"\\n📄 Uvicorn 로그 파일: {UVICORN_LOG_FILE}\")\n",
        "    print(\"\\n🚀 FastAPI 서버와 ngrok 터널(실행된 경우)이 백그라운드에서 실행 중입니다.\")\n",
        "    print(\"   - API를 계속 사용하려면 이 Colab 노트북 세션을 활성 상태로 유지하세요.\")\n",
        "    print(\"   - 서버/터널을 중지하려면 아래 8번 셀의 주석을 해제하고 실행하세요.\")\n",
        "else:\n",
        "    print(\"\\n❌ 서버 시작 또는 모델 준비에 실패했습니다.\")\n",
        "    print(\"   - 위의 로그를 검토하여 원인을 확인하세요.\")\n",
        "    print(\"   - 특히 Uvicorn 로그 파일을 확인하는 것이 도움이 될 수 있습니다.\")\n",
        "\n",
        "# ==============================================================================\n",
        "# 8. 서버 및 터널 종료 (주석 해제 후 실행)\n",
        "# ==============================================================================\n",
        "# logger.info(\"\\n⏹️ FastAPI 서버 및 ngrok 터널 종료 중...\")\n",
        "# try:\n",
        "#     from pyngrok import ngrok\n",
        "#     logger.info(\"   - ngrok 터널 종료 시도...\")\n",
        "#     ngrok.kill()\n",
        "#     logger.info(\"   - ngrok 프로세스 종료 완료.\")\n",
        "# except ImportError: logger.warning(\"   - pyngrok 미설치됨.\")\n",
        "# except Exception as e: logger.error(f\"   - ngrok 종료 중 오류: {e}\")\n",
        "#\n",
        "# logger.info(\"   - Uvicorn 프로세스 종료 시도 (pkill)...\")\n",
        "# # pkill이 항상 성공하는 것은 아님\n",
        "# result_pkill = subprocess.run(['pkill', '-f', 'uvicorn main:app'], capture_output=True)\n",
        "# logger.info(f\"   - pkill 결과: {result_pkill.returncode}\")\n",
        "# time.sleep(3)\n",
        "# logger.info(\"✅ 서버 종료 완료 시도.\")\n",
        "# # Uvicorn 프로세스가 여전히 살아있는지 확인 (선택적)\n",
        "# result_pgrep = subprocess.run(['pgrep', '-f', 'uvicorn main:app'], capture_output=True)\n",
        "# if result_pgrep.stdout: logger.warning(\"   - Uvicorn 프로세스가 여전히 실행 중일 수 있습니다.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fJlUUH5CsPYd",
        "outputId": "49d0f0bc-accb-4568-ded3-292278eb7912"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:pyngrok.process.ngrok:t=2025-04-18T07:37:24+0000 lvl=warn msg=\"Stopping forwarder\" name=fastapi-colab-20250418071242 acceptErr=\"failed to accept connection: Listener closed\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Uvicorn 로그 (마지막 20줄) ---\n",
            "E0000 00:00:1744961699.488144   47255 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1744961699.494652   47255 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-04-18 07:34:59.516337: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "INFO:     Started server process [47255]\n",
            "INFO:     Waiting for application startup.\n",
            "INFO:     Lifespan: 애플리케이션 시작 - 모델 로딩...\n",
            "INFO:     Lifespan: Device=cuda, Dtype=torch.float16\n",
            "INFO:     Lifespan: 메모리 (로딩 전) Allocated=0.00GB, Reserved=0.00GB\n",
            "\n",
            "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]\n",
            "Loading pipeline components...:  29%|██▊       | 2/7 [00:00<00:00, 14.82it/s]\n",
            "Loading pipeline components...:  71%|███████▏  | 5/7 [00:00<00:00, 18.40it/s]\n",
            "Loading pipeline components...: 100%|██████████| 7/7 [00:01<00:00,  4.17it/s]\n",
            "Loading pipeline components...: 100%|██████████| 7/7 [00:01<00:00,  5.26it/s]\n",
            "INFO:     Lifespan: 모델 파이프라인 로딩 완료. Device=cuda:0\n",
            "INFO:     Lifespan: 메모리 (파이프라인 로딩 후) Allocated=7.06GB, Reserved=7.35GB\n",
            "INFO:     Lifespan: 모델 및 초기 LoRA 로딩 완료. (소요 시간: 4.18초)\n",
            "INFO:     Application startup complete.\n",
            "INFO:     Uvicorn running on http://0.0.0.0:9080 (Press CTRL+C to quit)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:__main__:   - 상태 조회 실패: No connection adapters were found for 'NgrokTunnel: \"https://172f-35-247-173-221.ngrok-free.app\" -> \"http://localhost:9080\"/status'\n",
            "ERROR:__main__:   - 추론 실패: No connection adapters were found for 'NgrokTunnel: \"https://172f-35-247-173-221.ngrok-free.app\" -> \"http://localhost:9080\"/predictions'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "🎉 스크립트 실행 완료 🎉\n",
            "============================================================\n",
            "============================================================\n",
            "✅ 외부 접속 URL (ngrok): NgrokTunnel: \"https://172f-35-247-173-221.ngrok-free.app\" -> \"http://localhost:9080\"\n",
            "\n",
            "🔗 API 엔드포인트:\n",
            "  - 추론 (POST) : NgrokTunnel: \"https://172f-35-247-173-221.ngrok-free.app\" -> \"http://localhost:9080\"/predictions\n",
            "  - 상태 (GET)  : NgrokTunnel: \"https://172f-35-247-173-221.ngrok-free.app\" -> \"http://localhost:9080\"/status\n",
            "  - LoRA 로드(POST): NgrokTunnel: \"https://172f-35-247-173-221.ngrok-free.app\" -> \"http://localhost:9080\"/load-lora\n",
            "  - LoRA언로드(POST): NgrokTunnel: \"https://172f-35-247-173-221.ngrok-free.app\" -> \"http://localhost:9080\"/unload-lora\n",
            "\n",
            "📄 Uvicorn 로그 파일: /content/uvicorn.log\n",
            "\n",
            "🚀 FastAPI 서버와 ngrok 터널(실행된 경우)이 백그라운드에서 실행 중입니다.\n",
            "   - API를 계속 사용하려면 이 Colab 노트북 세션을 활성 상태로 유지하세요.\n",
            "   - 서버/터널을 중지하려면 아래 8번 셀의 주석을 해제하고 실행하세요.\n"
          ]
        }
      ]
    }
  ]
}